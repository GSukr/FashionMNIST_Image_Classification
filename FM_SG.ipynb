{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FM_SG.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RMtUYlLMYLer"
      },
      "source": [
        "### Image classification using Fashion MNIST data set\n",
        "#### This notebook investigates whether multiple CNN models can achieve higher classification accuracy than any individual model. Two simple strategies for combining models are examined:\n",
        "\n",
        "> 1.   Classification based on the average class probabilities of models\n",
        "\n",
        "> 2.   Using the mode class for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iw0jg7HkU1Sy",
        "colab": {}
      },
      "source": [
        "# Import\n",
        "import os # for file handling\n",
        "import pandas as pd # for data handling\n",
        "import numpy as np # for linear algebra\n",
        "import time # to time runs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fms-I928o1wq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt # to display images\n",
        "from sklearn import metrics # to evaluate classification accuracy\n",
        "import tensorflow as tf # for neural networks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ql7WEyPo1ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9XVY7_EHYcjT"
      },
      "source": [
        "#### Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dldlOy18YnkA",
        "outputId": "18f04e82-8c37-4508-cc10-1ec65fd35493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# get fashion mnist data\n",
        "(x_train,y_train), (x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# show shapes of tensors\n",
        "print(\"x_train shape:\", x_train.shape, \", y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape, \", y_test shape:\", y_test.shape)\n",
        "\n",
        "# get number of classes\n",
        "nClasses = len(np.unique(y_train)) # number of output classes\n",
        "print(\"Number of classes: \", nClasses)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28) , y_train shape: (60000,)\n",
            "x_test shape: (10000, 28, 28) , y_test shape: (10000,)\n",
            "Number of classes:  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0vCgZiggY8Wx"
      },
      "source": [
        "#### Pre-process data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UO4Cf8ZCVb18",
        "outputId": "02b842cc-5e1e-4152-b7d4-baf247c89aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# normalize grayscale pixel values (0-255) to (0,1)\n",
        "x_train = x_train.astype('float32')/255 # normalized training inputs\n",
        "x_test = x_test.astype('float32')/255 # normalized test inputs\n",
        "\n",
        "# reshape to needed input shape for network\n",
        "x_train, x_test = x_train.reshape((-1,28,28,1)), x_test.reshape((-1,28,28,1))\n",
        "input_shape = x_train.shape[1:] # input shape for network\n",
        "\n",
        "# show shapes of re-shaped tensors\n",
        "print(\"x_train shape:\", x_train.shape, \", y_train shape:\", y_train.shape)\n",
        "print(\"x_test shape:\", x_test.shape, \", y_test shape:\", y_test.shape)\n",
        "\n",
        "# get image dimensions\n",
        "img_h, img_w, img_channels = x_train.shape[1:] # size of image\n",
        "print(\"Image height = %d, image width = %d, number of channels = %d\" \n",
        "      %(img_h, img_w, img_channels))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1) , y_train shape: (60000,)\n",
            "x_test shape: (10000, 28, 28, 1) , y_test shape: (10000,)\n",
            "Image height = 28, image width = 28, number of channels = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tnbugna9Z_7i"
      },
      "source": [
        "#### Define function to create Convolution Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_0t1UyAfUGAQ",
        "colab": {}
      },
      "source": [
        "def convNN(model, ch1, ch2, kernel, pool, nDense, drop, dropDense):\n",
        "    \n",
        "    model = tf.keras.models.Sequential() # create model                \n",
        "    \n",
        "    # first CONV => RELU => CONV => RELU => POOL layer set\n",
        "    model.add(Conv2D(ch1, kernel, padding=\"same\", \n",
        "                     activation=\"relu\", input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(ch1, kernel, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=pool))\n",
        "    model.add(tf.keras.layers.Dropout(drop))\n",
        " \n",
        "    # second CONV => RELU => CONV => RELU => POOL layer set\n",
        "    model.add(Conv2D(ch2, kernel, padding=\"same\", \n",
        "                     activation=\"relu\", input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(ch2, kernel, padding=\"same\", activation=\"relu\"))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=pool))\n",
        "    model.add(tf.keras.layers.Dropout(drop))\n",
        " \n",
        "    model.add(tf.keras.layers.Flatten()) \n",
        "    \n",
        "    # FC => RELU layers\n",
        "    model.add(tf.keras.layers.Dense(nDense, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(tf.keras.layers.Dropout(dropDense))\n",
        "    \n",
        "    # output softmax layer\n",
        "    model.add(tf.keras.layers.Dense(nClasses, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "                  optimizer=tf.keras.optimizers.Adadelta(),\n",
        "                  metrics=['accuracy'])\n",
        " \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XoV4YAnEaGi9"
      },
      "source": [
        "#### Specify parameters for convolution network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gv72JOA4Obgr",
        "outputId": "d0dbf38e-4036-44a3-f431-49d3a0cdc3f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "# Parameters for CNN models (change as desired)\n",
        "ch1, ch2 = 32, 64 # number of output channels\n",
        "kernel = (3,3) # filter shape\n",
        "pool = (2,2) # max pool size\n",
        "nDense = 512 # dense layer size\n",
        "drop, dropDense = 0.25, 0.5\n",
        "\n",
        "# create model\n",
        "mod = convNN('model', ch1, ch2, kernel, pool, nDense, drop, dropDense)\n",
        "mod.summary() # show model summary"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_1 (Ba (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_2 (Ba (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_3 (Ba (None, 14, 14, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_4 (Ba (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,679,082\n",
            "Trainable params: 1,677,674\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bIUuMmiVaa_2"
      },
      "source": [
        "#### Define function to plot accuracy with training and validation data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "42uggfi9V4Ii",
        "colab": {}
      },
      "source": [
        "def plotHistorty(model, history):\n",
        "    # plot history for accuracy\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title(model+' accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZKw36AbLadKY"
      },
      "source": [
        "#### Specify parameters for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J2iGkIIwakBL",
        "colab": {}
      },
      "source": [
        "batchSize = 32 # batch size for training\n",
        "epochs = 100 # number of training epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cgbQTfnYawhH"
      },
      "source": [
        "#### Train CNN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e36k6SJlUErq",
        "outputId": "88748cc9-5e49-4eb3-b636-6a6b7ba9726f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21985
        }
      },
      "source": [
        "nModels = 3 # number of models to train\n",
        "probs = np.zeros((len(y_test), nClasses)) # mean probabilities for classes\n",
        "result = [] # for results\n",
        "\n",
        "# Train models\n",
        "for i in range(nModels):\n",
        "    model = 'model_'+str(i+1) # model name\n",
        "    print('\\nTraining Model: '+ model + '\\n')\n",
        "    bestWts = model+\".weights.hdf5\" # best weights file\n",
        "  \n",
        "  # checkpoint to save best model\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(bestWts,\n",
        "                                                  monitor='val_acc',\n",
        "                                                  verbose=1,\n",
        "                                                  save_best_only=True,\n",
        "                                                  mode='max')\n",
        "  \n",
        "  # create model\n",
        "    mod = convNN(model, ch1, ch2, kernel, pool, nDense, drop, dropDense)\n",
        "  \n",
        "    st = time.time() # start time for training\n",
        "  \n",
        "  # train models and maintain history\n",
        "    hist = mod.fit(x_train,\n",
        "                 tf.keras.utils.to_categorical(y_train, num_classes=nClasses), \n",
        "                 batch_size=batchSize,\n",
        "                 epochs=epochs, \n",
        "                 validation_split = 1/6,\n",
        "                 callbacks=[checkpoint])\n",
        "    t = time.time() - st # time to train model\n",
        "  \n",
        "    print(\"\\nTime to train classifier: %4.2f seconds\\n\" %(t))\n",
        "  \n",
        "    mod.save_weights(bestWts) # save best weights\n",
        "\n",
        "    prob = mod.predict(x_test) # predict probabilities for test examples\n",
        "    predicted = prob.argmax(axis=1) # most likely class\n",
        "    acc = metrics.accuracy_score(y_test, predicted) # best accuracy \n",
        "    print('%s Test accuracy = %4.2f%%\\n\\n' %(model, acc*100.0))\n",
        "  \n",
        "    probs += prob\n",
        "    predicted = probs.argmax(axis=1) # most likely class\n",
        "    accCum = metrics.accuracy_score(y_test, predicted) # best accuracy \n",
        "    print('%s Gestalt Test accuracy = %4.2f%%\\n\\n' %(model,accCum*100.0))\n",
        "  \n",
        "    result.append([model,acc,accCum,t])\n",
        "    plotHistorty(model, hist) # display training and test accuracy"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: model_1\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.8321\n",
            "Epoch 00001: val_acc improved from -inf to 0.89430, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 32s 642us/sample - loss: 0.4941 - acc: 0.8322 - val_loss: 0.2839 - val_acc: 0.8943\n",
            "Epoch 2/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.8888\n",
            "Epoch 00002: val_acc improved from 0.89430 to 0.90800, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.3119 - acc: 0.8888 - val_loss: 0.2696 - val_acc: 0.9080\n",
            "Epoch 3/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.2718 - acc: 0.9035\n",
            "Epoch 00003: val_acc improved from 0.90800 to 0.91590, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.2717 - acc: 0.9036 - val_loss: 0.2289 - val_acc: 0.9159\n",
            "Epoch 4/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.2429 - acc: 0.9144\n",
            "Epoch 00004: val_acc did not improve from 0.91590\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.2429 - acc: 0.9144 - val_loss: 0.2780 - val_acc: 0.9032\n",
            "Epoch 5/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.2243 - acc: 0.9202\n",
            "Epoch 00005: val_acc improved from 0.91590 to 0.92830, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 555us/sample - loss: 0.2243 - acc: 0.9202 - val_loss: 0.2016 - val_acc: 0.9283\n",
            "Epoch 6/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9247\n",
            "Epoch 00006: val_acc improved from 0.92830 to 0.92860, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 556us/sample - loss: 0.2135 - acc: 0.9247 - val_loss: 0.2046 - val_acc: 0.9286\n",
            "Epoch 7/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1968 - acc: 0.9303\n",
            "Epoch 00007: val_acc improved from 0.92860 to 0.93100, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.1968 - acc: 0.9303 - val_loss: 0.1984 - val_acc: 0.9310\n",
            "Epoch 8/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1853 - acc: 0.9335\n",
            "Epoch 00008: val_acc did not improve from 0.93100\n",
            "50000/50000 [==============================] - 28s 552us/sample - loss: 0.1853 - acc: 0.9334 - val_loss: 0.2036 - val_acc: 0.9296\n",
            "Epoch 9/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1790 - acc: 0.9360\n",
            "Epoch 00009: val_acc improved from 0.93100 to 0.93310, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 579us/sample - loss: 0.1791 - acc: 0.9359 - val_loss: 0.1915 - val_acc: 0.9331\n",
            "Epoch 10/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1709 - acc: 0.9394\n",
            "Epoch 00010: val_acc did not improve from 0.93310\n",
            "50000/50000 [==============================] - 28s 557us/sample - loss: 0.1713 - acc: 0.9393 - val_loss: 0.1931 - val_acc: 0.9313\n",
            "Epoch 11/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.9425\n",
            "Epoch 00011: val_acc did not improve from 0.93310\n",
            "50000/50000 [==============================] - 28s 556us/sample - loss: 0.1620 - acc: 0.9425 - val_loss: 0.2022 - val_acc: 0.9331\n",
            "Epoch 12/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1544 - acc: 0.9464\n",
            "Epoch 00012: val_acc improved from 0.93310 to 0.93500, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.1544 - acc: 0.9464 - val_loss: 0.1958 - val_acc: 0.9350\n",
            "Epoch 13/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1508 - acc: 0.9468\n",
            "Epoch 00013: val_acc improved from 0.93500 to 0.93710, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 557us/sample - loss: 0.1508 - acc: 0.9468 - val_loss: 0.1972 - val_acc: 0.9371\n",
            "Epoch 14/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9487\n",
            "Epoch 00014: val_acc did not improve from 0.93710\n",
            "50000/50000 [==============================] - 28s 553us/sample - loss: 0.1442 - acc: 0.9487 - val_loss: 0.2007 - val_acc: 0.9336\n",
            "Epoch 15/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.9503\n",
            "Epoch 00015: val_acc improved from 0.93710 to 0.93950, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 555us/sample - loss: 0.1404 - acc: 0.9504 - val_loss: 0.1824 - val_acc: 0.9395\n",
            "Epoch 16/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1340 - acc: 0.9527\n",
            "Epoch 00016: val_acc did not improve from 0.93950\n",
            "50000/50000 [==============================] - 28s 553us/sample - loss: 0.1340 - acc: 0.9527 - val_loss: 0.2090 - val_acc: 0.9355\n",
            "Epoch 17/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1321 - acc: 0.9531\n",
            "Epoch 00017: val_acc did not improve from 0.93950\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.1321 - acc: 0.9531 - val_loss: 0.2012 - val_acc: 0.9355\n",
            "Epoch 18/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1276 - acc: 0.9548\n",
            "Epoch 00018: val_acc improved from 0.93950 to 0.93970, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.1275 - acc: 0.9549 - val_loss: 0.1865 - val_acc: 0.9397\n",
            "Epoch 19/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.1188 - acc: 0.9571\n",
            "Epoch 00019: val_acc did not improve from 0.93970\n",
            "50000/50000 [==============================] - 28s 554us/sample - loss: 0.1189 - acc: 0.9571 - val_loss: 0.1900 - val_acc: 0.9356\n",
            "Epoch 20/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1141 - acc: 0.9597\n",
            "Epoch 00020: val_acc did not improve from 0.93970\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.1143 - acc: 0.9596 - val_loss: 0.2002 - val_acc: 0.9387\n",
            "Epoch 21/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1137 - acc: 0.9605\n",
            "Epoch 00021: val_acc improved from 0.93970 to 0.94150, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.1136 - acc: 0.9605 - val_loss: 0.1931 - val_acc: 0.9415\n",
            "Epoch 22/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9621\n",
            "Epoch 00022: val_acc did not improve from 0.94150\n",
            "50000/50000 [==============================] - 28s 556us/sample - loss: 0.1067 - acc: 0.9621 - val_loss: 0.2178 - val_acc: 0.9366\n",
            "Epoch 23/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1099 - acc: 0.9618\n",
            "Epoch 00023: val_acc did not improve from 0.94150\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.1099 - acc: 0.9618 - val_loss: 0.2029 - val_acc: 0.9398\n",
            "Epoch 24/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1065 - acc: 0.9625\n",
            "Epoch 00024: val_acc did not improve from 0.94150\n",
            "50000/50000 [==============================] - 29s 571us/sample - loss: 0.1064 - acc: 0.9626 - val_loss: 0.2020 - val_acc: 0.9407\n",
            "Epoch 25/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0979 - acc: 0.9659\n",
            "Epoch 00025: val_acc did not improve from 0.94150\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0981 - acc: 0.9659 - val_loss: 0.2077 - val_acc: 0.9380\n",
            "Epoch 26/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9668\n",
            "Epoch 00026: val_acc did not improve from 0.94150\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0936 - acc: 0.9669 - val_loss: 0.2327 - val_acc: 0.9367\n",
            "Epoch 27/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0919 - acc: 0.9671\n",
            "Epoch 00027: val_acc improved from 0.94150 to 0.94170, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 566us/sample - loss: 0.0919 - acc: 0.9671 - val_loss: 0.2256 - val_acc: 0.9417\n",
            "Epoch 28/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0867 - acc: 0.9692\n",
            "Epoch 00028: val_acc did not improve from 0.94170\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0868 - acc: 0.9691 - val_loss: 0.2192 - val_acc: 0.9406\n",
            "Epoch 29/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0884 - acc: 0.9696\n",
            "Epoch 00029: val_acc did not improve from 0.94170\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0884 - acc: 0.9696 - val_loss: 0.2314 - val_acc: 0.9330\n",
            "Epoch 30/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9692\n",
            "Epoch 00030: val_acc did not improve from 0.94170\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0861 - acc: 0.9692 - val_loss: 0.2431 - val_acc: 0.9338\n",
            "Epoch 31/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0837 - acc: 0.9706\n",
            "Epoch 00031: val_acc did not improve from 0.94170\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0836 - acc: 0.9706 - val_loss: 0.2263 - val_acc: 0.9371\n",
            "Epoch 32/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9715\n",
            "Epoch 00032: val_acc did not improve from 0.94170\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0837 - acc: 0.9714 - val_loss: 0.2130 - val_acc: 0.9407\n",
            "Epoch 33/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0794 - acc: 0.9721\n",
            "Epoch 00033: val_acc did not improve from 0.94170\n",
            "50000/50000 [==============================] - 28s 554us/sample - loss: 0.0795 - acc: 0.9721 - val_loss: 0.2330 - val_acc: 0.9397\n",
            "Epoch 34/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0760 - acc: 0.9749\n",
            "Epoch 00034: val_acc did not improve from 0.94170\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0761 - acc: 0.9749 - val_loss: 0.2262 - val_acc: 0.9397\n",
            "Epoch 35/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0764 - acc: 0.9732\n",
            "Epoch 00035: val_acc improved from 0.94170 to 0.94240, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 556us/sample - loss: 0.0764 - acc: 0.9732 - val_loss: 0.2491 - val_acc: 0.9424\n",
            "Epoch 36/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9749\n",
            "Epoch 00036: val_acc did not improve from 0.94240\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0727 - acc: 0.9749 - val_loss: 0.2274 - val_acc: 0.9410\n",
            "Epoch 37/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9743\n",
            "Epoch 00037: val_acc did not improve from 0.94240\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0725 - acc: 0.9743 - val_loss: 0.2325 - val_acc: 0.9406\n",
            "Epoch 38/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0722 - acc: 0.9751\n",
            "Epoch 00038: val_acc did not improve from 0.94240\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0722 - acc: 0.9751 - val_loss: 0.2439 - val_acc: 0.9408\n",
            "Epoch 39/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9767\n",
            "Epoch 00039: val_acc did not improve from 0.94240\n",
            "50000/50000 [==============================] - 29s 572us/sample - loss: 0.0691 - acc: 0.9766 - val_loss: 0.2277 - val_acc: 0.9378\n",
            "Epoch 40/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9760\n",
            "Epoch 00040: val_acc did not improve from 0.94240\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0682 - acc: 0.9760 - val_loss: 0.2381 - val_acc: 0.9379\n",
            "Epoch 41/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0656 - acc: 0.9776\n",
            "Epoch 00041: val_acc did not improve from 0.94240\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0657 - acc: 0.9775 - val_loss: 0.2511 - val_acc: 0.9396\n",
            "Epoch 42/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9773\n",
            "Epoch 00042: val_acc did not improve from 0.94240\n",
            "50000/50000 [==============================] - 28s 557us/sample - loss: 0.0646 - acc: 0.9774 - val_loss: 0.2612 - val_acc: 0.9360\n",
            "Epoch 43/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0613 - acc: 0.9794\n",
            "Epoch 00043: val_acc did not improve from 0.94240\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0612 - acc: 0.9794 - val_loss: 0.2742 - val_acc: 0.9384\n",
            "Epoch 44/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9781\n",
            "Epoch 00044: val_acc did not improve from 0.94240\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0638 - acc: 0.9781 - val_loss: 0.2554 - val_acc: 0.9387\n",
            "Epoch 45/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0641 - acc: 0.9783\n",
            "Epoch 00045: val_acc improved from 0.94240 to 0.94330, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0643 - acc: 0.9783 - val_loss: 0.2496 - val_acc: 0.9433\n",
            "Epoch 46/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9786\n",
            "Epoch 00046: val_acc did not improve from 0.94330\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0615 - acc: 0.9786 - val_loss: 0.2378 - val_acc: 0.9427\n",
            "Epoch 47/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9797\n",
            "Epoch 00047: val_acc did not improve from 0.94330\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0610 - acc: 0.9797 - val_loss: 0.2229 - val_acc: 0.9410\n",
            "Epoch 48/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9791\n",
            "Epoch 00048: val_acc did not improve from 0.94330\n",
            "50000/50000 [==============================] - 28s 555us/sample - loss: 0.0602 - acc: 0.9791 - val_loss: 0.2657 - val_acc: 0.9388\n",
            "Epoch 49/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9799\n",
            "Epoch 00049: val_acc did not improve from 0.94330\n",
            "50000/50000 [==============================] - 28s 553us/sample - loss: 0.0590 - acc: 0.9800 - val_loss: 0.2715 - val_acc: 0.9414\n",
            "Epoch 50/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9803\n",
            "Epoch 00050: val_acc did not improve from 0.94330\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0577 - acc: 0.9803 - val_loss: 0.2508 - val_acc: 0.9424\n",
            "Epoch 51/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9807\n",
            "Epoch 00051: val_acc improved from 0.94330 to 0.94490, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 568us/sample - loss: 0.0590 - acc: 0.9807 - val_loss: 0.2688 - val_acc: 0.9449\n",
            "Epoch 52/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9804\n",
            "Epoch 00052: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 566us/sample - loss: 0.0581 - acc: 0.9804 - val_loss: 0.2488 - val_acc: 0.9422\n",
            "Epoch 53/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9807\n",
            "Epoch 00053: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 570us/sample - loss: 0.0566 - acc: 0.9807 - val_loss: 0.2501 - val_acc: 0.9424\n",
            "Epoch 54/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9824\n",
            "Epoch 00054: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0527 - acc: 0.9824 - val_loss: 0.2727 - val_acc: 0.9419\n",
            "Epoch 55/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9827\n",
            "Epoch 00055: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0532 - acc: 0.9827 - val_loss: 0.2609 - val_acc: 0.9436\n",
            "Epoch 56/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9825\n",
            "Epoch 00056: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 557us/sample - loss: 0.0540 - acc: 0.9825 - val_loss: 0.2657 - val_acc: 0.9406\n",
            "Epoch 57/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9818\n",
            "Epoch 00057: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 556us/sample - loss: 0.0520 - acc: 0.9818 - val_loss: 0.2603 - val_acc: 0.9413\n",
            "Epoch 58/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9825\n",
            "Epoch 00058: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 552us/sample - loss: 0.0523 - acc: 0.9825 - val_loss: 0.2403 - val_acc: 0.9427\n",
            "Epoch 59/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9837\n",
            "Epoch 00059: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 551us/sample - loss: 0.0480 - acc: 0.9836 - val_loss: 0.2688 - val_acc: 0.9429\n",
            "Epoch 60/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9836\n",
            "Epoch 00060: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 567us/sample - loss: 0.0475 - acc: 0.9835 - val_loss: 0.2729 - val_acc: 0.9445\n",
            "Epoch 61/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9841\n",
            "Epoch 00061: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 29s 571us/sample - loss: 0.0456 - acc: 0.9841 - val_loss: 0.2860 - val_acc: 0.9431\n",
            "Epoch 62/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9835\n",
            "Epoch 00062: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 29s 573us/sample - loss: 0.0478 - acc: 0.9835 - val_loss: 0.2684 - val_acc: 0.9422\n",
            "Epoch 63/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9840\n",
            "Epoch 00063: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0485 - acc: 0.9840 - val_loss: 0.2714 - val_acc: 0.9426\n",
            "Epoch 64/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9831\n",
            "Epoch 00064: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 29s 572us/sample - loss: 0.0490 - acc: 0.9831 - val_loss: 0.2467 - val_acc: 0.9432\n",
            "Epoch 65/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9843\n",
            "Epoch 00065: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0456 - acc: 0.9843 - val_loss: 0.2555 - val_acc: 0.9445\n",
            "Epoch 66/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9850\n",
            "Epoch 00066: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 555us/sample - loss: 0.0438 - acc: 0.9850 - val_loss: 0.2600 - val_acc: 0.9424\n",
            "Epoch 67/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9850\n",
            "Epoch 00067: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 555us/sample - loss: 0.0463 - acc: 0.9850 - val_loss: 0.2606 - val_acc: 0.9431\n",
            "Epoch 68/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9851\n",
            "Epoch 00068: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0450 - acc: 0.9851 - val_loss: 0.2852 - val_acc: 0.9415\n",
            "Epoch 69/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9857\n",
            "Epoch 00069: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0408 - acc: 0.9857 - val_loss: 0.2728 - val_acc: 0.9441\n",
            "Epoch 70/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9865\n",
            "Epoch 00070: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0417 - acc: 0.9865 - val_loss: 0.2935 - val_acc: 0.9447\n",
            "Epoch 71/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9860\n",
            "Epoch 00071: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 568us/sample - loss: 0.0424 - acc: 0.9860 - val_loss: 0.2795 - val_acc: 0.9446\n",
            "Epoch 72/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9862\n",
            "Epoch 00072: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 29s 585us/sample - loss: 0.0417 - acc: 0.9862 - val_loss: 0.2899 - val_acc: 0.9432\n",
            "Epoch 73/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9855\n",
            "Epoch 00073: val_acc did not improve from 0.94490\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0425 - acc: 0.9855 - val_loss: 0.2824 - val_acc: 0.9403\n",
            "Epoch 74/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9864\n",
            "Epoch 00074: val_acc improved from 0.94490 to 0.94530, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 555us/sample - loss: 0.0425 - acc: 0.9864 - val_loss: 0.2653 - val_acc: 0.9453\n",
            "Epoch 75/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9869\n",
            "Epoch 00075: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0380 - acc: 0.9869 - val_loss: 0.2829 - val_acc: 0.9453\n",
            "Epoch 76/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9857\n",
            "Epoch 00076: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 552us/sample - loss: 0.0434 - acc: 0.9857 - val_loss: 0.2690 - val_acc: 0.9431\n",
            "Epoch 77/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9858\n",
            "Epoch 00077: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 557us/sample - loss: 0.0413 - acc: 0.9858 - val_loss: 0.2788 - val_acc: 0.9433\n",
            "Epoch 78/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9867\n",
            "Epoch 00078: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0426 - acc: 0.9867 - val_loss: 0.2987 - val_acc: 0.9420\n",
            "Epoch 79/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9862\n",
            "Epoch 00079: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 556us/sample - loss: 0.0427 - acc: 0.9862 - val_loss: 0.2768 - val_acc: 0.9409\n",
            "Epoch 80/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9869\n",
            "Epoch 00080: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0392 - acc: 0.9869 - val_loss: 0.2704 - val_acc: 0.9428\n",
            "Epoch 81/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9872\n",
            "Epoch 00081: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0393 - acc: 0.9872 - val_loss: 0.2840 - val_acc: 0.9433\n",
            "Epoch 82/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9871\n",
            "Epoch 00082: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 554us/sample - loss: 0.0396 - acc: 0.9871 - val_loss: 0.2788 - val_acc: 0.9433\n",
            "Epoch 83/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9873\n",
            "Epoch 00083: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 555us/sample - loss: 0.0379 - acc: 0.9873 - val_loss: 0.2673 - val_acc: 0.9445\n",
            "Epoch 84/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9871\n",
            "Epoch 00084: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 553us/sample - loss: 0.0387 - acc: 0.9871 - val_loss: 0.2754 - val_acc: 0.9435\n",
            "Epoch 85/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9876\n",
            "Epoch 00085: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0365 - acc: 0.9877 - val_loss: 0.2763 - val_acc: 0.9427\n",
            "Epoch 86/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9876\n",
            "Epoch 00086: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 29s 572us/sample - loss: 0.0374 - acc: 0.9876 - val_loss: 0.2929 - val_acc: 0.9419\n",
            "Epoch 87/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9879\n",
            "Epoch 00087: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 569us/sample - loss: 0.0367 - acc: 0.9879 - val_loss: 0.2701 - val_acc: 0.9404\n",
            "Epoch 88/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9881\n",
            "Epoch 00088: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0378 - acc: 0.9881 - val_loss: 0.2959 - val_acc: 0.9420\n",
            "Epoch 89/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9873\n",
            "Epoch 00089: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0373 - acc: 0.9873 - val_loss: 0.2640 - val_acc: 0.9424\n",
            "Epoch 90/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9876\n",
            "Epoch 00090: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 567us/sample - loss: 0.0357 - acc: 0.9876 - val_loss: 0.2901 - val_acc: 0.9433\n",
            "Epoch 91/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9886\n",
            "Epoch 00091: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 557us/sample - loss: 0.0351 - acc: 0.9886 - val_loss: 0.2965 - val_acc: 0.9424\n",
            "Epoch 92/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9888\n",
            "Epoch 00092: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 556us/sample - loss: 0.0350 - acc: 0.9888 - val_loss: 0.2982 - val_acc: 0.9429\n",
            "Epoch 93/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9876\n",
            "Epoch 00093: val_acc did not improve from 0.94530\n",
            "50000/50000 [==============================] - 28s 554us/sample - loss: 0.0371 - acc: 0.9876 - val_loss: 0.2952 - val_acc: 0.9451\n",
            "Epoch 94/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9889\n",
            "Epoch 00094: val_acc improved from 0.94530 to 0.94570, saving model to model_1.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 555us/sample - loss: 0.0342 - acc: 0.9889 - val_loss: 0.2763 - val_acc: 0.9457\n",
            "Epoch 95/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9884\n",
            "Epoch 00095: val_acc did not improve from 0.94570\n",
            "50000/50000 [==============================] - 28s 556us/sample - loss: 0.0359 - acc: 0.9885 - val_loss: 0.3129 - val_acc: 0.9438\n",
            "Epoch 96/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9887\n",
            "Epoch 00096: val_acc did not improve from 0.94570\n",
            "50000/50000 [==============================] - 28s 553us/sample - loss: 0.0337 - acc: 0.9887 - val_loss: 0.2875 - val_acc: 0.9435\n",
            "Epoch 97/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9889\n",
            "Epoch 00097: val_acc did not improve from 0.94570\n",
            "50000/50000 [==============================] - 29s 576us/sample - loss: 0.0341 - acc: 0.9890 - val_loss: 0.2883 - val_acc: 0.9444\n",
            "Epoch 98/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9888\n",
            "Epoch 00098: val_acc did not improve from 0.94570\n",
            "50000/50000 [==============================] - 28s 567us/sample - loss: 0.0335 - acc: 0.9888 - val_loss: 0.2943 - val_acc: 0.9435\n",
            "Epoch 99/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9891\n",
            "Epoch 00099: val_acc did not improve from 0.94570\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0344 - acc: 0.9891 - val_loss: 0.2753 - val_acc: 0.9439\n",
            "Epoch 100/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9897\n",
            "Epoch 00100: val_acc did not improve from 0.94570\n",
            "50000/50000 [==============================] - 28s 566us/sample - loss: 0.0321 - acc: 0.9897 - val_loss: 0.2938 - val_acc: 0.9438\n",
            "\n",
            "Time to train classifier: 2810.56 seconds\n",
            "\n",
            "model_1 Test accuracy = 93.80%\n",
            "\n",
            "\n",
            "model_1 Gestalt Test accuracy = 93.80%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDTIsJEmDPgOo3LkIo8PoigAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: model_2\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.4889 - acc: 0.8341\n",
            "Epoch 00001: val_acc improved from -inf to 0.90010, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 31s 616us/sample - loss: 0.4888 - acc: 0.8342 - val_loss: 0.2766 - val_acc: 0.9001\n",
            "Epoch 2/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8880\n",
            "Epoch 00002: val_acc improved from 0.90010 to 0.91840, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 575us/sample - loss: 0.3144 - acc: 0.8881 - val_loss: 0.2306 - val_acc: 0.9184\n",
            "Epoch 3/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.2701 - acc: 0.9055\n",
            "Epoch 00003: val_acc did not improve from 0.91840\n",
            "50000/50000 [==============================] - 29s 574us/sample - loss: 0.2702 - acc: 0.9054 - val_loss: 0.2671 - val_acc: 0.9055\n",
            "Epoch 4/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9132\n",
            "Epoch 00004: val_acc did not improve from 0.91840\n",
            "50000/50000 [==============================] - 28s 569us/sample - loss: 0.2474 - acc: 0.9132 - val_loss: 0.2243 - val_acc: 0.9169\n",
            "Epoch 5/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.2310 - acc: 0.9183\n",
            "Epoch 00005: val_acc improved from 0.91840 to 0.92650, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 568us/sample - loss: 0.2310 - acc: 0.9183 - val_loss: 0.2076 - val_acc: 0.9265\n",
            "Epoch 6/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.2123 - acc: 0.9247\n",
            "Epoch 00006: val_acc improved from 0.92650 to 0.92990, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.2125 - acc: 0.9247 - val_loss: 0.1961 - val_acc: 0.9299\n",
            "Epoch 7/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.2010 - acc: 0.9272\n",
            "Epoch 00007: val_acc did not improve from 0.92990\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.2011 - acc: 0.9272 - val_loss: 0.1991 - val_acc: 0.9290\n",
            "Epoch 8/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9328\n",
            "Epoch 00008: val_acc improved from 0.92990 to 0.93550, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 579us/sample - loss: 0.1892 - acc: 0.9327 - val_loss: 0.1873 - val_acc: 0.9355\n",
            "Epoch 9/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1796 - acc: 0.9368\n",
            "Epoch 00009: val_acc did not improve from 0.93550\n",
            "50000/50000 [==============================] - 29s 572us/sample - loss: 0.1796 - acc: 0.9368 - val_loss: 0.1881 - val_acc: 0.9355\n",
            "Epoch 10/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.9398\n",
            "Epoch 00010: val_acc improved from 0.93550 to 0.93680, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 571us/sample - loss: 0.1715 - acc: 0.9398 - val_loss: 0.1932 - val_acc: 0.9368\n",
            "Epoch 11/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1632 - acc: 0.9419\n",
            "Epoch 00011: val_acc improved from 0.93680 to 0.93750, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 574us/sample - loss: 0.1632 - acc: 0.9420 - val_loss: 0.1841 - val_acc: 0.9375\n",
            "Epoch 12/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1558 - acc: 0.9444\n",
            "Epoch 00012: val_acc improved from 0.93750 to 0.93830, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 575us/sample - loss: 0.1558 - acc: 0.9444 - val_loss: 0.1803 - val_acc: 0.9383\n",
            "Epoch 13/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1483 - acc: 0.9467\n",
            "Epoch 00013: val_acc did not improve from 0.93830\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.1483 - acc: 0.9466 - val_loss: 0.1887 - val_acc: 0.9344\n",
            "Epoch 14/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.9501\n",
            "Epoch 00014: val_acc did not improve from 0.93830\n",
            "50000/50000 [==============================] - 29s 576us/sample - loss: 0.1425 - acc: 0.9502 - val_loss: 0.1998 - val_acc: 0.9334\n",
            "Epoch 15/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.9521\n",
            "Epoch 00015: val_acc did not improve from 0.93830\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.1389 - acc: 0.9521 - val_loss: 0.1833 - val_acc: 0.9376\n",
            "Epoch 16/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.9515\n",
            "Epoch 00016: val_acc improved from 0.93830 to 0.93950, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.1366 - acc: 0.9515 - val_loss: 0.1968 - val_acc: 0.9395\n",
            "Epoch 17/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9549\n",
            "Epoch 00017: val_acc did not improve from 0.93950\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.1281 - acc: 0.9548 - val_loss: 0.2099 - val_acc: 0.9334\n",
            "Epoch 18/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1239 - acc: 0.9556\n",
            "Epoch 00018: val_acc did not improve from 0.93950\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.1241 - acc: 0.9556 - val_loss: 0.2068 - val_acc: 0.9370\n",
            "Epoch 19/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1216 - acc: 0.9574\n",
            "Epoch 00019: val_acc improved from 0.93950 to 0.94000, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 579us/sample - loss: 0.1218 - acc: 0.9574 - val_loss: 0.1907 - val_acc: 0.9400\n",
            "Epoch 20/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1170 - acc: 0.9593\n",
            "Epoch 00020: val_acc improved from 0.94000 to 0.94220, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 577us/sample - loss: 0.1170 - acc: 0.9593 - val_loss: 0.1900 - val_acc: 0.9422\n",
            "Epoch 21/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1155 - acc: 0.9593\n",
            "Epoch 00021: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 29s 576us/sample - loss: 0.1155 - acc: 0.9593 - val_loss: 0.2037 - val_acc: 0.9400\n",
            "Epoch 22/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9602\n",
            "Epoch 00022: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 29s 575us/sample - loss: 0.1121 - acc: 0.9603 - val_loss: 0.1969 - val_acc: 0.9397\n",
            "Epoch 23/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9633\n",
            "Epoch 00023: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 29s 571us/sample - loss: 0.1066 - acc: 0.9634 - val_loss: 0.1976 - val_acc: 0.9413\n",
            "Epoch 24/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1038 - acc: 0.9635\n",
            "Epoch 00024: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.1038 - acc: 0.9636 - val_loss: 0.2062 - val_acc: 0.9406\n",
            "Epoch 25/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9641\n",
            "Epoch 00025: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.1014 - acc: 0.9641 - val_loss: 0.2445 - val_acc: 0.9333\n",
            "Epoch 26/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0968 - acc: 0.9656\n",
            "Epoch 00026: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0968 - acc: 0.9656 - val_loss: 0.2089 - val_acc: 0.9408\n",
            "Epoch 27/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0952 - acc: 0.9662\n",
            "Epoch 00027: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0952 - acc: 0.9662 - val_loss: 0.2076 - val_acc: 0.9395\n",
            "Epoch 28/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0934 - acc: 0.9681\n",
            "Epoch 00028: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0933 - acc: 0.9681 - val_loss: 0.2101 - val_acc: 0.9416\n",
            "Epoch 29/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0877 - acc: 0.9694\n",
            "Epoch 00029: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0877 - acc: 0.9694 - val_loss: 0.1985 - val_acc: 0.9392\n",
            "Epoch 30/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0860 - acc: 0.9696\n",
            "Epoch 00030: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0859 - acc: 0.9697 - val_loss: 0.2187 - val_acc: 0.9422\n",
            "Epoch 31/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0845 - acc: 0.9708\n",
            "Epoch 00031: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0845 - acc: 0.9708 - val_loss: 0.2240 - val_acc: 0.9388\n",
            "Epoch 32/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0830 - acc: 0.9711\n",
            "Epoch 00032: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 29s 579us/sample - loss: 0.0830 - acc: 0.9710 - val_loss: 0.2165 - val_acc: 0.9410\n",
            "Epoch 33/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0785 - acc: 0.9724\n",
            "Epoch 00033: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 569us/sample - loss: 0.0785 - acc: 0.9724 - val_loss: 0.2267 - val_acc: 0.9393\n",
            "Epoch 34/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0792 - acc: 0.9724\n",
            "Epoch 00034: val_acc did not improve from 0.94220\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0792 - acc: 0.9724 - val_loss: 0.2223 - val_acc: 0.9405\n",
            "Epoch 35/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9733\n",
            "Epoch 00035: val_acc improved from 0.94220 to 0.94380, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 568us/sample - loss: 0.0762 - acc: 0.9733 - val_loss: 0.2290 - val_acc: 0.9438\n",
            "Epoch 36/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0766 - acc: 0.9739\n",
            "Epoch 00036: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0768 - acc: 0.9739 - val_loss: 0.2379 - val_acc: 0.9385\n",
            "Epoch 37/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9754\n",
            "Epoch 00037: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0714 - acc: 0.9754 - val_loss: 0.2377 - val_acc: 0.9422\n",
            "Epoch 38/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9742\n",
            "Epoch 00038: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 29s 570us/sample - loss: 0.0738 - acc: 0.9742 - val_loss: 0.2199 - val_acc: 0.9423\n",
            "Epoch 39/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9751\n",
            "Epoch 00039: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0728 - acc: 0.9751 - val_loss: 0.2272 - val_acc: 0.9403\n",
            "Epoch 40/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9760\n",
            "Epoch 00040: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0690 - acc: 0.9759 - val_loss: 0.2450 - val_acc: 0.9414\n",
            "Epoch 41/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9771\n",
            "Epoch 00041: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0663 - acc: 0.9771 - val_loss: 0.2291 - val_acc: 0.9435\n",
            "Epoch 42/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9768\n",
            "Epoch 00042: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0669 - acc: 0.9767 - val_loss: 0.2437 - val_acc: 0.9409\n",
            "Epoch 43/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9766\n",
            "Epoch 00043: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0686 - acc: 0.9766 - val_loss: 0.2289 - val_acc: 0.9431\n",
            "Epoch 44/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0637 - acc: 0.9791\n",
            "Epoch 00044: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0636 - acc: 0.9791 - val_loss: 0.2478 - val_acc: 0.9406\n",
            "Epoch 45/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9785\n",
            "Epoch 00045: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0623 - acc: 0.9785 - val_loss: 0.2547 - val_acc: 0.9430\n",
            "Epoch 46/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9786\n",
            "Epoch 00046: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0620 - acc: 0.9787 - val_loss: 0.2522 - val_acc: 0.9437\n",
            "Epoch 47/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9790\n",
            "Epoch 00047: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0605 - acc: 0.9791 - val_loss: 0.2532 - val_acc: 0.9380\n",
            "Epoch 48/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9797\n",
            "Epoch 00048: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0605 - acc: 0.9797 - val_loss: 0.2383 - val_acc: 0.9416\n",
            "Epoch 49/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9797\n",
            "Epoch 00049: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0596 - acc: 0.9797 - val_loss: 0.2514 - val_acc: 0.9414\n",
            "Epoch 50/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9811\n",
            "Epoch 00050: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 29s 573us/sample - loss: 0.0577 - acc: 0.9811 - val_loss: 0.2382 - val_acc: 0.9435\n",
            "Epoch 51/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9793\n",
            "Epoch 00051: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0585 - acc: 0.9793 - val_loss: 0.2407 - val_acc: 0.9404\n",
            "Epoch 52/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0588 - acc: 0.9803\n",
            "Epoch 00052: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0590 - acc: 0.9802 - val_loss: 0.2197 - val_acc: 0.9414\n",
            "Epoch 53/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9817\n",
            "Epoch 00053: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0533 - acc: 0.9816 - val_loss: 0.2645 - val_acc: 0.9432\n",
            "Epoch 54/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9807\n",
            "Epoch 00054: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0552 - acc: 0.9807 - val_loss: 0.2401 - val_acc: 0.9433\n",
            "Epoch 55/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9809\n",
            "Epoch 00055: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0578 - acc: 0.9809 - val_loss: 0.2792 - val_acc: 0.9372\n",
            "Epoch 56/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9824\n",
            "Epoch 00056: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0532 - acc: 0.9824 - val_loss: 0.2421 - val_acc: 0.9392\n",
            "Epoch 57/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9817\n",
            "Epoch 00057: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0535 - acc: 0.9817 - val_loss: 0.2559 - val_acc: 0.9426\n",
            "Epoch 58/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9827\n",
            "Epoch 00058: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0523 - acc: 0.9827 - val_loss: 0.2753 - val_acc: 0.9431\n",
            "Epoch 59/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9831\n",
            "Epoch 00059: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0515 - acc: 0.9831 - val_loss: 0.2682 - val_acc: 0.9435\n",
            "Epoch 60/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0494 - acc: 0.9834\n",
            "Epoch 00060: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0493 - acc: 0.9834 - val_loss: 0.2656 - val_acc: 0.9427\n",
            "Epoch 61/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9819\n",
            "Epoch 00061: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0516 - acc: 0.9819 - val_loss: 0.2460 - val_acc: 0.9424\n",
            "Epoch 62/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9843\n",
            "Epoch 00062: val_acc did not improve from 0.94380\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0462 - acc: 0.9843 - val_loss: 0.2543 - val_acc: 0.9419\n",
            "Epoch 63/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9836\n",
            "Epoch 00063: val_acc improved from 0.94380 to 0.94390, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0493 - acc: 0.9836 - val_loss: 0.2829 - val_acc: 0.9439\n",
            "Epoch 64/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9830\n",
            "Epoch 00064: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0495 - acc: 0.9830 - val_loss: 0.2688 - val_acc: 0.9433\n",
            "Epoch 65/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9850\n",
            "Epoch 00065: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0446 - acc: 0.9850 - val_loss: 0.2944 - val_acc: 0.9417\n",
            "Epoch 66/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9846\n",
            "Epoch 00066: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0470 - acc: 0.9846 - val_loss: 0.2595 - val_acc: 0.9419\n",
            "Epoch 67/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9840\n",
            "Epoch 00067: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0486 - acc: 0.9840 - val_loss: 0.2677 - val_acc: 0.9414\n",
            "Epoch 68/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9856\n",
            "Epoch 00068: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 554us/sample - loss: 0.0424 - acc: 0.9855 - val_loss: 0.2729 - val_acc: 0.9420\n",
            "Epoch 69/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9857\n",
            "Epoch 00069: val_acc improved from 0.94390 to 0.94410, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0438 - acc: 0.9857 - val_loss: 0.2694 - val_acc: 0.9441\n",
            "Epoch 70/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9857\n",
            "Epoch 00070: val_acc did not improve from 0.94410\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0417 - acc: 0.9857 - val_loss: 0.2620 - val_acc: 0.9432\n",
            "Epoch 71/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9862\n",
            "Epoch 00071: val_acc did not improve from 0.94410\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0424 - acc: 0.9862 - val_loss: 0.2660 - val_acc: 0.9424\n",
            "Epoch 72/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9856\n",
            "Epoch 00072: val_acc did not improve from 0.94410\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0420 - acc: 0.9856 - val_loss: 0.2686 - val_acc: 0.9439\n",
            "Epoch 73/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9857\n",
            "Epoch 00073: val_acc did not improve from 0.94410\n",
            "50000/50000 [==============================] - 28s 557us/sample - loss: 0.0429 - acc: 0.9857 - val_loss: 0.2947 - val_acc: 0.9423\n",
            "Epoch 74/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9859\n",
            "Epoch 00074: val_acc did not improve from 0.94410\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0414 - acc: 0.9859 - val_loss: 0.2595 - val_acc: 0.9433\n",
            "Epoch 75/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9866\n",
            "Epoch 00075: val_acc improved from 0.94410 to 0.94520, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0405 - acc: 0.9866 - val_loss: 0.2614 - val_acc: 0.9452\n",
            "Epoch 76/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9863\n",
            "Epoch 00076: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0433 - acc: 0.9863 - val_loss: 0.2631 - val_acc: 0.9452\n",
            "Epoch 77/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9872\n",
            "Epoch 00077: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0388 - acc: 0.9872 - val_loss: 0.2778 - val_acc: 0.9442\n",
            "Epoch 78/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9855\n",
            "Epoch 00078: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0422 - acc: 0.9855 - val_loss: 0.2644 - val_acc: 0.9435\n",
            "Epoch 79/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9871\n",
            "Epoch 00079: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0377 - acc: 0.9871 - val_loss: 0.2774 - val_acc: 0.9443\n",
            "Epoch 80/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9868\n",
            "Epoch 00080: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0393 - acc: 0.9868 - val_loss: 0.2970 - val_acc: 0.9419\n",
            "Epoch 81/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9875\n",
            "Epoch 00081: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0377 - acc: 0.9875 - val_loss: 0.2708 - val_acc: 0.9445\n",
            "Epoch 82/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9883\n",
            "Epoch 00082: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0363 - acc: 0.9883 - val_loss: 0.2904 - val_acc: 0.9440\n",
            "Epoch 83/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9878\n",
            "Epoch 00083: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0366 - acc: 0.9878 - val_loss: 0.2674 - val_acc: 0.9446\n",
            "Epoch 84/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9879\n",
            "Epoch 00084: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0374 - acc: 0.9879 - val_loss: 0.2723 - val_acc: 0.9425\n",
            "Epoch 85/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9875\n",
            "Epoch 00085: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0374 - acc: 0.9875 - val_loss: 0.2790 - val_acc: 0.9450\n",
            "Epoch 86/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9879\n",
            "Epoch 00086: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0374 - acc: 0.9879 - val_loss: 0.2880 - val_acc: 0.9434\n",
            "Epoch 87/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9886\n",
            "Epoch 00087: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0341 - acc: 0.9886 - val_loss: 0.2840 - val_acc: 0.9433\n",
            "Epoch 88/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9887\n",
            "Epoch 00088: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 29s 581us/sample - loss: 0.0340 - acc: 0.9887 - val_loss: 0.3077 - val_acc: 0.9446\n",
            "Epoch 89/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9875\n",
            "Epoch 00089: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0347 - acc: 0.9875 - val_loss: 0.3085 - val_acc: 0.9432\n",
            "Epoch 90/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9879\n",
            "Epoch 00090: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 29s 585us/sample - loss: 0.0355 - acc: 0.9879 - val_loss: 0.3251 - val_acc: 0.9424\n",
            "Epoch 91/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9882\n",
            "Epoch 00091: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 29s 585us/sample - loss: 0.0344 - acc: 0.9882 - val_loss: 0.2768 - val_acc: 0.9440\n",
            "Epoch 92/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9885\n",
            "Epoch 00092: val_acc did not improve from 0.94520\n",
            "50000/50000 [==============================] - 28s 567us/sample - loss: 0.0358 - acc: 0.9884 - val_loss: 0.2589 - val_acc: 0.9418\n",
            "Epoch 93/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9889\n",
            "Epoch 00093: val_acc improved from 0.94520 to 0.94590, saving model to model_2.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 577us/sample - loss: 0.0339 - acc: 0.9889 - val_loss: 0.2622 - val_acc: 0.9459\n",
            "Epoch 94/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9886\n",
            "Epoch 00094: val_acc did not improve from 0.94590\n",
            "50000/50000 [==============================] - 29s 575us/sample - loss: 0.0348 - acc: 0.9886 - val_loss: 0.2804 - val_acc: 0.9410\n",
            "Epoch 95/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9896\n",
            "Epoch 00095: val_acc did not improve from 0.94590\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0332 - acc: 0.9896 - val_loss: 0.2928 - val_acc: 0.9459\n",
            "Epoch 96/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9892\n",
            "Epoch 00096: val_acc did not improve from 0.94590\n",
            "50000/50000 [==============================] - 29s 575us/sample - loss: 0.0342 - acc: 0.9892 - val_loss: 0.2888 - val_acc: 0.9423\n",
            "Epoch 97/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9893\n",
            "Epoch 00097: val_acc did not improve from 0.94590\n",
            "50000/50000 [==============================] - 28s 570us/sample - loss: 0.0317 - acc: 0.9893 - val_loss: 0.2791 - val_acc: 0.9430\n",
            "Epoch 98/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9895\n",
            "Epoch 00098: val_acc did not improve from 0.94590\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0321 - acc: 0.9895 - val_loss: 0.3009 - val_acc: 0.9431\n",
            "Epoch 99/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9896\n",
            "Epoch 00099: val_acc did not improve from 0.94590\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0322 - acc: 0.9896 - val_loss: 0.3043 - val_acc: 0.9412\n",
            "Epoch 100/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9888\n",
            "Epoch 00100: val_acc did not improve from 0.94590\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0331 - acc: 0.9888 - val_loss: 0.3010 - val_acc: 0.9433\n",
            "\n",
            "Time to train classifier: 2832.49 seconds\n",
            "\n",
            "model_2 Test accuracy = 93.74%\n",
            "\n",
            "\n",
            "model_2 Gestalt Test accuracy = 94.11%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLX381AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: model_3\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.4950 - acc: 0.8342\n",
            "Epoch 00001: val_acc improved from -inf to 0.89880, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 31s 612us/sample - loss: 0.4948 - acc: 0.8343 - val_loss: 0.2880 - val_acc: 0.8988\n",
            "Epoch 2/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.3119 - acc: 0.8890\n",
            "Epoch 00002: val_acc improved from 0.89880 to 0.90980, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.3119 - acc: 0.8890 - val_loss: 0.2443 - val_acc: 0.9098\n",
            "Epoch 3/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9026\n",
            "Epoch 00003: val_acc improved from 0.90980 to 0.91870, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.2745 - acc: 0.9026 - val_loss: 0.2296 - val_acc: 0.9187\n",
            "Epoch 4/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.2474 - acc: 0.9123\n",
            "Epoch 00004: val_acc improved from 0.91870 to 0.92530, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 569us/sample - loss: 0.2474 - acc: 0.9123 - val_loss: 0.2126 - val_acc: 0.9253\n",
            "Epoch 5/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.2298 - acc: 0.9183\n",
            "Epoch 00005: val_acc did not improve from 0.92530\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.2298 - acc: 0.9183 - val_loss: 0.2330 - val_acc: 0.9135\n",
            "Epoch 6/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.2140 - acc: 0.9242\n",
            "Epoch 00006: val_acc improved from 0.92530 to 0.92620, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.2139 - acc: 0.9242 - val_loss: 0.1999 - val_acc: 0.9262\n",
            "Epoch 7/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.2027 - acc: 0.9280\n",
            "Epoch 00007: val_acc improved from 0.92620 to 0.93030, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 568us/sample - loss: 0.2026 - acc: 0.9280 - val_loss: 0.2013 - val_acc: 0.9303\n",
            "Epoch 8/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.9299\n",
            "Epoch 00008: val_acc improved from 0.93030 to 0.93430, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.1961 - acc: 0.9299 - val_loss: 0.1870 - val_acc: 0.9343\n",
            "Epoch 9/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1838 - acc: 0.9348\n",
            "Epoch 00009: val_acc did not improve from 0.93430\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.1837 - acc: 0.9348 - val_loss: 0.1968 - val_acc: 0.9298\n",
            "Epoch 10/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1723 - acc: 0.9375\n",
            "Epoch 00010: val_acc did not improve from 0.93430\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.1724 - acc: 0.9375 - val_loss: 0.1863 - val_acc: 0.9329\n",
            "Epoch 11/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9404\n",
            "Epoch 00011: val_acc did not improve from 0.93430\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.1683 - acc: 0.9404 - val_loss: 0.2014 - val_acc: 0.9340\n",
            "Epoch 12/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.9430\n",
            "Epoch 00012: val_acc did not improve from 0.93430\n",
            "50000/50000 [==============================] - 29s 572us/sample - loss: 0.1603 - acc: 0.9430 - val_loss: 0.1880 - val_acc: 0.9338\n",
            "Epoch 13/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.9456\n",
            "Epoch 00013: val_acc improved from 0.93430 to 0.93780, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 31s 612us/sample - loss: 0.1536 - acc: 0.9457 - val_loss: 0.1792 - val_acc: 0.9378\n",
            "Epoch 14/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9473\n",
            "Epoch 00014: val_acc did not improve from 0.93780\n",
            "50000/50000 [==============================] - 28s 568us/sample - loss: 0.1478 - acc: 0.9473 - val_loss: 0.1900 - val_acc: 0.9361\n",
            "Epoch 15/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.9504\n",
            "Epoch 00015: val_acc did not improve from 0.93780\n",
            "50000/50000 [==============================] - 28s 570us/sample - loss: 0.1402 - acc: 0.9504 - val_loss: 0.1925 - val_acc: 0.9348\n",
            "Epoch 16/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.9517\n",
            "Epoch 00016: val_acc improved from 0.93780 to 0.93970, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 568us/sample - loss: 0.1348 - acc: 0.9518 - val_loss: 0.1843 - val_acc: 0.9397\n",
            "Epoch 17/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1308 - acc: 0.9529\n",
            "Epoch 00017: val_acc did not improve from 0.93970\n",
            "50000/50000 [==============================] - 28s 566us/sample - loss: 0.1309 - acc: 0.9529 - val_loss: 0.1973 - val_acc: 0.9368\n",
            "Epoch 18/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1254 - acc: 0.9563\n",
            "Epoch 00018: val_acc did not improve from 0.93970\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.1255 - acc: 0.9563 - val_loss: 0.1897 - val_acc: 0.9397\n",
            "Epoch 19/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1217 - acc: 0.9564\n",
            "Epoch 00019: val_acc did not improve from 0.93970\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.1216 - acc: 0.9564 - val_loss: 0.2356 - val_acc: 0.9300\n",
            "Epoch 20/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1178 - acc: 0.9586\n",
            "Epoch 00020: val_acc did not improve from 0.93970\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.1179 - acc: 0.9586 - val_loss: 0.1906 - val_acc: 0.9378\n",
            "Epoch 21/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.1172 - acc: 0.9591\n",
            "Epoch 00021: val_acc improved from 0.93970 to 0.94060, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 567us/sample - loss: 0.1174 - acc: 0.9591 - val_loss: 0.1963 - val_acc: 0.9406\n",
            "Epoch 22/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.1122 - acc: 0.9609\n",
            "Epoch 00022: val_acc improved from 0.94060 to 0.94110, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 575us/sample - loss: 0.1123 - acc: 0.9609 - val_loss: 0.1944 - val_acc: 0.9411\n",
            "Epoch 23/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.1084 - acc: 0.9628\n",
            "Epoch 00023: val_acc did not improve from 0.94110\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.1086 - acc: 0.9627 - val_loss: 0.2044 - val_acc: 0.9395\n",
            "Epoch 24/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9635\n",
            "Epoch 00024: val_acc did not improve from 0.94110\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.1032 - acc: 0.9635 - val_loss: 0.2017 - val_acc: 0.9408\n",
            "Epoch 25/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9637\n",
            "Epoch 00025: val_acc improved from 0.94110 to 0.94210, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 566us/sample - loss: 0.1034 - acc: 0.9637 - val_loss: 0.1942 - val_acc: 0.9421\n",
            "Epoch 26/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0966 - acc: 0.9662\n",
            "Epoch 00026: val_acc did not improve from 0.94210\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0966 - acc: 0.9662 - val_loss: 0.1971 - val_acc: 0.9395\n",
            "Epoch 27/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0969 - acc: 0.9662\n",
            "Epoch 00027: val_acc did not improve from 0.94210\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0968 - acc: 0.9662 - val_loss: 0.2130 - val_acc: 0.9404\n",
            "Epoch 28/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0935 - acc: 0.9668\n",
            "Epoch 00028: val_acc did not improve from 0.94210\n",
            "50000/50000 [==============================] - 29s 572us/sample - loss: 0.0937 - acc: 0.9668 - val_loss: 0.2002 - val_acc: 0.9420\n",
            "Epoch 29/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0925 - acc: 0.9671\n",
            "Epoch 00029: val_acc improved from 0.94210 to 0.94310, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 31s 613us/sample - loss: 0.0924 - acc: 0.9671 - val_loss: 0.2053 - val_acc: 0.9431\n",
            "Epoch 30/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0913 - acc: 0.9687\n",
            "Epoch 00030: val_acc improved from 0.94310 to 0.94320, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 29s 582us/sample - loss: 0.0913 - acc: 0.9687 - val_loss: 0.2079 - val_acc: 0.9432\n",
            "Epoch 31/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9706\n",
            "Epoch 00031: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0855 - acc: 0.9705 - val_loss: 0.2140 - val_acc: 0.9388\n",
            "Epoch 32/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9700\n",
            "Epoch 00032: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 28s 566us/sample - loss: 0.0857 - acc: 0.9700 - val_loss: 0.1981 - val_acc: 0.9418\n",
            "Epoch 33/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0831 - acc: 0.9705\n",
            "Epoch 00033: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 29s 573us/sample - loss: 0.0833 - acc: 0.9705 - val_loss: 0.2047 - val_acc: 0.9411\n",
            "Epoch 34/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9713\n",
            "Epoch 00034: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 29s 579us/sample - loss: 0.0836 - acc: 0.9713 - val_loss: 0.2145 - val_acc: 0.9367\n",
            "Epoch 35/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0793 - acc: 0.9718\n",
            "Epoch 00035: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 29s 586us/sample - loss: 0.0793 - acc: 0.9717 - val_loss: 0.2171 - val_acc: 0.9419\n",
            "Epoch 36/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0761 - acc: 0.9730\n",
            "Epoch 00036: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 29s 576us/sample - loss: 0.0760 - acc: 0.9730 - val_loss: 0.2233 - val_acc: 0.9427\n",
            "Epoch 37/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0788 - acc: 0.9733\n",
            "Epoch 00037: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 29s 586us/sample - loss: 0.0789 - acc: 0.9733 - val_loss: 0.2177 - val_acc: 0.9423\n",
            "Epoch 38/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9746\n",
            "Epoch 00038: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 29s 579us/sample - loss: 0.0733 - acc: 0.9746 - val_loss: 0.2386 - val_acc: 0.9419\n",
            "Epoch 39/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0711 - acc: 0.9756\n",
            "Epoch 00039: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 29s 570us/sample - loss: 0.0711 - acc: 0.9756 - val_loss: 0.2182 - val_acc: 0.9430\n",
            "Epoch 40/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9765\n",
            "Epoch 00040: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0709 - acc: 0.9765 - val_loss: 0.2465 - val_acc: 0.9405\n",
            "Epoch 41/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9752\n",
            "Epoch 00041: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0713 - acc: 0.9752 - val_loss: 0.2073 - val_acc: 0.9419\n",
            "Epoch 42/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.9766\n",
            "Epoch 00042: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 30s 592us/sample - loss: 0.0660 - acc: 0.9766 - val_loss: 0.2200 - val_acc: 0.9407\n",
            "Epoch 43/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9775\n",
            "Epoch 00043: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 29s 578us/sample - loss: 0.0670 - acc: 0.9775 - val_loss: 0.2290 - val_acc: 0.9405\n",
            "Epoch 44/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9778\n",
            "Epoch 00044: val_acc did not improve from 0.94320\n",
            "50000/50000 [==============================] - 29s 589us/sample - loss: 0.0657 - acc: 0.9778 - val_loss: 0.2287 - val_acc: 0.9424\n",
            "Epoch 45/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0649 - acc: 0.9773\n",
            "Epoch 00045: val_acc improved from 0.94320 to 0.94390, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 568us/sample - loss: 0.0649 - acc: 0.9773 - val_loss: 0.2098 - val_acc: 0.9439\n",
            "Epoch 46/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9777\n",
            "Epoch 00046: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0643 - acc: 0.9776 - val_loss: 0.2011 - val_acc: 0.9420\n",
            "Epoch 47/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9788\n",
            "Epoch 00047: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0625 - acc: 0.9788 - val_loss: 0.2199 - val_acc: 0.9428\n",
            "Epoch 48/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9801\n",
            "Epoch 00048: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0570 - acc: 0.9801 - val_loss: 0.2472 - val_acc: 0.9417\n",
            "Epoch 49/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9802\n",
            "Epoch 00049: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 29s 571us/sample - loss: 0.0572 - acc: 0.9802 - val_loss: 0.2278 - val_acc: 0.9429\n",
            "Epoch 50/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9798\n",
            "Epoch 00050: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 30s 595us/sample - loss: 0.0601 - acc: 0.9798 - val_loss: 0.2329 - val_acc: 0.9439\n",
            "Epoch 51/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9808\n",
            "Epoch 00051: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0566 - acc: 0.9807 - val_loss: 0.2457 - val_acc: 0.9379\n",
            "Epoch 52/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9816\n",
            "Epoch 00052: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0540 - acc: 0.9815 - val_loss: 0.2469 - val_acc: 0.9437\n",
            "Epoch 53/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9807\n",
            "Epoch 00053: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0559 - acc: 0.9807 - val_loss: 0.2443 - val_acc: 0.9429\n",
            "Epoch 54/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9811\n",
            "Epoch 00054: val_acc did not improve from 0.94390\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0544 - acc: 0.9811 - val_loss: 0.2461 - val_acc: 0.9402\n",
            "Epoch 55/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9811\n",
            "Epoch 00055: val_acc improved from 0.94390 to 0.94470, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0572 - acc: 0.9811 - val_loss: 0.2517 - val_acc: 0.9447\n",
            "Epoch 56/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9818\n",
            "Epoch 00056: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0543 - acc: 0.9818 - val_loss: 0.2443 - val_acc: 0.9440\n",
            "Epoch 57/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9830\n",
            "Epoch 00057: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0510 - acc: 0.9830 - val_loss: 0.2399 - val_acc: 0.9433\n",
            "Epoch 58/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9832\n",
            "Epoch 00058: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0508 - acc: 0.9833 - val_loss: 0.2563 - val_acc: 0.9429\n",
            "Epoch 59/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9829\n",
            "Epoch 00059: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0500 - acc: 0.9829 - val_loss: 0.2501 - val_acc: 0.9426\n",
            "Epoch 60/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9824\n",
            "Epoch 00060: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0515 - acc: 0.9824 - val_loss: 0.2404 - val_acc: 0.9417\n",
            "Epoch 61/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9838\n",
            "Epoch 00061: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0474 - acc: 0.9838 - val_loss: 0.2579 - val_acc: 0.9435\n",
            "Epoch 62/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9831\n",
            "Epoch 00062: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0495 - acc: 0.9831 - val_loss: 0.2790 - val_acc: 0.9434\n",
            "Epoch 63/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9838\n",
            "Epoch 00063: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0492 - acc: 0.9838 - val_loss: 0.2560 - val_acc: 0.9421\n",
            "Epoch 64/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9839\n",
            "Epoch 00064: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0501 - acc: 0.9839 - val_loss: 0.2789 - val_acc: 0.9421\n",
            "Epoch 65/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9834\n",
            "Epoch 00065: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0492 - acc: 0.9833 - val_loss: 0.2555 - val_acc: 0.9431\n",
            "Epoch 66/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9836\n",
            "Epoch 00066: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0497 - acc: 0.9835 - val_loss: 0.2804 - val_acc: 0.9430\n",
            "Epoch 67/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9843\n",
            "Epoch 00067: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0460 - acc: 0.9842 - val_loss: 0.2685 - val_acc: 0.9435\n",
            "Epoch 68/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9849\n",
            "Epoch 00068: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0463 - acc: 0.9849 - val_loss: 0.2690 - val_acc: 0.9377\n",
            "Epoch 69/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9845\n",
            "Epoch 00069: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0465 - acc: 0.9845 - val_loss: 0.2722 - val_acc: 0.9419\n",
            "Epoch 70/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9843\n",
            "Epoch 00070: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0471 - acc: 0.9844 - val_loss: 0.2859 - val_acc: 0.9435\n",
            "Epoch 71/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9859\n",
            "Epoch 00071: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 555us/sample - loss: 0.0438 - acc: 0.9859 - val_loss: 0.3063 - val_acc: 0.9414\n",
            "Epoch 72/100\n",
            "49888/50000 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9847\n",
            "Epoch 00072: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0437 - acc: 0.9847 - val_loss: 0.2815 - val_acc: 0.9406\n",
            "Epoch 73/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9847\n",
            "Epoch 00073: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 557us/sample - loss: 0.0451 - acc: 0.9847 - val_loss: 0.2833 - val_acc: 0.9419\n",
            "Epoch 74/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9868\n",
            "Epoch 00074: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0407 - acc: 0.9868 - val_loss: 0.2749 - val_acc: 0.9438\n",
            "Epoch 75/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9865\n",
            "Epoch 00075: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0410 - acc: 0.9865 - val_loss: 0.2746 - val_acc: 0.9427\n",
            "Epoch 76/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9865\n",
            "Epoch 00076: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0411 - acc: 0.9865 - val_loss: 0.2769 - val_acc: 0.9410\n",
            "Epoch 77/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9866\n",
            "Epoch 00077: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0407 - acc: 0.9866 - val_loss: 0.2936 - val_acc: 0.9421\n",
            "Epoch 78/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9876\n",
            "Epoch 00078: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0373 - acc: 0.9876 - val_loss: 0.2832 - val_acc: 0.9440\n",
            "Epoch 79/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9868\n",
            "Epoch 00079: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0396 - acc: 0.9868 - val_loss: 0.2966 - val_acc: 0.9419\n",
            "Epoch 80/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9877\n",
            "Epoch 00080: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0375 - acc: 0.9877 - val_loss: 0.2959 - val_acc: 0.9415\n",
            "Epoch 81/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9863\n",
            "Epoch 00081: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 566us/sample - loss: 0.0395 - acc: 0.9863 - val_loss: 0.2949 - val_acc: 0.9425\n",
            "Epoch 82/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9875\n",
            "Epoch 00082: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0379 - acc: 0.9875 - val_loss: 0.3056 - val_acc: 0.9400\n",
            "Epoch 83/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9885\n",
            "Epoch 00083: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0357 - acc: 0.9884 - val_loss: 0.2898 - val_acc: 0.9417\n",
            "Epoch 84/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9877\n",
            "Epoch 00084: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0371 - acc: 0.9877 - val_loss: 0.3277 - val_acc: 0.9411\n",
            "Epoch 85/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9877\n",
            "Epoch 00085: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 569us/sample - loss: 0.0373 - acc: 0.9877 - val_loss: 0.2890 - val_acc: 0.9430\n",
            "Epoch 86/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9876\n",
            "Epoch 00086: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 558us/sample - loss: 0.0370 - acc: 0.9876 - val_loss: 0.3016 - val_acc: 0.9415\n",
            "Epoch 87/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9879\n",
            "Epoch 00087: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0373 - acc: 0.9879 - val_loss: 0.2821 - val_acc: 0.9429\n",
            "Epoch 88/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9873\n",
            "Epoch 00088: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0391 - acc: 0.9873 - val_loss: 0.2691 - val_acc: 0.9432\n",
            "Epoch 89/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9873\n",
            "Epoch 00089: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 565us/sample - loss: 0.0394 - acc: 0.9873 - val_loss: 0.2700 - val_acc: 0.9439\n",
            "Epoch 90/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9876\n",
            "Epoch 00090: val_acc did not improve from 0.94470\n",
            "50000/50000 [==============================] - 28s 562us/sample - loss: 0.0375 - acc: 0.9877 - val_loss: 0.2865 - val_acc: 0.9439\n",
            "Epoch 91/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9882\n",
            "Epoch 00091: val_acc improved from 0.94470 to 0.94560, saving model to model_3.weights.hdf5\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0365 - acc: 0.9882 - val_loss: 0.2719 - val_acc: 0.9456\n",
            "Epoch 92/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9884\n",
            "Epoch 00092: val_acc did not improve from 0.94560\n",
            "50000/50000 [==============================] - 28s 563us/sample - loss: 0.0344 - acc: 0.9884 - val_loss: 0.2713 - val_acc: 0.9434\n",
            "Epoch 93/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9878\n",
            "Epoch 00093: val_acc did not improve from 0.94560\n",
            "50000/50000 [==============================] - 28s 564us/sample - loss: 0.0367 - acc: 0.9878 - val_loss: 0.2571 - val_acc: 0.9435\n",
            "Epoch 94/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9891\n",
            "Epoch 00094: val_acc did not improve from 0.94560\n",
            "50000/50000 [==============================] - 28s 567us/sample - loss: 0.0333 - acc: 0.9891 - val_loss: 0.3010 - val_acc: 0.9445\n",
            "Epoch 95/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9881\n",
            "Epoch 00095: val_acc did not improve from 0.94560\n",
            "50000/50000 [==============================] - 28s 561us/sample - loss: 0.0367 - acc: 0.9881 - val_loss: 0.2978 - val_acc: 0.9434\n",
            "Epoch 96/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9889\n",
            "Epoch 00096: val_acc did not improve from 0.94560\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0348 - acc: 0.9889 - val_loss: 0.2777 - val_acc: 0.9453\n",
            "Epoch 97/100\n",
            "49984/50000 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9887\n",
            "Epoch 00097: val_acc did not improve from 0.94560\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0351 - acc: 0.9887 - val_loss: 0.3071 - val_acc: 0.9441\n",
            "Epoch 98/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9884\n",
            "Epoch 00098: val_acc did not improve from 0.94560\n",
            "50000/50000 [==============================] - 28s 560us/sample - loss: 0.0356 - acc: 0.9885 - val_loss: 0.2986 - val_acc: 0.9405\n",
            "Epoch 99/100\n",
            "49952/50000 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9892\n",
            "Epoch 00099: val_acc did not improve from 0.94560\n",
            "50000/50000 [==============================] - 28s 559us/sample - loss: 0.0337 - acc: 0.9892 - val_loss: 0.2912 - val_acc: 0.9437\n",
            "Epoch 100/100\n",
            "49920/50000 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9893\n",
            "Epoch 00100: val_acc did not improve from 0.94560\n",
            "50000/50000 [==============================] - 28s 556us/sample - loss: 0.0347 - acc: 0.9893 - val_loss: 0.2961 - val_acc: 0.9439\n",
            "\n",
            "Time to train classifier: 2838.25 seconds\n",
            "\n",
            "model_3 Test accuracy = 93.90%\n",
            "\n",
            "\n",
            "model_3 Gestalt Test accuracy = 94.34%\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njRIsIiKiUYJFREQ0SrCIiIhG/wBcdqBPzL9d/wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxKBGGNXmW_9"
      },
      "source": [
        "#### Show results on model accuracy, cumulative accuracy, and training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eCQcAA4rgyI",
        "outputId": "0f28ea2a-ec8a-4418-fa14-af4e30e53243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "result = pd.DataFrame(result, \n",
        "                      columns=['model', 'accuracy', 'cum. accuracy', 'time'])\n",
        "result"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>cum. accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>model_1</td>\n",
              "      <td>0.9380</td>\n",
              "      <td>0.9380</td>\n",
              "      <td>2810.556279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>model_2</td>\n",
              "      <td>0.9374</td>\n",
              "      <td>0.9411</td>\n",
              "      <td>2832.492863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>model_3</td>\n",
              "      <td>0.9390</td>\n",
              "      <td>0.9434</td>\n",
              "      <td>2838.253579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     model  accuracy  cum. accuracy         time\n",
              "0  model_1    0.9380         0.9380  2810.556279\n",
              "1  model_2    0.9374         0.9411  2832.492863\n",
              "2  model_3    0.9390         0.9434  2838.253579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ezgrLIdYmuC-"
      },
      "source": [
        "#### specify label for classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q6IC3VTMnTDr",
        "colab": {}
      },
      "source": [
        "items = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', \n",
        "         'Shirt', 'Sneaker', 'Bag', 'Ankle boot'] # labels\n",
        "item = dict(zip(range(10), items)) # create dictionary mapping class to labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Gljkbcm3nBBc"
      },
      "source": [
        "#### Show confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F97agyFxsgVi",
        "outputId": "51cfa94d-3c01-442c-8b74-97a78fa86181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "cm = metrics.confusion_matrix(y_test, predicted)\n",
        "cm = pd.DataFrame(cm, columns=items)\n",
        "cm.insert(0,\"True\",items)\n",
        "cm"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>True</th>\n",
              "      <th>T-shirt/top</th>\n",
              "      <th>Trouser</th>\n",
              "      <th>Pullover</th>\n",
              "      <th>Dress</th>\n",
              "      <th>Coat</th>\n",
              "      <th>Sandal</th>\n",
              "      <th>Shirt</th>\n",
              "      <th>Sneaker</th>\n",
              "      <th>Bag</th>\n",
              "      <th>Ankle boot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>T-shirt/top</td>\n",
              "      <td>878</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Trouser</td>\n",
              "      <td>2</td>\n",
              "      <td>989</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pullover</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>930</td>\n",
              "      <td>5</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dress</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>948</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Coat</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>936</td>\n",
              "      <td>0</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Sandal</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>990</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Shirt</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "      <td>37</td>\n",
              "      <td>22</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>816</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Sneaker</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>986</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Bag</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>990</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Ankle boot</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          True  T-shirt/top  Trouser  Pullover  ...  Shirt  Sneaker  Bag  Ankle boot\n",
              "0  T-shirt/top          878        0        18  ...     89        0    1           0\n",
              "1      Trouser            2      989         0  ...      2        0    1           0\n",
              "2     Pullover           11        1       930  ...     26        0    0           0\n",
              "3        Dress            9        0         6  ...     15        0    0           0\n",
              "4         Coat            1        0        19  ...     29        0    0           0\n",
              "5       Sandal            0        0         0  ...      0        9    0           1\n",
              "6        Shirt           69        0        37  ...    816        0    4           0\n",
              "7      Sneaker            0        0         0  ...      0      986    0          12\n",
              "8          Bag            3        1         0  ...      1        0  990           0\n",
              "9   Ankle boot            0        0         0  ...      0       27    1         971\n",
              "\n",
              "[10 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9DNkiXv0nJsz"
      },
      "source": [
        "#### Show classification report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e2HSBqb_p6FH",
        "outputId": "e7e65030-99d0-4b72-ae8b-c14850ca57fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "print(metrics.classification_report([items[i] for i in y_test], \n",
        "                                    [items[i] for i in predicted]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Ankle boot       0.99      0.97      0.98      1000\n",
            "         Bag       0.99      0.99      0.99      1000\n",
            "        Coat       0.90      0.94      0.92      1000\n",
            "       Dress       0.94      0.95      0.94      1000\n",
            "    Pullover       0.92      0.93      0.93      1000\n",
            "      Sandal       0.99      0.99      0.99      1000\n",
            "       Shirt       0.83      0.82      0.83      1000\n",
            "     Sneaker       0.96      0.99      0.98      1000\n",
            " T-shirt/top       0.90      0.88      0.89      1000\n",
            "     Trouser       1.00      0.99      0.99      1000\n",
            "\n",
            "   micro avg       0.94      0.94      0.94     10000\n",
            "   macro avg       0.94      0.94      0.94     10000\n",
            "weighted avg       0.94      0.94      0.94     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FHEZASPVnWEd"
      },
      "source": [
        "#### Use mode class for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YoQi3UtQKa4G",
        "outputId": "ec90c8a4-d7ad-4dda-a8a6-9f10e2610cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "predClass = pd.DataFrame()\n",
        "for i in range(nModels):\n",
        "  model = 'model_'+str(i+1)\n",
        "  print(\"processing \" + model)\n",
        "  bestWts = model+\".weights.hdf5\" # best weights file\n",
        "  mod.load_weights(bestWts) # load best weights\n",
        "  prob = mod.predict(x_test) # predict probabilities for test examples\n",
        "  predClass[model] = prob.argmax(axis=1) # mst likely class\n",
        "  \n",
        "modeClass = predClass.mode(axis=1)\n",
        "modeAcc = metrics.accuracy_score(y_test, modeClass[0])\n",
        "print(\"Accuracy based on mode class = %4.2f%%\" %(100.0*modeAcc))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing model_1\n",
            "processing model_2\n",
            "processing model_3\n",
            "Accuracy based on mode class = 94.30%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZgxxR0dI6hEM"
      },
      "source": [
        "#### Compare accuracy for easy and hard classes\n",
        "- Note that 'T-shirt/top', 'Pullover', 'Coat', and  'Shirt' are harder to classify than other items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HWjc3ZiV6ppb",
        "outputId": "52f9eb3e-cb3f-4057-ce2c-2047f0d4d07f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "hardClasses = [0,2,4,6] # classes with low classification accuracy\n",
        "hardIndxTest = [i for i in range(len(y_test)) if y_test[i] in hardClasses]\n",
        "easyIndxTest = [i for i in range(len(y_test)) if y_test[i] not in hardClasses]\n",
        "print('Test data contains %d hard and %d easy examples' \n",
        "      %(len(hardIndxTest), len(easyIndxTest)))\n",
        "\n",
        "accEasy = metrics.accuracy_score(y_test[easyIndxTest], predicted[easyIndxTest])\n",
        "accHard = metrics.accuracy_score(y_test[hardIndxTest], predicted[hardIndxTest])\n",
        "\n",
        "print('Test accuracy with easy examples = %4.2f%%' %(100.0*accEasy)) \n",
        "print('Test accuracy with hard examples = %4.2f%%' %(100.0*accHard)) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data contains 4000 hard and 6000 easy examples\n",
            "Test accuracy with easy examples = 97.90%\n",
            "Test accuracy with hard examples = 89.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nyOUsvAGnjRv"
      },
      "source": [
        "#### Observations:\n",
        "\n",
        "1.   A simple CNN can achieve classification accuracy of over 93%\n",
        "2.   Combining 3 models improves accuracy around 94.4%\n",
        "\n",
        "1. It takes around 16 seconds per epoch using Colaboratory GPU accelerator and Test accuracy does not improve significantly after the first 20 epochs\n",
        "\n",
        "1.   Combining a few more models trained over 20 epochs may further improve classification accuracy in a resonable amount of time.\n",
        "\n",
        "1.   Classification accuracy is significantly lower for 4 classes: 'T-shirt/top', 'Pullover', 'Coat', and 'Shirt' \n",
        "\n",
        "\n",
        "\n",
        "#### Opportunities for improvement:\n",
        "\n",
        "\n",
        "1.   Devise alternate methods for combining models\n",
        "2.   Increase the diversity of constituent models\n",
        "\n",
        "1.   Introduce regularization methods that prevent over-fitting beyond 20 epochs\n",
        "2.   Develop a two-phased approach:  Predict using a combination of models in the first phase and use a separate model to re-classify examples predicted as 'T-shirt/top', 'Pullover', 'Coat', or 'Shirt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}