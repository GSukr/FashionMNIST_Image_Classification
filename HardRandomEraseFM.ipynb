{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i36-27v6-55X"
   },
   "source": [
    "### Image classification using Fashion MNIST data set\n",
    "#### Classification of hard items using random erase for regularization\n",
    "\n",
    "Results indicate that 4 of the item classes - 0: T-shirt/top, 2: Pullover, 4: Coat, and 6: Shirt - are harder to classify; \n",
    "test accuracy for these items are about 89%, while other items are classified with about 98% accuracy. \n",
    "\n",
    "In this notebook we use random erase method for regularization and train 5 models for 100 epochs to investigate whether classification accuracy for these hard classes can be improved.\n",
    "\n",
    "We find that this approach increases classification accuracy for these hard to classify items from 89% to 93.5%\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WNwY3B16-1Gg"
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import os # for file handling\n",
    "import pandas as pd # for data handling\n",
    "import numpy as np # for linear algebra\n",
    "import time # to time runs\n",
    "import matplotlib.pyplot as plt # to display plots\n",
    "from sklearn import metrics # to evaluate classification accuracy\n",
    "from sklearn.model_selection import train_test_split # for validation data\n",
    "import tensorflow as tf # for neural networks\n",
    "# import from keras:\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "# for on-line image generation (data augmentation)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zI3VZ14_RJi"
   },
   "source": [
    "#### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "HsXcR-Mk_XLJ",
    "outputId": "dc38da3e-1cdf-4f81-ce16-73c792c4727c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28) , y_train shape: (60000,)\n",
      "x_test shape: (10000, 28, 28) , y_test shape: (10000,)\n",
      "Number of classes:  10\n"
     ]
    }
   ],
   "source": [
    "# get fashion mnist data\n",
    "(x_train,y_train), (x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# show shapes of tensors\n",
    "print(\"x_train shape:\", x_train.shape, \", y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape, \", y_test shape:\", y_test.shape)\n",
    "\n",
    "# get number of classes\n",
    "nClasses = len(np.unique(y_train)) # number of output classes\n",
    "print(\"Number of classes: \", nClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_L5TGhMVVkXW"
   },
   "source": [
    "#### Isolate hard cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "6mJz5ch4Vpvp",
    "outputId": "f1cc7a8e-b059-45db-d196-533f447ef542"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 T-shirt/top\n",
      "1 Trouser\n",
      "2 Pullover\n",
      "3 Dress\n",
      "4 Coat\n",
      "5 Sandal\n",
      "6 Shirt\n",
      "7 Sneaker\n",
      "8 Bag\n",
      "9 Ankle boot\n",
      "Train data contains 24000 and test data contains 4000 hard examples\n"
     ]
    }
   ],
   "source": [
    "# specify label for classes\n",
    "items = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', \n",
    "         'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "item = dict(zip(range(10), items))\n",
    "for i in item:\n",
    "  print(i, item[i])\n",
    "  \n",
    "hardClasses = [0,2,4,6] # classes with low classification accuracy\n",
    "\n",
    "# get indices of hard examplesin training and test data\n",
    "hardIndxTrain = [i for i in range(len(y_train)) if y_train[i] in hardClasses]\n",
    "hardIndxTest = [i for i in range(len(y_test)) if y_test[i] in hardClasses]\n",
    "print('Train data contains %d and test data contains %d hard examples' \n",
    "      %(len(hardIndxTrain), len(hardIndxTest)))\n",
    "\n",
    "# retain only hard examples\n",
    "x_train, y_train = x_train[hardIndxTrain], y_train[hardIndxTrain]\n",
    "x_test, y_test = x_train[hardIndxTest], y_train[hardIndxTest]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmWdp8OfXZgh"
   },
   "source": [
    "#### Map old labels to new labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZDgY9kQSXf29",
    "outputId": "04500019-51c4-461f-d7d0-877f2612e5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  4\n"
     ]
    }
   ],
   "source": [
    "newClass = {0:0, 2:1, 4:2, 6:3} # new labels \n",
    "oldClass = {0:0, 1:2, 2:4, 3:6} # recover old labels\n",
    "\n",
    "y_train = [newClass[c] for c in y_train]\n",
    "y_test = [newClass[c] for c in y_test]\n",
    "\n",
    "# get number of classes\n",
    "nClasses = len(np.unique(y_train)) # number of output classes\n",
    "print(\"Number of classes: \", nClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "po8pulDO_tTs"
   },
   "source": [
    "#### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WcmFSglxBSUx",
    "outputId": "bf46a242-fdf1-4f2c-d388-bc668c7b34ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (20000, 28, 28, 1) , y_train shape: (20000, 4)\n",
      "x_valid shape: (4000, 28, 28, 1) , y_valid shape: (4000, 4)\n",
      "x_test shape: (4000, 28, 28, 1) , y_test shape: (4000, 4)\n",
      "Image height = 28, image width = 28, number of channels = 1\n"
     ]
    }
   ],
   "source": [
    "# normalize grayscale pixel values (0-255) to (0,1)\n",
    "x_train = x_train.astype('float32')/255 # normalized training inputs\n",
    "x_test = x_test.astype('float32')/255 # normalized test inputs\n",
    "\n",
    "# reshape to needed input shape for network\n",
    "x_train, x_test = x_train.reshape((-1,28,28,1)), x_test.reshape((-1,28,28,1))\n",
    "input_shape = x_train.shape[1:] # input shape for network\n",
    "\n",
    "# reserve some training data for validation\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train, y_train, test_size=1/6) # 1/6 reserved for validation\n",
    "\n",
    "# reshape output to output one-hot representation needed for network\n",
    "y_train = tf.keras.utils.to_categorical(y_train, nClasses)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, nClasses)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, nClasses)\n",
    "\n",
    "# show shapes of re-shaped tensors\n",
    "print(\"x_train shape:\", x_train.shape, \", y_train shape:\", y_train.shape)\n",
    "print(\"x_valid shape:\", x_valid.shape, \", y_valid shape:\", y_valid.shape)\n",
    "print(\"x_test shape:\", x_test.shape, \", y_test shape:\", y_test.shape)\n",
    "\n",
    "# get image dimensions\n",
    "img_h, img_w, img_channels = x_train.shape[1:] # size of image\n",
    "print(\"Image height = %d, image width = %d, number of channels = %d\" \n",
    "      %(img_h, img_w, img_channels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0K3vO6EOFHU"
   },
   "source": [
    "#### Define function to create Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WLSvi77CHSo"
   },
   "outputs": [],
   "source": [
    "def convNN(model, ch1, ch2, kernel, pool, nDense, drop, dropDense):\n",
    "  \"\"\"returns CNN model with specified architecture and parameters\"\"\"\n",
    "\n",
    "  model = tf.keras.models.Sequential() # create model                \n",
    "\n",
    "  # first CONV => RELU => CONV => RELU => POOL layer set\n",
    "  model.add(Conv2D(ch1, kernel, padding=\"same\", \n",
    "                   activation=\"relu\", input_shape=input_shape))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Conv2D(ch1, kernel, padding=\"same\", activation=\"relu\"))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=pool))\n",
    "  model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "  # second CONV => RELU => CONV => RELU => POOL layer set\n",
    "  model.add(Conv2D(ch2, kernel, padding=\"same\", \n",
    "                   activation=\"relu\", input_shape=input_shape))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Conv2D(ch2, kernel, padding=\"same\", activation=\"relu\"))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(MaxPooling2D(pool_size=pool))\n",
    "  model.add(tf.keras.layers.Dropout(drop))\n",
    "\n",
    "  model.add(tf.keras.layers.Flatten()) \n",
    "\n",
    "  # FC => RELU layers\n",
    "  model.add(tf.keras.layers.Dense(nDense, activation='relu'))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(tf.keras.layers.Dropout(dropDense))\n",
    "\n",
    "  # output softmax layer\n",
    "  model.add(tf.keras.layers.Dense(nClasses, activation='softmax'))\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                optimizer=tf.keras.optimizers.Adadelta(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fq-Q2dpoO4cL"
   },
   "source": [
    "#### Specify parameters for convolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "4g2WZxFuBjJT",
    "outputId": "4d03113b-e701-494c-d397-452e071f9e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 1,676,004\n",
      "Trainable params: 1,674,596\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parameters for CNN models (change as desired)\n",
    "ch1, ch2 = 32, 64 # number of output channels\n",
    "kernel = (3,3) # filter shape\n",
    "pool = (2,2) # max pool size\n",
    "nDense = 512 # dense layer size\n",
    "drop, dropDense = 0.25, 0.5\n",
    "\n",
    "# create model\n",
    "mod = convNN('model', ch1, ch2, kernel, pool, nDense, drop, dropDense)\n",
    "mod.summary() # show model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnf88LMBDUVX"
   },
   "source": [
    "#### Define function for data augmentation and regularization through random erasing\n",
    "- Randomly selected rectangular sections are erased from images\n",
    "- Souce: https://github.com/yu4u/cutout-random-erasing/blob/master/random_eraser.py\n",
    "-  Implements method described in:\n",
    "\n",
    "> -   T. DeVries and G. W. Taylor, \"Improved Regularization of Convolutional Neural Networks with Cutout,\" in arXiv:1708.04552, 2017.\n",
    "\n",
    "> -    Z. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang, \"Random Erasing Data Augmentation,\" in arXiv:1708.04896, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pfv2O0LyAXcI"
   },
   "outputs": [],
   "source": [
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, \n",
    "                      v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FnVUFC2_LfHo"
   },
   "source": [
    "#### Create real time data generator for images with random erasing\n",
    "- > https://keras.io/preprocessing/image/#imagedatagenerator-class\n",
    "- > see: https://github.com/yu4u/cutout-random-erasing/blob/master/cifar10_resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5p5wqHERChUT"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # do not rotate images\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally by 0.1 of width\n",
    "    height_shift_range=0.1,  # randomly shift images vertically by 0.1 of height\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=False))\n",
    "\n",
    "# Compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2OuTLB9PCfR"
   },
   "source": [
    "#### Define function to plot accuracy with training and validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0GZk-cMCMj1"
   },
   "outputs": [],
   "source": [
    "def plotHistorty(model, history):\n",
    "    # plot history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title(model+' accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ogw2xbybRRCa"
   },
   "source": [
    "#### Specify parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RrqxeEQyRyP5"
   },
   "outputs": [],
   "source": [
    "batchSize = 32 # batch size for training\n",
    "epochs = 100 # number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btHHSAu3SAOM"
   },
   "source": [
    "#### Train CNN models with random erasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 44978
    },
    "colab_type": "code",
    "id": "WV66FLTYCRZ5",
    "outputId": "397f1a70-7111-43d4-b6b7-1e14e6990b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: hard_model_random_erase1\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 1s 135us/sample - loss: 0.6714 - acc: 0.7237\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72375, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 1.0958 - acc: 0.5844 - val_loss: 0.6714 - val_acc: 0.7237\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 1s 139us/sample - loss: 0.6130 - acc: 0.7500\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.72375 to 0.75000, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.7680 - acc: 0.6809 - val_loss: 0.6130 - val_acc: 0.7500\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.5188 - acc: 0.7855\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.75000 to 0.78550, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.6895 - acc: 0.7193 - val_loss: 0.5188 - val_acc: 0.7855\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.4523 - acc: 0.8155\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78550 to 0.81550, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6406 - acc: 0.7439 - val_loss: 0.4523 - val_acc: 0.8155\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.4412 - acc: 0.8278\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.81550 to 0.82775, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6038 - acc: 0.7596 - val_loss: 0.4412 - val_acc: 0.8278\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.4472 - acc: 0.8227\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82775\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5848 - acc: 0.7678 - val_loss: 0.4472 - val_acc: 0.8227\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 1s 131us/sample - loss: 0.4449 - acc: 0.8170\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82775\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5686 - acc: 0.7750 - val_loss: 0.4449 - val_acc: 0.8170\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.4326 - acc: 0.8335\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.82775 to 0.83350, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.5520 - acc: 0.7798 - val_loss: 0.4326 - val_acc: 0.8335\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.4759 - acc: 0.8060\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.83350\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5420 - acc: 0.7834 - val_loss: 0.4759 - val_acc: 0.8060\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.4031 - acc: 0.8372\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.83350 to 0.83725, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5362 - acc: 0.7886 - val_loss: 0.4031 - val_acc: 0.8372\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.5357 - acc: 0.7745\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.83725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5163 - acc: 0.7980 - val_loss: 0.5357 - val_acc: 0.7745\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3952 - acc: 0.8340\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.83725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5208 - acc: 0.7972 - val_loss: 0.3952 - val_acc: 0.8340\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.4992 - acc: 0.7985\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5048 - acc: 0.8023 - val_loss: 0.4992 - val_acc: 0.7985\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.4139 - acc: 0.8307\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.83725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5015 - acc: 0.8040 - val_loss: 0.4139 - val_acc: 0.8307\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3538 - acc: 0.8618\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.83725 to 0.86175, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4983 - acc: 0.8064 - val_loss: 0.3538 - val_acc: 0.8618\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 1s 130us/sample - loss: 0.3741 - acc: 0.8535\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86175\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4846 - acc: 0.8127 - val_loss: 0.3741 - val_acc: 0.8535\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3674 - acc: 0.8553\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86175\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4735 - acc: 0.8156 - val_loss: 0.3674 - val_acc: 0.8553\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3941 - acc: 0.8407\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86175\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4845 - acc: 0.8113 - val_loss: 0.3941 - val_acc: 0.8407\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3571 - acc: 0.8615\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86175\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4735 - acc: 0.8156 - val_loss: 0.3571 - val_acc: 0.8615\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.3573 - acc: 0.8610\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.86175\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4661 - acc: 0.8194 - val_loss: 0.3573 - val_acc: 0.8610\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.4727 - acc: 0.8155\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.86175\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4684 - acc: 0.8151 - val_loss: 0.4727 - val_acc: 0.8155\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3602 - acc: 0.8547\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86175\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4610 - acc: 0.8220 - val_loss: 0.3602 - val_acc: 0.8547\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 0s 118us/sample - loss: 0.3673 - acc: 0.8583\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86175\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4557 - acc: 0.8227 - val_loss: 0.3673 - val_acc: 0.8583\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3258 - acc: 0.8775\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.86175 to 0.87750, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.4515 - acc: 0.8253 - val_loss: 0.3258 - val_acc: 0.8775\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 1s 131us/sample - loss: 0.3287 - acc: 0.8723\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87750\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4524 - acc: 0.8267 - val_loss: 0.3287 - val_acc: 0.8723\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3256 - acc: 0.8758\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87750\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4471 - acc: 0.8278 - val_loss: 0.3256 - val_acc: 0.8758\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3478 - acc: 0.8668\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4399 - acc: 0.8317 - val_loss: 0.3478 - val_acc: 0.8668\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3294 - acc: 0.8735\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4427 - acc: 0.8314 - val_loss: 0.3294 - val_acc: 0.8735\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3446 - acc: 0.8668\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4386 - acc: 0.8337 - val_loss: 0.3446 - val_acc: 0.8668\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3699 - acc: 0.8550\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4293 - acc: 0.8345 - val_loss: 0.3699 - val_acc: 0.8550\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.4045 - acc: 0.8407\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4355 - acc: 0.8327 - val_loss: 0.4045 - val_acc: 0.8407\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3206 - acc: 0.8760\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4292 - acc: 0.8357 - val_loss: 0.3206 - val_acc: 0.8760\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3215 - acc: 0.8808\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.87750 to 0.88075, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4243 - acc: 0.8375 - val_loss: 0.3215 - val_acc: 0.8808\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 0s 123us/sample - loss: 0.3041 - acc: 0.8823\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.88075 to 0.88225, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4201 - acc: 0.8372 - val_loss: 0.3041 - val_acc: 0.8823\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3832 - acc: 0.8505\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4307 - acc: 0.8328 - val_loss: 0.3832 - val_acc: 0.8505\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 1s 134us/sample - loss: 0.3212 - acc: 0.8790\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.4154 - acc: 0.8407 - val_loss: 0.3212 - val_acc: 0.8790\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3480 - acc: 0.8660\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4159 - acc: 0.8374 - val_loss: 0.3480 - val_acc: 0.8660\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3432 - acc: 0.8652\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4206 - acc: 0.8391 - val_loss: 0.3432 - val_acc: 0.8652\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3591 - acc: 0.8608\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4125 - acc: 0.8409 - val_loss: 0.3591 - val_acc: 0.8608\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3384 - acc: 0.8627\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4181 - acc: 0.8403 - val_loss: 0.3384 - val_acc: 0.8627\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3103 - acc: 0.8800\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4169 - acc: 0.8415 - val_loss: 0.3103 - val_acc: 0.8800\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2972 - acc: 0.8805\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4103 - acc: 0.8435 - val_loss: 0.2972 - val_acc: 0.8805\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 1s 132us/sample - loss: 0.3468 - acc: 0.8680\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.4092 - acc: 0.8442 - val_loss: 0.3468 - val_acc: 0.8680\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3147 - acc: 0.8712\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3963 - acc: 0.8483 - val_loss: 0.3147 - val_acc: 0.8712\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3405 - acc: 0.8655\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.88225\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4065 - acc: 0.8434 - val_loss: 0.3405 - val_acc: 0.8655\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2992 - acc: 0.8840\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.88225 to 0.88400, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3995 - acc: 0.8482 - val_loss: 0.2992 - val_acc: 0.8840\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3297 - acc: 0.8758\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88400\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3954 - acc: 0.8496 - val_loss: 0.3297 - val_acc: 0.8758\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2896 - acc: 0.8913\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.88400 to 0.89125, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3883 - acc: 0.8515 - val_loss: 0.2896 - val_acc: 0.8913\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3202 - acc: 0.8755\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3950 - acc: 0.8496 - val_loss: 0.3202 - val_acc: 0.8755\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3089 - acc: 0.8805\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3995 - acc: 0.8475 - val_loss: 0.3089 - val_acc: 0.8805\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3373 - acc: 0.8758\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3909 - acc: 0.8504 - val_loss: 0.3373 - val_acc: 0.8758\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 1s 126us/sample - loss: 0.2875 - acc: 0.8935\n",
      "\n",
      "Epoch 00052: val_acc improved from 0.89125 to 0.89350, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3969 - acc: 0.8493 - val_loss: 0.2875 - val_acc: 0.8935\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.3188 - acc: 0.8723\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89350\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3959 - acc: 0.8511 - val_loss: 0.3188 - val_acc: 0.8723\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3302 - acc: 0.8783\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89350\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3886 - acc: 0.8551 - val_loss: 0.3302 - val_acc: 0.8783\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2986 - acc: 0.8875\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89350\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3899 - acc: 0.8516 - val_loss: 0.2986 - val_acc: 0.8875\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2892 - acc: 0.8938\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.89350 to 0.89375, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3885 - acc: 0.8512 - val_loss: 0.2892 - val_acc: 0.8938\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 1s 135us/sample - loss: 0.3079 - acc: 0.8867\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89375\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3928 - acc: 0.8515 - val_loss: 0.3079 - val_acc: 0.8867\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2763 - acc: 0.8975\n",
      "\n",
      "Epoch 00058: val_acc improved from 0.89375 to 0.89750, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3896 - acc: 0.8536 - val_loss: 0.2763 - val_acc: 0.8975\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2927 - acc: 0.8835\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3864 - acc: 0.8520 - val_loss: 0.2927 - val_acc: 0.8835\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3188 - acc: 0.8755\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3803 - acc: 0.8582 - val_loss: 0.3188 - val_acc: 0.8755\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 1s 127us/sample - loss: 0.2909 - acc: 0.8888\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3851 - acc: 0.8507 - val_loss: 0.2909 - val_acc: 0.8888\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.2972 - acc: 0.8910\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3812 - acc: 0.8547 - val_loss: 0.2972 - val_acc: 0.8910\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 0s 101us/sample - loss: 0.2931 - acc: 0.8892\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3722 - acc: 0.8579 - val_loss: 0.2931 - val_acc: 0.8892\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2988 - acc: 0.8820\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3790 - acc: 0.8569 - val_loss: 0.2988 - val_acc: 0.8820\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2803 - acc: 0.8947\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3798 - acc: 0.8558 - val_loss: 0.2803 - val_acc: 0.8947\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3013 - acc: 0.8942\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3837 - acc: 0.8529 - val_loss: 0.3013 - val_acc: 0.8942\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3450 - acc: 0.8765\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3754 - acc: 0.8595 - val_loss: 0.3450 - val_acc: 0.8765\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2866 - acc: 0.8935\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3650 - acc: 0.8614 - val_loss: 0.2866 - val_acc: 0.8935\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2988 - acc: 0.8870\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3649 - acc: 0.8594 - val_loss: 0.2988 - val_acc: 0.8870\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 1s 131us/sample - loss: 0.2942 - acc: 0.8898\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3706 - acc: 0.8580 - val_loss: 0.2942 - val_acc: 0.8898\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 1s 130us/sample - loss: 0.2786 - acc: 0.8920\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.3784 - acc: 0.8541 - val_loss: 0.2786 - val_acc: 0.8920\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2942 - acc: 0.8835\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3709 - acc: 0.8580 - val_loss: 0.2942 - val_acc: 0.8835\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3075 - acc: 0.8830\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3707 - acc: 0.8591 - val_loss: 0.3075 - val_acc: 0.8830\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.2766 - acc: 0.8888\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3748 - acc: 0.8564 - val_loss: 0.2766 - val_acc: 0.8888\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2864 - acc: 0.8932\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3729 - acc: 0.8597 - val_loss: 0.2864 - val_acc: 0.8932\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.3195 - acc: 0.8802\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3651 - acc: 0.8633 - val_loss: 0.3195 - val_acc: 0.8802\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2771 - acc: 0.8953\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3668 - acc: 0.8594 - val_loss: 0.2771 - val_acc: 0.8953\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3225 - acc: 0.8800\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3664 - acc: 0.8575 - val_loss: 0.3225 - val_acc: 0.8800\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 1s 128us/sample - loss: 0.2801 - acc: 0.8975\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3640 - acc: 0.8625 - val_loss: 0.2801 - val_acc: 0.8975\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2789 - acc: 0.8950\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3599 - acc: 0.8627 - val_loss: 0.2789 - val_acc: 0.8950\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3058 - acc: 0.8860\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3595 - acc: 0.8630 - val_loss: 0.3058 - val_acc: 0.8860\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2768 - acc: 0.8963\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3627 - acc: 0.8633 - val_loss: 0.2768 - val_acc: 0.8963\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2908 - acc: 0.8945\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3637 - acc: 0.8650 - val_loss: 0.2908 - val_acc: 0.8945\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2857 - acc: 0.8907\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3580 - acc: 0.8653 - val_loss: 0.2857 - val_acc: 0.8907\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2727 - acc: 0.8957\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3607 - acc: 0.8629 - val_loss: 0.2727 - val_acc: 0.8957\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2755 - acc: 0.8930\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3604 - acc: 0.8615 - val_loss: 0.2755 - val_acc: 0.8930\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3051 - acc: 0.8802\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3559 - acc: 0.8662 - val_loss: 0.3051 - val_acc: 0.8802\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 1s 128us/sample - loss: 0.2931 - acc: 0.8930\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3578 - acc: 0.8654 - val_loss: 0.2931 - val_acc: 0.8930\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2731 - acc: 0.8970\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3577 - acc: 0.8638 - val_loss: 0.2731 - val_acc: 0.8970\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2856 - acc: 0.8945\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89750\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3518 - acc: 0.8648 - val_loss: 0.2856 - val_acc: 0.8945\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.2769 - acc: 0.8978\n",
      "\n",
      "Epoch 00091: val_acc improved from 0.89750 to 0.89775, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3539 - acc: 0.8633 - val_loss: 0.2769 - val_acc: 0.8978\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3007 - acc: 0.8880\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3514 - acc: 0.8675 - val_loss: 0.3007 - val_acc: 0.8880\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.2902 - acc: 0.8940\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3551 - acc: 0.8671 - val_loss: 0.2902 - val_acc: 0.8940\n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2828 - acc: 0.8985\n",
      "\n",
      "Epoch 00094: val_acc improved from 0.89775 to 0.89850, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3532 - acc: 0.8676 - val_loss: 0.2828 - val_acc: 0.8985\n",
      "Epoch 95/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2780 - acc: 0.8925\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89850\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3586 - acc: 0.8651 - val_loss: 0.2780 - val_acc: 0.8925\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2898 - acc: 0.8917\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89850\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3534 - acc: 0.8658 - val_loss: 0.2898 - val_acc: 0.8917\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 1s 126us/sample - loss: 0.2750 - acc: 0.8957\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89850\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3461 - acc: 0.8694 - val_loss: 0.2750 - val_acc: 0.8957\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2796 - acc: 0.8988\n",
      "\n",
      "Epoch 00098: val_acc improved from 0.89850 to 0.89875, saving model to hard_model_random_erase1.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3526 - acc: 0.8680 - val_loss: 0.2796 - val_acc: 0.8988\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2859 - acc: 0.8930\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.89875\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3545 - acc: 0.8655 - val_loss: 0.2859 - val_acc: 0.8930\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2925 - acc: 0.8907\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.89875\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3497 - acc: 0.8676 - val_loss: 0.2925 - val_acc: 0.8907\n",
      "\n",
      "Time to train classifier: 903.72 seconds\n",
      "\n",
      "hard_model_random_erase1 Test accuracy = 91.53%\n",
      "\n",
      "\n",
      "hard_model_random_erase1 Gestalt Test accuracy = 91.53%\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: hard_model_random_erase2\n",
      "\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 1s 140us/sample - loss: 0.7176 - acc: 0.6992\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69925, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 1.0653 - acc: 0.5972 - val_loss: 0.7176 - val_acc: 0.6992\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.5549 - acc: 0.7720\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.69925 to 0.77200, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.7603 - acc: 0.6891 - val_loss: 0.5549 - val_acc: 0.7720\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.5262 - acc: 0.7845\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77200 to 0.78450, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6793 - acc: 0.7279 - val_loss: 0.5262 - val_acc: 0.7845\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.4472 - acc: 0.8217\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78450 to 0.82175, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6358 - acc: 0.7444 - val_loss: 0.4472 - val_acc: 0.8217\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 1s 130us/sample - loss: 0.4174 - acc: 0.8320\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.82175 to 0.83200, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.6081 - acc: 0.7550 - val_loss: 0.4174 - val_acc: 0.8320\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 0s 120us/sample - loss: 0.4241 - acc: 0.8292\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.83200\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.5900 - acc: 0.7626 - val_loss: 0.4241 - val_acc: 0.8292\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.4752 - acc: 0.8080\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.83200\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5687 - acc: 0.7750 - val_loss: 0.4752 - val_acc: 0.8080\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3930 - acc: 0.8407\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.83200 to 0.84075, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5520 - acc: 0.7833 - val_loss: 0.3930 - val_acc: 0.8407\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.4953 - acc: 0.7995\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.84075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5367 - acc: 0.7904 - val_loss: 0.4953 - val_acc: 0.7995\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3993 - acc: 0.8450\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.84075 to 0.84500, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5310 - acc: 0.7911 - val_loss: 0.3993 - val_acc: 0.8450\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3996 - acc: 0.8450\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.84500\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5227 - acc: 0.7959 - val_loss: 0.3996 - val_acc: 0.8450\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3998 - acc: 0.8415\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.84500\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5148 - acc: 0.8001 - val_loss: 0.3998 - val_acc: 0.8415\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3860 - acc: 0.8515\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.84500 to 0.85150, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5055 - acc: 0.8008 - val_loss: 0.3860 - val_acc: 0.8515\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3739 - acc: 0.8550\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.85150 to 0.85500, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4944 - acc: 0.8060 - val_loss: 0.3739 - val_acc: 0.8550\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 1s 140us/sample - loss: 0.3547 - acc: 0.8630\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.85500 to 0.86300, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.4912 - acc: 0.8098 - val_loss: 0.3547 - val_acc: 0.8630\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.3614 - acc: 0.8568\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86300\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4862 - acc: 0.8120 - val_loss: 0.3614 - val_acc: 0.8568\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3834 - acc: 0.8478\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86300\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4826 - acc: 0.8119 - val_loss: 0.3834 - val_acc: 0.8478\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3576 - acc: 0.8572\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86300\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4763 - acc: 0.8139 - val_loss: 0.3576 - val_acc: 0.8572\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.3414 - acc: 0.8705\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.86300 to 0.87050, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4727 - acc: 0.8162 - val_loss: 0.3414 - val_acc: 0.8705\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3431 - acc: 0.8677\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87050\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4618 - acc: 0.8253 - val_loss: 0.3431 - val_acc: 0.8677\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3493 - acc: 0.8637\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87050\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4627 - acc: 0.8211 - val_loss: 0.3493 - val_acc: 0.8637\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3641 - acc: 0.8580\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87050\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4552 - acc: 0.8228 - val_loss: 0.3641 - val_acc: 0.8580\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3158 - acc: 0.8788\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.87050 to 0.87875, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4522 - acc: 0.8271 - val_loss: 0.3158 - val_acc: 0.8788\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 1s 126us/sample - loss: 0.3265 - acc: 0.8745\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87875\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4531 - acc: 0.8256 - val_loss: 0.3265 - val_acc: 0.8745\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 1s 129us/sample - loss: 0.4566 - acc: 0.8227\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87875\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4435 - acc: 0.8291 - val_loss: 0.4566 - val_acc: 0.8227\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3396 - acc: 0.8610\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87875\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4454 - acc: 0.8274 - val_loss: 0.3396 - val_acc: 0.8610\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3224 - acc: 0.8773\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87875\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4406 - acc: 0.8317 - val_loss: 0.3224 - val_acc: 0.8773\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3188 - acc: 0.8767\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87875\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4389 - acc: 0.8291 - val_loss: 0.3188 - val_acc: 0.8767\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3145 - acc: 0.8810\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.87875 to 0.88100, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4330 - acc: 0.8326 - val_loss: 0.3145 - val_acc: 0.8810\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3220 - acc: 0.8783\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88100\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4312 - acc: 0.8336 - val_loss: 0.3220 - val_acc: 0.8783\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3235 - acc: 0.8765\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88100\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4294 - acc: 0.8355 - val_loss: 0.3235 - val_acc: 0.8765\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3079 - acc: 0.8780\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88100\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4226 - acc: 0.8368 - val_loss: 0.3079 - val_acc: 0.8780\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 1s 131us/sample - loss: 0.3169 - acc: 0.8775\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88100\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4274 - acc: 0.8365 - val_loss: 0.3169 - val_acc: 0.8775\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.3104 - acc: 0.8808\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88100\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4225 - acc: 0.8364 - val_loss: 0.3104 - val_acc: 0.8808\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.3242 - acc: 0.8745\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88100\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4163 - acc: 0.8408 - val_loss: 0.3242 - val_acc: 0.8745\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3022 - acc: 0.8852\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.88100 to 0.88525, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4158 - acc: 0.8375 - val_loss: 0.3022 - val_acc: 0.8852\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3418 - acc: 0.8675\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88525\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4158 - acc: 0.8395 - val_loss: 0.3418 - val_acc: 0.8675\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2997 - acc: 0.8835\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88525\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4138 - acc: 0.8418 - val_loss: 0.2997 - val_acc: 0.8835\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3261 - acc: 0.8783\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88525\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4053 - acc: 0.8448 - val_loss: 0.3261 - val_acc: 0.8783\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 1s 139us/sample - loss: 0.3031 - acc: 0.8882\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.88525 to 0.88825, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4122 - acc: 0.8436 - val_loss: 0.3031 - val_acc: 0.8882\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3512 - acc: 0.8750\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88825\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4024 - acc: 0.8438 - val_loss: 0.3512 - val_acc: 0.8750\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 1s 133us/sample - loss: 0.3067 - acc: 0.8838\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88825\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3988 - acc: 0.8472 - val_loss: 0.3067 - val_acc: 0.8838\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3032 - acc: 0.8895\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.88825 to 0.88950, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4071 - acc: 0.8444 - val_loss: 0.3032 - val_acc: 0.8895\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3072 - acc: 0.8820\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.88950\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4089 - acc: 0.8448 - val_loss: 0.3072 - val_acc: 0.8820\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2899 - acc: 0.8898\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.88950 to 0.88975, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3979 - acc: 0.8490 - val_loss: 0.2899 - val_acc: 0.8898\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2994 - acc: 0.8850\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.88975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3941 - acc: 0.8485 - val_loss: 0.2994 - val_acc: 0.8850\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3243 - acc: 0.8710\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3916 - acc: 0.8510 - val_loss: 0.3243 - val_acc: 0.8710\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.2938 - acc: 0.8885\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3875 - acc: 0.8490 - val_loss: 0.2938 - val_acc: 0.8885\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3043 - acc: 0.8880\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3885 - acc: 0.8507 - val_loss: 0.3043 - val_acc: 0.8880\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3232 - acc: 0.8755\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88975\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3884 - acc: 0.8512 - val_loss: 0.3232 - val_acc: 0.8755\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 1s 135us/sample - loss: 0.2895 - acc: 0.8935\n",
      "\n",
      "Epoch 00051: val_acc improved from 0.88975 to 0.89350, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3903 - acc: 0.8492 - val_loss: 0.2895 - val_acc: 0.8935\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.2954 - acc: 0.8892\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89350\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3899 - acc: 0.8504 - val_loss: 0.2954 - val_acc: 0.8892\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3058 - acc: 0.8852\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89350\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3834 - acc: 0.8565 - val_loss: 0.3058 - val_acc: 0.8852\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3027 - acc: 0.8840\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89350\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3852 - acc: 0.8547 - val_loss: 0.3027 - val_acc: 0.8840\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2993 - acc: 0.8860\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89350\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3838 - acc: 0.8530 - val_loss: 0.2993 - val_acc: 0.8860\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2984 - acc: 0.8888\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89350\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3862 - acc: 0.8529 - val_loss: 0.2984 - val_acc: 0.8888\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2813 - acc: 0.8903\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89350\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3807 - acc: 0.8527 - val_loss: 0.2813 - val_acc: 0.8903\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2957 - acc: 0.8945\n",
      "\n",
      "Epoch 00058: val_acc improved from 0.89350 to 0.89450, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3795 - acc: 0.8517 - val_loss: 0.2957 - val_acc: 0.8945\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2770 - acc: 0.8920\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89450\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3781 - acc: 0.8560 - val_loss: 0.2770 - val_acc: 0.8920\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 1s 135us/sample - loss: 0.3080 - acc: 0.8815\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89450\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3787 - acc: 0.8554 - val_loss: 0.3080 - val_acc: 0.8815\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.2867 - acc: 0.8903\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89450\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3764 - acc: 0.8563 - val_loss: 0.2867 - val_acc: 0.8903\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2890 - acc: 0.8963\n",
      "\n",
      "Epoch 00062: val_acc improved from 0.89450 to 0.89625, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3797 - acc: 0.8589 - val_loss: 0.2890 - val_acc: 0.8963\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3133 - acc: 0.8845\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89625\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3699 - acc: 0.8604 - val_loss: 0.3133 - val_acc: 0.8845\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 0s 101us/sample - loss: 0.2846 - acc: 0.8888\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89625\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3778 - acc: 0.8537 - val_loss: 0.2846 - val_acc: 0.8888\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 0s 101us/sample - loss: 0.3120 - acc: 0.8805\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89625\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3715 - acc: 0.8610 - val_loss: 0.3120 - val_acc: 0.8805\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2895 - acc: 0.8972\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.89625 to 0.89725, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3764 - acc: 0.8581 - val_loss: 0.2895 - val_acc: 0.8972\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3087 - acc: 0.8830\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3751 - acc: 0.8555 - val_loss: 0.3087 - val_acc: 0.8830\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2713 - acc: 0.8950\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3644 - acc: 0.8628 - val_loss: 0.2713 - val_acc: 0.8950\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 1s 132us/sample - loss: 0.3017 - acc: 0.8890\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3673 - acc: 0.8578 - val_loss: 0.3017 - val_acc: 0.8890\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2902 - acc: 0.8928\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3716 - acc: 0.8583 - val_loss: 0.2902 - val_acc: 0.8928\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.2795 - acc: 0.8960\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3675 - acc: 0.8613 - val_loss: 0.2795 - val_acc: 0.8960\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3033 - acc: 0.8890\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3634 - acc: 0.8609 - val_loss: 0.3033 - val_acc: 0.8890\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2776 - acc: 0.8923\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3634 - acc: 0.8612 - val_loss: 0.2776 - val_acc: 0.8923\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.2745 - acc: 0.8945\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3667 - acc: 0.8604 - val_loss: 0.2745 - val_acc: 0.8945\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 1s 139us/sample - loss: 0.3050 - acc: 0.8885\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3613 - acc: 0.8645 - val_loss: 0.3050 - val_acc: 0.8885\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2762 - acc: 0.8972\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.3659 - acc: 0.8611 - val_loss: 0.2762 - val_acc: 0.8972\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2832 - acc: 0.8915\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3675 - acc: 0.8608 - val_loss: 0.2832 - val_acc: 0.8915\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 1s 128us/sample - loss: 0.3063 - acc: 0.8885\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3564 - acc: 0.8678 - val_loss: 0.3063 - val_acc: 0.8885\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2875 - acc: 0.8913\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3658 - acc: 0.8604 - val_loss: 0.2875 - val_acc: 0.8913\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2967 - acc: 0.8925\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3606 - acc: 0.8625 - val_loss: 0.2967 - val_acc: 0.8925\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3091 - acc: 0.8820\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3629 - acc: 0.8615 - val_loss: 0.3091 - val_acc: 0.8820\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2956 - acc: 0.8885\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3554 - acc: 0.8659 - val_loss: 0.2956 - val_acc: 0.8885\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2826 - acc: 0.8982\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.89725 to 0.89825, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3620 - acc: 0.8615 - val_loss: 0.2826 - val_acc: 0.8982\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2793 - acc: 0.8900\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89825\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3576 - acc: 0.8662 - val_loss: 0.2793 - val_acc: 0.8900\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2723 - acc: 0.8967\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89825\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3573 - acc: 0.8645 - val_loss: 0.2723 - val_acc: 0.8967\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3089 - acc: 0.8880\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89825\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3486 - acc: 0.8683 - val_loss: 0.3089 - val_acc: 0.8880\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 1s 131us/sample - loss: 0.2820 - acc: 0.8963\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89825\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3554 - acc: 0.8650 - val_loss: 0.2820 - val_acc: 0.8963\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2855 - acc: 0.8940\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89825\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.3538 - acc: 0.8632 - val_loss: 0.2855 - val_acc: 0.8940\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2784 - acc: 0.8945\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89825\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3530 - acc: 0.8658 - val_loss: 0.2784 - val_acc: 0.8945\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2872 - acc: 0.8940\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89825\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3490 - acc: 0.8694 - val_loss: 0.2872 - val_acc: 0.8940\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2786 - acc: 0.8980\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89825\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3532 - acc: 0.8653 - val_loss: 0.2786 - val_acc: 0.8980\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2651 - acc: 0.8992\n",
      "\n",
      "Epoch 00092: val_acc improved from 0.89825 to 0.89925, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3564 - acc: 0.8649 - val_loss: 0.2651 - val_acc: 0.8992\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.2968 - acc: 0.8980\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3549 - acc: 0.8648 - val_loss: 0.2968 - val_acc: 0.8980\n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2707 - acc: 0.8978\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3520 - acc: 0.8645 - val_loss: 0.2707 - val_acc: 0.8978\n",
      "Epoch 95/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2780 - acc: 0.8945\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3480 - acc: 0.8686 - val_loss: 0.2780 - val_acc: 0.8945\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2863 - acc: 0.8880\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3419 - acc: 0.8706 - val_loss: 0.2863 - val_acc: 0.8880\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 0s 114us/sample - loss: 0.2954 - acc: 0.8878\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.3527 - acc: 0.8679 - val_loss: 0.2954 - val_acc: 0.8878\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2793 - acc: 0.8940\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3480 - acc: 0.8698 - val_loss: 0.2793 - val_acc: 0.8940\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 0s 100us/sample - loss: 0.2766 - acc: 0.9015\n",
      "\n",
      "Epoch 00099: val_acc improved from 0.89925 to 0.90150, saving model to hard_model_random_erase2.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3418 - acc: 0.8698 - val_loss: 0.2766 - val_acc: 0.9015\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2978 - acc: 0.8880\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90150\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3534 - acc: 0.8669 - val_loss: 0.2978 - val_acc: 0.8880\n",
      "\n",
      "Time to train classifier: 887.89 seconds\n",
      "\n",
      "hard_model_random_erase2 Test accuracy = 91.25%\n",
      "\n",
      "\n",
      "hard_model_random_erase2 Gestalt Test accuracy = 92.00%\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: hard_model_random_erase3\n",
      "\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 1s 156us/sample - loss: 0.7057 - acc: 0.6942\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69425, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 1.0676 - acc: 0.5878 - val_loss: 0.7057 - val_acc: 0.6942\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.5701 - acc: 0.7715\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.69425 to 0.77150, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.7506 - acc: 0.6931 - val_loss: 0.5701 - val_acc: 0.7715\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.5111 - acc: 0.7965\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77150 to 0.79650, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6805 - acc: 0.7293 - val_loss: 0.5111 - val_acc: 0.7965\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.4716 - acc: 0.8115\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.79650 to 0.81150, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6357 - acc: 0.7452 - val_loss: 0.4716 - val_acc: 0.8115\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 1s 130us/sample - loss: 0.5380 - acc: 0.7875\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.81150\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.6064 - acc: 0.7560 - val_loss: 0.5380 - val_acc: 0.7875\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.4179 - acc: 0.8347\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.81150 to 0.83475, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.5848 - acc: 0.7661 - val_loss: 0.4179 - val_acc: 0.8347\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.4635 - acc: 0.8065\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.83475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5654 - acc: 0.7749 - val_loss: 0.4635 - val_acc: 0.8065\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.4361 - acc: 0.8192\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.83475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.5506 - acc: 0.7822 - val_loss: 0.4361 - val_acc: 0.8192\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3904 - acc: 0.8497\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.83475 to 0.84975, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5362 - acc: 0.7871 - val_loss: 0.3904 - val_acc: 0.8497\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 1s 133us/sample - loss: 0.3944 - acc: 0.8438\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.84975\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.5191 - acc: 0.7934 - val_loss: 0.3944 - val_acc: 0.8438\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.4546 - acc: 0.8195\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.84975\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.5181 - acc: 0.7981 - val_loss: 0.4546 - val_acc: 0.8195\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3739 - acc: 0.8555\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.84975 to 0.85550, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5131 - acc: 0.7990 - val_loss: 0.3739 - val_acc: 0.8555\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3646 - acc: 0.8597\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.85550 to 0.85975, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5081 - acc: 0.8010 - val_loss: 0.3646 - val_acc: 0.8597\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 1s 134us/sample - loss: 0.3735 - acc: 0.8503\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85975\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.4984 - acc: 0.8042 - val_loss: 0.3735 - val_acc: 0.8503\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3412 - acc: 0.8673\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.85975 to 0.86725, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4923 - acc: 0.8075 - val_loss: 0.3412 - val_acc: 0.8673\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 0s 122us/sample - loss: 0.4298 - acc: 0.8242\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4796 - acc: 0.8147 - val_loss: 0.4298 - val_acc: 0.8242\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3455 - acc: 0.8635\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4797 - acc: 0.8141 - val_loss: 0.3455 - val_acc: 0.8635\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3890 - acc: 0.8438\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86725\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.4708 - acc: 0.8138 - val_loss: 0.3890 - val_acc: 0.8438\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.4390 - acc: 0.8298\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86725\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4675 - acc: 0.8170 - val_loss: 0.4390 - val_acc: 0.8298\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.3760 - acc: 0.8550\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.86725\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4649 - acc: 0.8170 - val_loss: 0.3760 - val_acc: 0.8550\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3714 - acc: 0.8550\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.86725\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4595 - acc: 0.8176 - val_loss: 0.3714 - val_acc: 0.8550\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3289 - acc: 0.8685\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.86725 to 0.86850, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4581 - acc: 0.8253 - val_loss: 0.3289 - val_acc: 0.8685\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 1s 131us/sample - loss: 0.3478 - acc: 0.8633\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86850\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4468 - acc: 0.8271 - val_loss: 0.3478 - val_acc: 0.8633\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3120 - acc: 0.8755\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.86850 to 0.87550, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4481 - acc: 0.8256 - val_loss: 0.3120 - val_acc: 0.8755\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3343 - acc: 0.8705\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87550\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4425 - acc: 0.8293 - val_loss: 0.3343 - val_acc: 0.8705\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3506 - acc: 0.8645\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87550\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4451 - acc: 0.8275 - val_loss: 0.3506 - val_acc: 0.8645\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3584 - acc: 0.8593\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87550\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4370 - acc: 0.8312 - val_loss: 0.3584 - val_acc: 0.8593\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.3521 - acc: 0.8698\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87550\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4415 - acc: 0.8283 - val_loss: 0.3521 - val_acc: 0.8698\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3200 - acc: 0.8777\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.87550 to 0.87775, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.4356 - acc: 0.8322 - val_loss: 0.3200 - val_acc: 0.8777\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3140 - acc: 0.8790\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.87775 to 0.87900, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4281 - acc: 0.8322 - val_loss: 0.3140 - val_acc: 0.8790\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3299 - acc: 0.8652\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87900\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4295 - acc: 0.8333 - val_loss: 0.3299 - val_acc: 0.8652\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 1s 126us/sample - loss: 0.3785 - acc: 0.8528\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87900\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4270 - acc: 0.8366 - val_loss: 0.3785 - val_acc: 0.8528\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3723 - acc: 0.8555\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87900\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4256 - acc: 0.8348 - val_loss: 0.3723 - val_acc: 0.8555\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3092 - acc: 0.8800\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.87900 to 0.88000, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4176 - acc: 0.8405 - val_loss: 0.3092 - val_acc: 0.8800\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3056 - acc: 0.8798\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88000\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4164 - acc: 0.8397 - val_loss: 0.3056 - val_acc: 0.8798\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3684 - acc: 0.8508\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88000\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4158 - acc: 0.8401 - val_loss: 0.3684 - val_acc: 0.8508\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3339 - acc: 0.8720\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88000\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4108 - acc: 0.8424 - val_loss: 0.3339 - val_acc: 0.8720\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3326 - acc: 0.8635\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.88000\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4196 - acc: 0.8393 - val_loss: 0.3326 - val_acc: 0.8635\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3446 - acc: 0.8710\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88000\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.4135 - acc: 0.8413 - val_loss: 0.3446 - val_acc: 0.8710\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 0s 114us/sample - loss: 0.3293 - acc: 0.8710\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88000\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4084 - acc: 0.8448 - val_loss: 0.3293 - val_acc: 0.8710\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3292 - acc: 0.8755\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88000\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4061 - acc: 0.8436 - val_loss: 0.3292 - val_acc: 0.8755\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3121 - acc: 0.8767\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88000\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.4044 - acc: 0.8438 - val_loss: 0.3121 - val_acc: 0.8767\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3217 - acc: 0.8785\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.88000\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4038 - acc: 0.8435 - val_loss: 0.3217 - val_acc: 0.8785\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3118 - acc: 0.8810\n",
      "\n",
      "Epoch 00044: val_acc improved from 0.88000 to 0.88100, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4028 - acc: 0.8453 - val_loss: 0.3118 - val_acc: 0.8810\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 1s 141us/sample - loss: 0.3065 - acc: 0.8845\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.88100 to 0.88450, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4017 - acc: 0.8465 - val_loss: 0.3065 - val_acc: 0.8845\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2986 - acc: 0.8850\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.88450 to 0.88500, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4003 - acc: 0.8489 - val_loss: 0.2986 - val_acc: 0.8850\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3357 - acc: 0.8737\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.88500\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4024 - acc: 0.8459 - val_loss: 0.3357 - val_acc: 0.8737\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.3234 - acc: 0.8795\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.88500\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3981 - acc: 0.8486 - val_loss: 0.3234 - val_acc: 0.8795\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3085 - acc: 0.8820\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.88500\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3921 - acc: 0.8522 - val_loss: 0.3085 - val_acc: 0.8820\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 1s 129us/sample - loss: 0.3053 - acc: 0.8788\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.88500\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3869 - acc: 0.8512 - val_loss: 0.3053 - val_acc: 0.8788\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3003 - acc: 0.8840\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.88500\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3960 - acc: 0.8508 - val_loss: 0.3003 - val_acc: 0.8840\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3046 - acc: 0.8850\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.88500\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3898 - acc: 0.8522 - val_loss: 0.3046 - val_acc: 0.8850\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3436 - acc: 0.8777\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.88500\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3867 - acc: 0.8522 - val_loss: 0.3436 - val_acc: 0.8777\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3002 - acc: 0.8892\n",
      "\n",
      "Epoch 00054: val_acc improved from 0.88500 to 0.88925, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3880 - acc: 0.8508 - val_loss: 0.3002 - val_acc: 0.8892\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3031 - acc: 0.8840\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.88925\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3820 - acc: 0.8541 - val_loss: 0.3031 - val_acc: 0.8840\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3340 - acc: 0.8755\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.88925\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3857 - acc: 0.8493 - val_loss: 0.3340 - val_acc: 0.8755\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2942 - acc: 0.8875\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.88925\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3825 - acc: 0.8547 - val_loss: 0.2942 - val_acc: 0.8875\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2901 - acc: 0.8852\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.88925\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3750 - acc: 0.8562 - val_loss: 0.2901 - val_acc: 0.8852\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2841 - acc: 0.8895\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.88925 to 0.88950, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3800 - acc: 0.8569 - val_loss: 0.2841 - val_acc: 0.8895\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 1s 130us/sample - loss: 0.3537 - acc: 0.8767\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.88950\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.3766 - acc: 0.8566 - val_loss: 0.3537 - val_acc: 0.8767\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3177 - acc: 0.8813\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.88950\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3743 - acc: 0.8595 - val_loss: 0.3177 - val_acc: 0.8813\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3019 - acc: 0.8842\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.88950\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3853 - acc: 0.8540 - val_loss: 0.3019 - val_acc: 0.8842\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.2842 - acc: 0.8947\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.88950 to 0.89475, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3744 - acc: 0.8586 - val_loss: 0.2842 - val_acc: 0.8947\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2880 - acc: 0.8923\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3679 - acc: 0.8608 - val_loss: 0.2880 - val_acc: 0.8923\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2845 - acc: 0.8923\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3775 - acc: 0.8558 - val_loss: 0.2845 - val_acc: 0.8923\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3855 - acc: 0.8683\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3697 - acc: 0.8593 - val_loss: 0.3855 - val_acc: 0.8683\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.2802 - acc: 0.8945\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3708 - acc: 0.8608 - val_loss: 0.2802 - val_acc: 0.8945\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3041 - acc: 0.8855\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3703 - acc: 0.8594 - val_loss: 0.3041 - val_acc: 0.8855\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 1s 128us/sample - loss: 0.2981 - acc: 0.8860\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3599 - acc: 0.8615 - val_loss: 0.2981 - val_acc: 0.8860\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3343 - acc: 0.8820\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3647 - acc: 0.8623 - val_loss: 0.3343 - val_acc: 0.8820\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3007 - acc: 0.8938\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3614 - acc: 0.8623 - val_loss: 0.3007 - val_acc: 0.8938\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3415 - acc: 0.8685\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3701 - acc: 0.8591 - val_loss: 0.3415 - val_acc: 0.8685\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2900 - acc: 0.8850\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3635 - acc: 0.8617 - val_loss: 0.2900 - val_acc: 0.8850\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3038 - acc: 0.8942\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3539 - acc: 0.8648 - val_loss: 0.3038 - val_acc: 0.8942\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2855 - acc: 0.8955\n",
      "\n",
      "Epoch 00075: val_acc improved from 0.89475 to 0.89550, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3687 - acc: 0.8621 - val_loss: 0.2855 - val_acc: 0.8955\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2862 - acc: 0.8992\n",
      "\n",
      "Epoch 00076: val_acc improved from 0.89550 to 0.89925, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3529 - acc: 0.8662 - val_loss: 0.2862 - val_acc: 0.8992\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.2816 - acc: 0.8955\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3602 - acc: 0.8619 - val_loss: 0.2816 - val_acc: 0.8955\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 1s 137us/sample - loss: 0.3010 - acc: 0.8885\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3613 - acc: 0.8601 - val_loss: 0.3010 - val_acc: 0.8885\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3044 - acc: 0.8857\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3609 - acc: 0.8629 - val_loss: 0.3044 - val_acc: 0.8857\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 1s 137us/sample - loss: 0.2703 - acc: 0.8957\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89925\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3614 - acc: 0.8627 - val_loss: 0.2703 - val_acc: 0.8957\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.2758 - acc: 0.9000\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.89925 to 0.90000, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3530 - acc: 0.8643 - val_loss: 0.2758 - val_acc: 0.9000\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2932 - acc: 0.8915\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3493 - acc: 0.8666 - val_loss: 0.2932 - val_acc: 0.8915\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3104 - acc: 0.8860\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3498 - acc: 0.8659 - val_loss: 0.3104 - val_acc: 0.8860\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3067 - acc: 0.8895\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3576 - acc: 0.8650 - val_loss: 0.3067 - val_acc: 0.8895\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2833 - acc: 0.8957\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3568 - acc: 0.8651 - val_loss: 0.2833 - val_acc: 0.8957\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2682 - acc: 0.8988\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3499 - acc: 0.8661 - val_loss: 0.2682 - val_acc: 0.8988\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 1s 133us/sample - loss: 0.3086 - acc: 0.8890\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3569 - acc: 0.8644 - val_loss: 0.3086 - val_acc: 0.8890\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3006 - acc: 0.8967\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3520 - acc: 0.8637 - val_loss: 0.3006 - val_acc: 0.8967\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2858 - acc: 0.8955\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3440 - acc: 0.8677 - val_loss: 0.2858 - val_acc: 0.8955\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2675 - acc: 0.8985\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3539 - acc: 0.8665 - val_loss: 0.2675 - val_acc: 0.8985\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2754 - acc: 0.8910\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3538 - acc: 0.8686 - val_loss: 0.2754 - val_acc: 0.8910\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2645 - acc: 0.8970\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90000\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3461 - acc: 0.8648 - val_loss: 0.2645 - val_acc: 0.8970\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2790 - acc: 0.9018\n",
      "\n",
      "Epoch 00093: val_acc improved from 0.90000 to 0.90175, saving model to hard_model_random_erase3.weights.hdf5\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3475 - acc: 0.8643 - val_loss: 0.2790 - val_acc: 0.9018\n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3273 - acc: 0.8855\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90175\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3479 - acc: 0.8686 - val_loss: 0.3273 - val_acc: 0.8855\n",
      "Epoch 95/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3019 - acc: 0.8903\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90175\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3455 - acc: 0.8713 - val_loss: 0.3019 - val_acc: 0.8903\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 1s 140us/sample - loss: 0.2732 - acc: 0.8940\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90175\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3459 - acc: 0.8670 - val_loss: 0.2732 - val_acc: 0.8940\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2893 - acc: 0.8938\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90175\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3474 - acc: 0.8687 - val_loss: 0.2893 - val_acc: 0.8938\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2800 - acc: 0.9010\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90175\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3529 - acc: 0.8656 - val_loss: 0.2800 - val_acc: 0.9010\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2832 - acc: 0.8950\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90175\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3487 - acc: 0.8689 - val_loss: 0.2832 - val_acc: 0.8950\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2730 - acc: 0.8930\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90175\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3438 - acc: 0.8700 - val_loss: 0.2730 - val_acc: 0.8930\n",
      "\n",
      "Time to train classifier: 888.48 seconds\n",
      "\n",
      "hard_model_random_erase3 Test accuracy = 91.90%\n",
      "\n",
      "\n",
      "hard_model_random_erase3 Gestalt Test accuracy = 92.55%\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXd8VFX2wL8nnQSSEEKvoffeVEAU\nEbCCBQGxruLq6lb9qWt3dS27uu7aexfFjooUCwgKAtJ7CS0hQHogPZn7++O+IZNkkkxIhgQ4388n\nn5n33r3vnZmBe+4p91wxxqAoiqIolRFQ1wIoiqIo9R9VFoqiKEqVqLJQFEVRqkSVhaIoilIlqiwU\nRVGUKlFloSiKolSJKot6jojsFpFz/Hj/B0XkPX/d30cZfPqMItJBRIyIBB0PuSqR41oRWVKXMijK\n8UaVhaIoPiEiZ4nIehHJEJFUEflcRFrXtVzK8UGVxSmCWE653/tU/NwiEuinW28CxhljooFWwHbg\nRT89q0acir+7v9Ev88Sgv4isE5FMEflIRMIARKSxiHwtIskiku68b+PuJCILReRREfkZyAE6ikic\niCwSkcMisgCIrerhHu6f60Rkn/Os34vIEEeuDBF5zqN9gIjcKyJ7ROSQiLwjIlEe169yrqWKyD1l\nnhUgIneJyE7n+iwRianOl1XB575ORDY7nzteRG7yaD9aRBJE5G+OvEkicp3H9SYiMltEskRkOdCp\nzPNOF5EVzu+zQkROLyPLIyLyi4gcEZGvnPu979xvhYh08OEzdReRBSKSJiJbRWSyx7W3RORFEZkj\nItnAWSJyvoisdp6xT0Qe9GgfJiLvOd9vhiNDc+dalIi87nwHiY7sgQDGmIPGmP0eYhUDnSuR2f07\nHhaRTSIyqcz1Gz1+k00iMtA531ZEPnP+Xae6/21JGZeplHFLVvd3d/pcLCJrnO9pp4iMF5HLReS3\nMu3+KiJfVvU7ndQYY/SvHv8Bu4Hl2JlcDLAZ+L1zrQlwKRAONAI+Br7w6LsQ2Av0AoKAYGAp8DQQ\nCowCDgPvVSFDB8AALwFhwLlAHvAF0AxoDRwCznTaXw/sADoCDYHPgHedaz2BI86zQx1ZioBznOt/\nApYBbZzrLwMzy8gRVIW83j73+dhBXoAzsYPJQKf9aEeGh5225znXGzvXPwRmARFAbyARWOJciwHS\ngaucZ011jpt4yLLDeXYUdna+DTjHaf8O8GYVnycC2Adc5/QZAKQAPZ3rbwGZwBnYCWCY85n6OMd9\ngYPARKf9TcBX2H83gcAgINK59rnznUc4v+1y4CYPWdoBGYALKASurUTuy7H/bgOAK4BsoKXHtURg\niPObdAbaO/KsBf7jyBAGjHD6PIjHv9Wy/x6O4Xcf6nxvYx0ZWwPdsf/u0oAeHs9aDVxa1+NBnY5F\ndS2A/lXxA1llMd3j+EngpQra9gfSPY4XAg97HLfDDooRHuc+wHdl0drjXCpwhcfxp8CfnfffA7d4\nXOvmDCxBwP3Ahx7XIoACSpTFZmCMx/WWHn1LDQ6VyFvqc1fQ5gvgT8770UCu532xym+4M3gVAt09\nrv2TEmVxFbC8zL2X4gyijiz3eFx7CvjW4/hCYE0Vsl4BLC5z7mXgAef9W8A7VdzjGeA/zvvrgV+A\nvmXaNAfygQYe56YCP3q5XwxwJzC8Gv+W1wAXO+/nub//Mm1OA5K9/cb4piyq87u/7P5OvLR7EXjU\ned8LOwEI9fWznox/6oY6MTjg8T4HO1tHRMJF5GXHpZMF/ARES2mf9T6P962wyiTb49yeashx0ON9\nrpfjhh7P8bzvHuxg39y5dlQmR5ZUj7btgc8d90gGVnkUO32rg+fnRkQmiMgyx42TgbUePF1wqcaY\nIo9j9/fc1JHd836en63sZ3Vf9wz8+vq9VUR7YJj7O3HkvxJo4dGm7OcdJiI/Oq6cTOD3lHzed7GD\n9Ycisl9EnhSRYOc5wUCSx3NexloYpTDGpAFvA19KBdlpInK14+Jx36u3hwxtgZ1eurUF9pT5LapD\ndX73imQA+9mmiYhgJwSzjDH5xyjTSYEqixObv2Fn7cOMMZFY1w5Yk9uNZ1nhJKCxiER4nGvnB7n2\nYwcez2cUYQfJJOx/UsAqPKw7zc0+YIIxJtrjL8wYk1hNGY5+bhEJxVo+/waaGxugnUPp76kikh3Z\n23qc8/zOyn5W9/XqylsZ+4BFZb6ThsaYmz3alC0f/QEwG2hrjInCuhAFwBhTaIx5yBjTEzgduAC4\n2nlOPhDr8ZxIY0yvCuQKwiqSyLIXRKQ98CpwK9YlFw1soOQ730eZ2I/H+XYVKKBsrOvMTQsvbarz\nu1ckA8aYZViLdyQwDatgT2lUWZzYNMLOTDPEBoEfqKyxMWYPsBJ4SERCRGQE1g1S28wE/iI2mN4Q\n67b5yJktfgJcICIjRCQEGyfw/Hf4EvCoM9ggIk1F5OIayhOC9UMnA0UiMgEbd6kSY0wxNubyoGPJ\n9QSu8WgyB+gqItNEJEhErsDGZb6uocyefO084yoRCXb+hohIj0r6NALSjDF5IjIUO+ABR1Ng+zgW\naBbWzeYyxiQB84GnRCRSbLJBJxE50+l3iYh0c843xcabVjtWRlkisAN3stP3Oqxl4eY14HYRGSSW\nzs5vvhw7oXhcRCLEBuPPcPqsAUaJSDuxCRN3V/G9VfW7vw5cJyJjnM/UWkS6e1x/B3gOKDTGnPLr\nalRZnNg8AzTABjuXAXN96DMNGIYN4D2A/Q9R27yBnYn9BOzCBsNvAzDGbAT+gJ35JmF9wQkeff+L\nnRHPF5HD2M81rCbCGGMOA3/EBqnTsd/B7Grc4lasq+gANj7wpse9U7Ez879h3Wn/B1xgjEmpicye\nOPKfC0zBWjIHgCewA2FF3AI87HyH92M/u5sWWKWdhXXzLaJk5nw1dpDdhP2uPsHGjcC61uZikyLW\nY4PcpTKcPGTehI3PLMValH2Anz2ufww8iv13cBgbS4hxlPOF2ID3Xuy/jSucPguAj4B1wG9UoZCr\n+t2NMcuxSQP/wQa6F1HaSnwXq+DqdNFqfUGcAI6iKIrigYg0wCY6DDTGbK9reeoatSwURVG8czOw\nQhWFpU5r7Cj1BxG5Epv5UpY9lQQ46wwROVLBpQnGmMXHVZhaQERGAt96u2aMqSpbSqllRGQ3NhA+\nsY5FqTeoG0pRFEWpEnVDKYqiKFVy0rihYmNjTYcOHepaDEVRlBOK3377LcUY07SqdieNsujQoQMr\nV66sazEURVFOKETEpyoO6oZSFEVRqkSVhaIoilIlqiwURVGUKjlpYhbeKCwsJCEhgby8vLoWxe+E\nhYXRpk0bgoOD61oURVFOQk5qZZGQkECjRo3o0KEDttLwyYkxhtTUVBISEoiLi6trcRRFOQk5qd1Q\neXl5NGnS5KRWFAAiQpMmTU4JC0pRlLrBr8rC2c92q4jsEJG7vFxvLyLfi93HeaGU3j/6GhHZ7vxd\nU7ZvNWQ41q4nFKfK51QUpW7wm7JwauU/D0zA1vef6uwF4Mm/sdtB9sXua/CY09e9N8Mw7D65D4hI\nY3/JqiiK4nfyD8Pq96G48Nj613FpJn9aFkOBHcaYeGNMAXbT+7Kb2PQEfnDe/+hxfRywwBiTZoxJ\nBxYA4/0oq9/IyMjghRdeqHa/8847j4yMDD9IpCinAC4XbJsPhbl1LYklLxPevQS+vAXWf1z9/oW5\n8Mpo+OymOlMa/lQWrSm9H24CpfclBlgLXOK8nwQ0EpEmPvZFRGaIyEoRWZmcnFxrgtcmFSmLoqLK\ntxieM2cO0dHR/hJLUeo3xYUw7x54aQTsX129voW58PE18MHlsOD+Y3v28ldhyze+90mLh00V7KeV\nkwbvXGw/R2gkbPVaXLhyfngEktbAug/hp39Vv38tUNcB7tuBM0VkNXAmdt/iYl87G2NeMcYMNsYM\nbtq0ytImdcJdd93Fzp076d+/P0OGDGHkyJFcdNFF9OxpPXITJ05k0KBB9OrVi1deeeVovw4dOpCS\nksLu3bvp0aMHN954I7169eLcc88lN7eezJYUxR9kJsJb58PS5yBjH7w+Dla+6duMOjsF3r4INn8F\nLfrCitfh0Gbfn733V3j5TJhzu53F55TZMTYrCXZ8B3lZ9rgwDxY+Ds8Ph1lXlVcYOWnwzkVwcCNc\n8R70vgR2/gBF+b7LtOcXWPo8DP4d9J0CPz5aPUVWS/gzdTaR0pvct6HMJvbGmP04loWzV/OlxpgM\nEUkERpfpu7Amwjz01UY27c+qyS3K0bNVJA9cWPlWD48//jgbNmxgzZo1LFy4kPPPP58NGzYcTXF9\n4403iImJITc3lyFDhnDppZfSpEmTUvfYvn07M2fO5NVXX2Xy5Ml8+umnTJ8+vVY/i3IKYgwUF0BQ\nZbuzHkdZDm2GXYvszLkoHy57A+JGw2c3wNd/hoQVcNGzEBDo/R7ZqfDaOXA4CSa/De3PgGcHwry/\nw/TPoLIkEGNg/r1WQUW2hnMfhfn3wLIX4Ox7bZviImutHFgPEgitB0JOqrUqel0Cqdvhm79B3Eho\n0Ni2//haSN4KU2dC53PsfX57C3Yvgc5jqv5eCrLhi1sguh2Mfdh+9pRt8NkMuH4utOhTjS+5ZvjT\nslgBdBGROBEJwe4fXErtikisiLhluBu7dzPAPOBcEWnsBLbPdc6d8AwdOrTUWoj//e9/9OvXj+HD\nh7Nv3z62by+/KVdcXBz9+/cHYNCgQezevft4iauczCx7AR5rC9/eCYcPVt0+OwWeHwZ7lvr+jF2L\nIXVn5W1WvQv/7gIvngZz77KD9Y0/Qu9LIaIJXPkJjLoD1rwPvzxb8X2WPgvpu+GqL6DnxRARC2fe\nZWfy26oYPla8ZhXFoGvhD8vh9Fuh50RY9lKJdfHri1ZRjLkfRv4VJABCG8FVn8Plb8LFz1vlMe8e\n237+vVb5XfCfEkXR8UwIagDb5nqXI/8IfP8P+PovMP8++Pg6SN8FE1+A0IYQ3ACmvA8hEfDyKJg5\nzVo6Llfln68W8JtlYYwpEpFbsYN8IPCGMWajiDwMrDTGzMZaD4+JiAF+Av7g9E0TkX9gFQ7Aw8aY\ntHIPqQZVWQDHi4iIiKPvFy5cyHfffcfSpUsJDw9n9OjRXtdKhIaWzPwCAwPVDaXUnMJcWPw0hMdY\n//xvb0O/K6BBjL0eFgWn31Z6Fr93KSRvgbl3wo0LIaCKuWZOGrx/GXQZa10w3jiSbJVV025wzoPQ\nYSQ0bl+6TUAgnHWPnaH/8IidkZedUeek2c/RayK0P63k/NAbYeUb1rrodDYEhZSXYd9ymHs3dDkX\nzv9Pyec6807Y9IV1AQ28Gn78J3QdDyP+6t1KadkPRvwZFj8FAUGw6m0YdjMM8PACBDeAjqNh61yY\n8GTp+yRvhY+uspZDeBMoOGItrFF3QIcRJe0iW1lluuJVq2i3fgOtB8MN31VuPdUQv67gNsbMAeaU\nOXe/x/tPgE8q6PsGJZbGCUujRo04fPiw12uZmZk0btyY8PBwtmzZwrJly46zdMopy5r3IScFrvna\nDj6LnoS1H4KrGIwLTDG0HmRdKm4OrLevSWth42fQ57IqnvEBFOXZ9hWx+Cnb5pJXILZLxe1E4IJn\nYN+v8OmNMGMhBIeVXP/1JTu4jrqjdL/AYBj3T+s++vFRGPtQ6etHDsGsqyGqtZXBUwE272mti19f\nss9F4Lx/Vz4gj/o/Gy9Z9TbEnQnnPlK+TbfxsO1bOLQJmjuT2A2fwpe3WWVy9RdWoYD9Pby53aJa\nW+U6+m77vIJsvyoKqPsA90lPkyZNOOOMM+jduzd33FH6H/L48eMpKiqiR48e3HXXXQwfPryOpFTq\nBUX5dtCuym3jtW+BdVv8+krVbYuLrDun9SA7Y23SCS55Ge49CPenwO3bbLuyWUgHNkCTLtC8D3z/\ncOVBWpcLVr5u32fsLR8odp9f+Tr0n1a5onAT0QQufgGSN8MP/yg5n5dp3UXdLygZfD3peq51L/38\njA14uzl8ED68EnLTYfK7Ns5QljPvtEpo92Ibu4huW76NJ8FhcOlrNhB9+VsQ6GU+3mWcfXVnRS19\nAT65Hlr0ht8vLlEUUHF8xk1QqFXag4553bLPnNS1oeoLH3zwgdfzoaGhfPut9zQ6d1wiNjaWDRs2\nHD1/++2317p8Sj1hw6d29vvb2/C7eRDVxnu7jV/YQaLLODsTzsu0s+P4hRDS0A6+oQ0rfs7mL61v\nf+w/vM9GI2Ihqp0XZbEe2g6x93/vUuveGX6z92fE/2ADv/2nw5r3bN+OZ5Zus/BxQGB0ueIOFdPl\nHBhyg40vHE6ys+t1H0F+Joyq5P/GeU/B4QM2y6lRC+sm+uIWOyOf9BK07Ou9X/OeNgspLR6G3eSb\njC37WeVbEZEtoWV/G0cJCITvHoQeF8Glr3t3k9UTVFkoSn3AGPj1ZTtI52XAOxNttktEbOl2mYnw\nyXXWVRTbFYbOsIN2yjYY9nvrMtn0JQy4suLnLHkGmnSG7udXLE+r/qWVRW46ZO6FIddDpzEQN8pa\nQWFRdhDOSbUz3FYDbPsVr0NEUzsbX/OedUV5KotDW2DtTOvTr0gpVsS4f9rYyi/Pwuavraupy7kl\nz/ZGYJDNrnr7Qph1DbgKrYV02es2XlIZ5z9lX2vTzdNtAix8DBKW20D+pFe8WyH1CHVDKUpFFBfZ\ngfX9y+3isH91trPRY6EoH9Z+BHPusOmdT/WApHUl1xNW2kVXI/4E0z6CzH3w3iUl+fxuVr1jB/zx\nT0BgqJ0pZybA9E9h/OMQ0wlWVxBMBtg6Bw6sg9P/WLmLo9UAm4WTm26PD260r8372EFz7MNWqX1x\nM3z/ECx70a6HWPuRdS9tm2uDwpEtIbKNfaYni/8NweE2q6i6BIXC2ffAbSuh1yTr1/fFOgmJgGmz\noM1gOO1WuPH7qhUF2M9b2/GAHhfa9Nt+U+GSV+u9ogC1LBTFO+m7bSA1YTk0721nvyGNbGB4xF8h\ntrPv9youtLPZbd9CcISdtRcXwOe/t4HaoBBY/rJd3dt3inUhTX4XZk6BBffBhf917lNkA6edz4Hh\nv7dukd1LbIC6SSfbZsB0O3in7CgtY16mzSRa8Ro0joN+UyqX2T1L378GOp1VEtx2ZyG1GgC3Onve\nN2phF6fNuho+nwFNe9jzg66zry37lQ5yFxVYf32fy8pbTtUhqo1190x8serMLDcRsdZiq2ua94K/\nboKGzf0emK4t1LJQFE9cxbbY20sjbZropa/DzT/b2f7ktyHQGdjLYozNd3/1bHgizqY0GmMDvV/+\nwSqKCU/C3fvgujk2J//QRlj0hA20bvwC+l9ZEmvoeq5N+1z1TslAvW2u9dMPvt4ei9hsJbeiADtT\nlQCr1Nxsmw/PDbGppUNugJsWVb0Qr5Vd13PUFXVgvXUrNWpe0qZJJ/sXEmGDz1d/Yf37yZuh64SS\nYHDLvpCy3cYHAPb8bIPGXWup3JuviqK+0ajFCaMoQC0L5Xhx+ID1M/sawFv7ofXhh0TYoG3bITDy\nb/6Tz+WCLV/Bj4/Zwa7tcJtK6Znz37CZ9S+v+cD64sOi7PkDG+zK3X3LIKqtjQfMvtXm6Ee2sgHY\ns+8rHSDtNt4qhyX/sfEGV6FVDp6MvgvWzbJrAK75ysYmIltb/3xFRLaEzmNtPOCse2D9LPjyVmjW\nE6Z+aFcd+0KDxtYC8VQWVa0WDgyGC562sRDPti36AsZ+T+2G2cBuUJhNLVVOGE5QlaycEGz/zs6q\nn+kLT3WzK1p95be3rSvIVWT93d//Awpyal/GrP3w8//gpTOsG8UUw2VvwnXfll8cBjaIXHCkJC6Q\nvgfenWizZc77N9z2G1w/Dyb8y9b0WfWO9Y97U3Tj/mndEJtnW9eSp4UAdsA+6+82bfOX/8HO72Hg\nNVX7twdeZS2Qz26wMYW4kdb14quicNNqgHVDFRVYK8vX0hKdx1jF6qZlP/t6YJ21trbNtQHykPDq\nyaPUKaos/MyxligHeOaZZ8jJ8cMAeTzY+yu8f6nNVmnRx64wXT/LDjxV4XLZgaXPZXaQG/coYOwM\nvKYUF9oVuz/9C966AJ7uaeMCwQ1g0stwyzJb7K0i10ar/tDuNGv15KbDB1fYz3TtN9YyCAq1fYfN\ngJt/sVku5z7i3d3QIBoufs7GQk7/o/fnDbrOWgUL7rcB0YFXVf0Zu4yD8FjY+LktezFtVuWptBXR\naoDNgNq71MZYmh9jHaLIVnZFctIaSN1hA+ddxx3bvZQ6Q5WFnzmhlcWun+DZQTZTpzoUF8E3f7Uu\nk79stLVszrzTDq47vqu6f+oOO3tv6fjNmzl7ZiVvqZ4cZdmzFJ7qDq+PtcHe3HQr122r4MYfbNC3\nqkVQYK2LjD3wylm2eNwV70DTruXbxcTZEhqV+aU7j4E7d5Vfg+AmMMhaIGDTLSNbVS1fUIjNjBp9\nt7WSjrVQoNsSWf2ufT3WonUiTpB7XUlNpC6qLE40NGbhZzxLlI8dO5ZmzZoxa9Ys8vPzmTRpEg89\n9BDZ2dlMnjyZhIQEiouLue+++zh48CD79+/nrLPOIjY2lh9//PH4C7/kGTtwv3sJXPNl5Xnsnix/\nGQ5usBk97hltp7Ps7HL9LOh+XuX9k9bYV3eQNaYjBARXr9R0WbbNsxlJUW1s3nyHEceeidP9Ahub\nSN9lM5U6jj52ucD6+iuj01nW6mk7zPd79r28ZjKBE2sQW3Y7MNTGYmpyr6XP29IUzXpVvRJaqXec\nOsri27tKskpqixZ9YMLjlTbxLFE+f/58PvnkE5YvX44xhosuuoiffvqJ5ORkWrVqxTff2Br1mZmZ\nREVF8fTTT/Pjjz8SG1uD9MJjJX23rdY50FkZ/M5EuPbrqmeXWfttwbXOY20uuZvAYFvGefW7du1A\nWGTF99i/xlbmjO1W0je2S3nL4sB6q9Au/G95N8uRQ3YbS7DZN1/92co+/dOapWuCne1PfNFaF55F\n4vxJVamu/iAs0n7vKdvsRKEmawFa9rNB/H2/2tRj5SjFLsPPO1IY2L4xDUPr75CsbqjjyPz585k/\nfz4DBgxg4MCBbNmyhe3bt9OnTx8WLFjAnXfeyeLFi4mKiqprUW3qp4h101zzlc1IeufiqusWzfu7\njQuc92R590vfybZo3JavK79H0hpbJ8dzcGravbxlsfZD2PCJrfnjyaYvbUD92YH2b/Zt0P50q+xq\nqijcxI08foqiLnFbk8171+w+7iA31F7K7EnAr/GpXPTcEq5+YzmXvfgLB7PKV52uiKJiFxv3Z/Lu\n0t28t2yP/4R0qL9qrLapwgI4HhhjuPvuu7nppvI1ZlatWsWcOXO49957GTNmDPfffwzbQR4rafEQ\nv8hm2gQE2MF+9Xs2RdNdiuGa2Xbl8cypthSyN8sgcZUNqp55l3UdlaXNEIhub9NB+0/zLovLZX3b\n/a4ofb5ZD1vptCDbptOCUwkUW/Zh4NV2g5icNJvG2ry3zUICW9yt6/j6scnPiUarATb1t0UFtZN8\npXGcDeQHBtsV1CcJBUUuQoJK5tzFLsMPWw6x7eBhpg9vT1QD7y7GzJxC/v75er5Zn0SrqDD+NrYr\nLy7aySUv/MLb1w+hXUwEi7YlM3/jAfq0ieLKYe0JDLCTr5Qj+Tzy9SbmbzpIToHdWHRgu2imD/eS\nvVeLnDrKoo7wLFE+btw47rvvPq688koaNmxIYmIiwcHBFBUVERMTw/Tp04mOjua1114r1feoG8oY\nW0ohIrZkwKwN5t1ra+KnbLeZR9vmwZEDtlKnmyadYPI71rr47EaYMrN8xtDPz0BoFJz2B+/PEYE+\nl8OSp+1CNM8FXm7SdkLB4ZLgtpum3e1r8lYbeC3Mte6q3pfZLSYX3G+rfM692waur/r8uO4idtIS\nN8rGKzqcUbP7BATYCUB4E9+SCLCTq7xCFw1CfGtfEcUuw9KdqQQGCG0aN6BFVBjBgZU7VYyzhatU\nkJyQmVvInz5czU/bkundOorTOjahUVgQM5fvIzHD7jcza+U+Xpo+iB4tS0+sEtJzuO7NFexJzeGv\nY7syY1RHwoIDGd2tGde9tYJJL/xCYICQkVNIg+BAPv4tgc9WJfL4pX3YfvAI93+5gez8Yi4b3IZh\ncTEMbNeYNo0b1Og78gVVFn7Gs0T5hAkTmDZtGqedZjdnadiwIe+99x47duzgjjvuICAggODgYF58\n8UUAZsyYwfjx42nVqpUNcBdkQ26anZ3VlrLISYPt86FRS1j2PIQ3tmmvjVrZuIMncSNhwhO2HtGP\nj8KY+0qupe60gdARf648HtF3sq0LtPEz7xVL95cJbrtp5pSQOLTZKov9q60PvPelNvC6yKmLtO5D\nu6eAKoraoXkvuCfJ5wG+UtwF+cqQll3Akbwi2jUpWXeRlVfIH2euZsWuNJ6bNpCzujfz2rcy8ouK\n+XxVIi//FM+ulOyj5wMEOjVtSN820fRtE0VUg2Dyi4rJK3SxJzWHDfsz2bw/i4ZhQVzQtyUX9WtN\n79aRRxXHrpRsfvf2Cvam5jB1aDu2HzzCGz/vorDYMLxjDPee34PGESH8ceZqJr3wMw9f1JszuzWl\nSUQIWw8e5ro3V5BbWMzb1w/ltE4lWyj3aRPF57eczt8/X0/j8BAmDWjNiC6xzFmfxMNfbWLCfxdj\nDPRrE8W/L+9Hl+aNqv2d1AQxvmyCfgIwePBgs3Jl6RTPzZs306NHjzqSyA9k7YcjB53VtR1KXyvI\nZvP2eHr0quYgueI167aZsdAWg1v3kT1/5p12QVhZjIGv/mgXm018scSd9NWfYM1M+PN67xaDJy+P\nsu6m3y8uH9eYd48tS/H3xNJZQsVF8M+WdhX0uY/Ylc/fPQh37LQF6Z4bDFmJ1gK56Sd1OZ0AGGP4\neGUCj3yziSP5RUwd2o6/jO1KTn4xv3t7BbtSsmkbE87etBwevrgXVw7z7mYpKHIRGCAEBgj5RcUs\ni09jwaYDzNt4kOTD+fRpHcWMUR2JiQghIT2HfWm5bErKYl1CBilHSq/7CQsOoEfLSHq1iuRAZh6L\ntiVTWGyIbRhCu5hw2saEs3BrMgECL04fxPCOdrDPKSgiM7eQllElM/xDh/O47YPV/LrL7uURIBAg\nQrNGobx1/VC6VmOwT88u4Jntm+v4AAAgAElEQVTvttG6cQOuPyOOoCoso+ogIr8ZY6r0DaplcSKR\n71QgLS4sfy0tHnIzqn/PtR/Zwm8t+9t6RXlZNgtqQAWLv8TZLSxjry0jERppfdBrPrDlK6pSFGDv\nPed2G8gum4673x3cLuPrDQyyJbkPORlRe3+1FoU7YD3+MauwLn5eFUUdsDc1h437M4lPySY+OZvs\n/CJcxuAyMKprLNOHtScgoGRisDslm3u+WM/PO1IZGhdD9xaN+ODXvXy5Zj/BgYLLwDvXD6Vf22hu\n/WAV93y+gfjkbG4Z3YkmDe3veygrj6fmb+Pj3/bhMhDk3L/IZWgQHGifO7w9IzrHenUnGWM4mJVP\nTkERocGBhAQG0Dg8uNRAnJFTwLcbDrBmbwZ703JYuTudjk0j+N+UAbSNKbGEwkOCCA8pPZw2axTG\n+zcMY/GOFBLTczmUlUduYTG/G9GRFlFhVIfGESE8dHENkwxqiCqL+ooxpWfdxYXWTw92Na0nrmJb\nFqMov3y/ykiLt1VVz3nQ9gkMtgvojhyyNYYqIigUrnjfxi8+uc7W+HEV2T2bfaHP5bb0x6p3SisL\nl8tWJ+072Xu/pt1tUNsY+9rNY71Gz4uh2/knRKnn+kZeYTFfrklkbUImd47v7jUom5SZyw9bDvHb\n7nS6NG/EqK6xdG8RyU/bk3ljyS4Wb0852rZ5ZChRDYIJECG/yMV3mw/y9doknrysLxGhQTz7w3Y+\n+HUvDYIDeXRSb6YOaUdAgHDN6R14/NstJKbn8ty0AXRsatOhX716MA9+tZHXl+zi7V92M7pbMzo1\ni+DdpXsoLHYxdWg7mkeGkV9UTLELhsY15vROsYQFV+46E5EqB+3o8BCmDm3H1KHtjuGbhaDAAM7q\nVn0XWn3kpP+fZYypMEhVb8lOsS6nxh1K/P9uqyK0EeQfKa0UigtsQM4U2QBws+6+PWfdLMAJOrsJ\nCKxcUbgJbQhXfgxvngc7Fti9isvWNqqIBtG2/fpPrEvJHX9Ji7fB7bLxCjfNuttU2f2rbOymXZlF\naqooyrE/I5cn524hK6+I56YNKDX7zcor5LXFu3h/2R5Ss+0EZOP+LN793VAiw6zC+DU+lX98s4kN\nifbfX0xECJ+tTuSJudZlk1foolmjUG4/tytndm1GXNOIUmsFjDF8uiqRh77ayPj//nRUgUwZ0pY/\njulC88iSwbpT04a8enV5b0hQYACPTOzD9OHt+XxVIp+vTuS7zQcZ36sFd03oTofYWkz2UCrkpP7f\nFRYWRmpqKk2aNDkxFIYxtgDckYP2OGu/VQ4i1j0UEGyzjfIP25m846oxRfmkZhcRlhkPu/f5piyM\nsfGJDiOqv1OZm/AYm3X0/UMwsprbvQ682gajN31ZEvdwr9wumwnlxr1Pwqp37Gtb3bPcTVZeIYnp\nueQVFhMeEkRYcACz1+znhYU7cRlDYbGL2z5YzctXDSIoMIDkw/lc9fqvbD14mDHdm3H9GXEcyS/i\nlvdXcc0by3nz2iG8ujieFxbupF1MOHdN6M6Y7s3o3Kwhhw7ns3h7Cr/tSWdYXAzn9WlZKn3UExHh\nskFtGNE5lifmbkGA28Z0Ie4YBvjuLSK5+7xI7hjXjdTsglKKRvE/J3WAu7CwkISEBPLyfF/oUmcY\nY2fLBdl2AVxQqN2qMiLWrmbOSrSB3OAGkJ0MDVuUlPvOP0zYgZW02fA8we2H2BTSip6R58Q1ktbB\nOxdZH39dLC4zxtadatisZDOauXfb7TjLBrfdpO60i+xCGtrrd8SfuHsZ1JCdyUf4btNBvt98iC0H\nssjKK/La7rw+Lbh7Qg8Wbj3EfV9uZOrQdtx6dmemv/YrBzLzeOXqQYzs0vRo+7kbDvCHD1YREhhA\nbmExkwe34YELexFRj1cWKzVDA9xAcHAwcXFxdS2Gb+z4DmZdatM+h//d7rH84ul2UL3gaZh1GVz+\nti1O99lkuOK9knIa8++Fla/a4/hFFcct5twBK14tOQ4KsxvF1wUi1rr47gFbTmTtR3YPho6jK66V\n1LiDldm9cc5JrChyC4r5aMVexvRoXiqQmpVXyPVvrmDlHrvdaa9WkVzcvzVtYxrQOjqc8JBAcgqK\nyS0sJi42nEHtYwC46rQOJGXm8cLCnXy1dj8i8N4NQ49edzO+dwuenTqApxds48/ndOGCvj4ULlRO\nCfyqLERkPPBfIBB4zRjzeJnr7YC3gWinzV3GmDki0gHYDGx1mi4zxvzen7LWOSk77OvQG509fwNt\n1dCPr7GprRJoC8oVOzPIzMSSvhl7bWG7DiNg/cd2cZ23KqjxC23ZhX5T7XGznpWvifA3/abCD/+w\ngfLAEBsgr2yDo4BAmxF1YF31iuodR7Lzi9h+6Agdm0Yc9fuX5dDhPFbtyQAMwYEBNAgOpG/b6KO+\n/vUJmfz5o9XsTM7mpUXxzJwxnLjYCPKLipnxzkrW7MvgnvN6cF7flrSO9n0x1h3jupF6pIAftx7i\nzeuG0KuV97Iy5/VpyXl9fIhbKacUflMWIhIIPA+MBRKAFSIy2xizyaPZvcAsY8yLItITmAN0cK7t\nNMZU4Lw+CUnfbfdnjihxCdDjIruHwMH10H6E3ZnNGDu7zkooaZex15a6aD/CHu9ZUl5Z5B+xFWRH\n3+19MVxd0Kg5nPFnK/9Zf7dWU1U062GVRbv6F69YtTedP324mn1pNmutTeMGdG3eiOaRoTR10j0X\nbU9h7b7yKc7BgcLg9jHENY1g1op9xDYM5Z+T+vDv+VuZ8spSPrhxOE8v2May+DT+c0U/Jg2ofpxJ\nRHjisr4Uu8zR0hGK4iv+tCyGAjuMMfEAIvIhcDHgqSwM4J7aRgH7/ShP3bF3mS3Md/6/bczBG+m7\n7GDp6T4KCICz74GZU6CLs5paxO4TkVlGWbTsb7ORGraA3T+X7NPs5uAGwJQu6FYf8FwF7gvtT4cd\n3/teLr0WycgpQETKpZYWuwwv/LiDZ77fTovIMJ66vB8HsvLYnJTFjkNHWJeQSVp2Pgbo3zaa28/t\nyumdYwkNCqCo2JCZW8jPO1NYuCWZpfGpXNC3JY9M7E10eAiD2jdm2qvLmPDfxRQUubh7QvdjUhSe\nqKJQjgV/KovWwD6P4wSgrO/gQWC+iNwGRADneFyLE5HVQBZwrzFmcdkHiMgMYAZAu3bHlgd9XFj/\nMax5D4rz4ZJXvccT0nbZctBl6Tre1mHy3BwnqnWJG6og2wbCo9va+3Y4A3YvKR+3SFpnX1vWsCBc\nXTPwGug3zfe9vGuBpMxcXl4UzwfL9xIcIPxlbFeuPb0DQYEBLItP5REntfSifq14ZFJvr+6nYpch\nv6i43MItN6O6NuXuCT3IKywutT6gW4tGzJwxnOveXMEF/VoyY5SXAo2Kchyo6wD3VOAtY8xTInIa\n8K6I9AaSgHbGmFQRGQR8ISK9jDFZnp2NMa8Ar4DNhjrewvtM8lbrk1//sa1ZdMafSl93uezeCF3G\nlu8rUn6zoMg2sGuRfZ/h6ONopxRC+zNgw6d2zYLnuoektdbF1egE90WLHBdFkZiRy9KdqSzZnsyc\n9QdwGcOkAa1JzS7gkW8288lvCbSNCWfBpoO0igrj2akDuKBvywpTtAMDpEJF4Ym3hWRdmzdiyZ1n\nnRjp38pJiz+VRSLguR1WG+ecJ78DxgMYY5aKSBgQa4w5BOQ7538TkZ1AV6Ca+3vWE1K22eqoRbmw\n4AG7U1gXDyPqyAG7z4MvPnuwlsXhJBvszthrz0U7llUHJ26xe0lpZXFgrS0zfYoNOIkZubyxZBet\noxtweucmdG3WqFTZCU/2pubwxZpEvlyTyM5kW3guJiKEywa34eYzO9E2JhxjDPM2HuChrzaxb0cK\nd4zrxu9GxFW5WrimqKJQ6hp/KosVQBcRicMqiSlA2U0M9gJjgLdEpAcQBiSLSFMgzRhTLCIdgS5A\nvB9l9R+5GXaRXdNuNtMpdQd8er3dmzrUKSSWtsu+li0OWBGRrW1q7ZED1iKBEmUR29VaELt+gkHX\n2HNF+bZa6+leLJcTGGMM+UWuCgfquRuSuPPT9RzJL6LYZQ3P6PBgQoMCyC2wVUZDggKICA0kNCiQ\nvWl2v/NhcTFMG9aeM7woFxFhfO+WnN29OS5j/K4kFKW+4DdlYYwpEpFbgXnYtNg3jDEbReRhYKUx\nZjbwN+BVEfkLNth9rTHGiMgo4GERKQRcwO+NMWn+ktWvpGyzr0272bIWYx6A9y+zBfPiRtpr6bvt\na2NfLQsnwJmZaC2LwFCIcOrPiEDXcbDxC1tLKriBVRSuohM/XuFQWOzi63X7eXlRPNsPHeH6Mzrw\n53O6Hl04duhwHs98Z+sP9W0Txf+mDCAoUFi6M5Xf9qRjDDQICSQsOJCCIhfZ+UVkFxRxxZC2TBzQ\n2qd01IpWLCvKyYpfYxbGmDnYdFjPc/d7vN8ElNtVxRjzKfCpP2U7biQ7S0VinVRWdxbP/tUeymIX\nSECJdVAVka3ta1aCkzbbtvQCtT6X253uts2FXpNsvALqXyZUBRhjeGLuVr5et5+42Ai6NGtEk4Yh\npB4pIOVIPit3p7E/M4+uzRtyQd+WvLp4F3PWH+DGkXH8vDOVH7YcothluGlUR/52brejA/vlg8O5\nfHDbKp6uKIo36jrAffKTstXO/N0upohYu4DOXQcJrBsqqk3FK5fLEuUoC7dlEVVmAOwwEho2t4X6\nek2y6xJCIyG6Q00/zXHh1cXxvLRoJ8PiYkjPKeCD5XvIK3QRERJIbKNQujRvxCOTejO6azMCAoSr\nhrfn75+v58GvNhHbMJQbRsZxxeC2R6uWKopSc1RZ+JvkbXbfBc+dxlr2s5aFm/TdvrugwC7OC2lk\n60Vl7IXu55e+HhAIvS6BlW/YmEnSOhvcroflMX7ekUJWbiFndW9GWHAgczck8di3Wzi/T0uenTqA\ngADB5TLkVZJ2OrhDDF/fNpKtBw7TvWWjKrfMVBSl+qiyqClHDtm9Jtyz/bKkbC2/gKzVANjytR3I\nG0RbN5S7zpOvRLW2ZT1yUry7r/pcBr++aKu6HtxQej/tekBGTgEPzN7Il2vsOsxGYUGc27MF36zf\nT/+20Tw1ud/RwHKAD2mnIUEB9GnjvXyFoig1R5VFTfnqzzYr6cYfyl8rzIX0PdB3Sunz7v0aktZa\nxZGTWj3LAmzcImGFfe9eY+FJ60HW9bXkaSjMqRfxCmMMiRm5LItP48m5W0jLLuAv53RlUPvGfLYq\ngTnrk2jWKIxXrx6sWUaKUs9QZVFT0uLtX3FR+c13UncApnydppaOpZG0xu6nDb6nzbqJag07v7fv\nvVkWInZtx+J/2+MW/suEKihylcsOyswp5Il5W0hIz8XlMhS5XOw4lE3KkXwAujVvxBvXDqF3a2sN\njOgSyyOTbJFEXxavKYpyfNH/lTXlcJIt45EWX14pHM2E6lb6fEQTiGpn4xZuJeHrgjw3kR71gSrK\nourjKIugsJJsrFrEGMMbP+/miW+3cE7PZtxzfk9aRzdg64HDzHh3JfszcunZKopAZ6P6UV1i6d8u\nmv5to+nZMrLcpvOqJBSl/qL/O2tCYW7JZkKHNpVXFinbbEpsk87l+7bqb9dauHeFOxbLAmwZkYbN\nvbdp1sOWFwmOqPGWox+v3McbP+9mWFwMlw1qQ9uYcP7vk7XM23iQQe0b88OWQ/yw5RCXDWrDZ6sS\niQgN4sMZw8vtl6AoyomJKouacDip5P2hzdBrYunryVttPCHYy/aPrfrD5tmOKyrGZjhVB/fCvKg2\nlWc5Tf3QFhX0gWKXYfH2ZLYeOMyork3p3qIR+UUuHvpqEzOX76VT0wg+WL6Xt37ZTYPgQAqLXdx7\nfg9+NyKOxIxcHv1mM+8t28vAdtG8OH2QbnupKCcRqixqQpansthY/nrKNrty2xvuDKlt86wFUF3c\nbqiqFvL5sL92enYBry6O57NViRzIslvQPvbtFuJiIwgNCmDLgcPcPLoTfxvblez8YmavTWT57nSu\nPb39UcuhTeNwXpw+iB2HjtAuJlxXOCvKSYYqi5rgtixiu1rLwpPiIhvg7nxO+X5Q4n4qzKl+JhRA\npLPdpa+rvivAGMNtM1fzy84UzuzalPsv7MmAdtH8uCWZbzcksTs1m5evGsS4Xi0AiAoP4KrTOnDV\naR283q9zM10IpygnI6osaoJbWXQ6G5a/UlKLCWyBv+KCii2L8BjrosrYU/3gNkBIOAydAd3Oq7pt\nJXy+OpElO1L4x8W9SimAacPaMW1YPd4jRFGU44r6CmrC4QMQ1MBu8WlcJdlPUHEmlCetjjG47ea8\nf9l9uY+RNGdvhgHtorlymJe1GoqiKA6qLGrC4SSIbGn3p4DSrqij1WYrSVl1xy2OxQ1VC/xzzmay\ncgt57JI+Fe7xoCiKAuqGqhlZSXbnuZiONoX1kMf24vE/WiVQWZZTz4k2fdZtYfiB7PwikjJzCQoI\nIChQyC0oJiE9l01JWXzyWwK3jO5E9xaRVd9IUZRTGlUWNeFwki2rERhk3U1uyyJ9D8QvgtF3V94/\nJg4mv+038b5Zl8T9X24gNbvA6/WeLSP54xgv+34riqKUQZXFsWKMVRaNbJYQzXrAnp/t+7Uz7Wv/\nqX57vMtleMKpr9QyugGtosKIiQghskEwYcGBvLRwJ3M3HqBP6yjuvaAHxkBRsSE0OIA2jRvQpnE4\nTRuGqvtJURSfUGVxrORl2H2z3SmszXvC+lmQkwar34eOo2uc1loZ7y7bw8s/xdMkIoS0nIJy6+5C\nggK4a0J3bhgRV66shqIoSnVRZeErK16zMYjOY+yxe0HeUcuip31d/ipk7oVzHvCbKLtSsnns282M\n7taUN68dQmGx4WBWHhk5hRzOKyQrr5CeLaNo1yTcbzIoinJqocrCF3Iz4Nu7oP1pJcrCvcaikWNZ\nuFdh//xfG9TufoFfRCl2GW7/eC0hgQE8fklfRISQIKFtTDhttQyToih+Qv0TvrBtLrgKbeaSy2XP\nHS5jWUS1tbvXFWZDn8ne60HVAq8tjue3Pek8dHEvWkRp7SVFUY4Pqix8YdNs+5qfZUuRg4eyaGlf\nRUqsiwHTa12EwmIXz3y3jX/N28q4Xs2Z2L+CnfkURVH8gLqhqiL/MOz4DjqMhN2L7R4UsZ1tzKJB\n49IWRLcJ9lwt70q39cBh/vbxGjYkZjGxfysentgbEc1iUhTl+OFXy0JExovIVhHZISJ3ebneTkR+\nFJHVIrJORM7zuHa302+riIzzp5yVsn2+3dzozP+zmwjtX23PHz5QYlW4GflXuHKWtTJqgcJiF8/9\nsJ0Ln11CUkYeL00fxDNTBhAZFlwr91cURfEVv1kWIhIIPA+MBRKAFSIy2xjjscyZe4FZxpgXRaQn\nMAfo4LyfAvQCWgHfiUhXY0yxv+StkE2zIaIZtD/Dbk16VFkklVcWtciGxEz+75N1bErK4oK+LXno\nol40aRjqt+cpiqJUhj/dUEOBHcaYeAAR+RC4GPBUFgZw15qIAvY77y8GPjTG5AO7RGSHc7+lfpS3\nPAU5sH0B9JsCAYG2ltPq98BVbJWFO122ljDG8POOVN5ZupvvNh+kScNQXpo+iPG9W9TqcxRFUaqL\nP5VFa2Cfx3ECMKxMmweB+SJyGxABuDd/aA0sK9O3XERXRGYAMwDatfPDArid39vspp4X2eNWA2D5\ny5C8BY4ctEUEa4DLZfh1VxqbkrLYeiCLFbvT2ZWSTUxECDed2YmbRnUkOjykFj6IoihKzajrAPdU\n4C1jzFMichrwroj09rWzMeYV4BWAwYMH+7Z3aHXYNNtuedp+hD323N3OuErSZo+BwmIXt3+8li/X\nWGMqtmEIPVpGcutZnTm/b0vCggNrKr2iKEqt4U9lkQi09Thu45zz5HfAeABjzFIRCQNifezrX1wu\n2D4Pul9oCwUCxHaB4AjY+q09di/Iqyb5RcXc+sFqFmw6yF/HdmXasHbEajxCUZR6jD+zoVYAXUQk\nTkRCsAHr2WXa7AXGAIhIDyAMSHbaTRGRUBGJA7oAy/0oa3nSd0FeJrTz8JwFBNq02IQV9vgYLIvc\ngmJueHslCzYd5KGLevHHMV1UUSiKUu/xm7IwxhQBtwLzgM3YrKeNIvKwiDhBAP4G3Cgia4GZwLXG\nshGYhQ2GzwX+cNwzoZLW2teyayZaDcDG5SkpIugjxhju+GQtS3ak8ORlfbnm9A41FlNRFOV44NeY\nhTFmDjYd1vPc/R7vNwFnVND3UeBRf8pXKUlrISAYmvYofb71QPsqgRDRtFq3fPuX3Xy9Lok7xnVj\n8uC2VXdQFEWpJ2i5j4pIWmvLjgeVyUZyB7kbNrduKR9ZtTedR+dsZkz3Ztx8ZqdaFFRRFMX/qLLw\nhjFWWXgr29E4DkKjqhWvSMsu4Nb3V9EiKoynJ/fXDYcURTnhqOvU2fpJZgLkpnlXFgEBdpFeg8a+\n3SqnkGveWE5KdgGf3Xw6UeFaqkNRlBMPVRbeOBrc7u/9+nlP+nSbjJwCpr/+K9sOHOHlqwbRu3VU\nLQmoKIpyfFFl4Y2ktTaA3bzXMd8iI6eAK1/7le2HjvDy1YM4q1uzWhRQURTl+KLKwhsH1kHTbhDc\n4Ji6G2P444dr2H7oCK9cNYjRqigURTnB0QC3NyoKbvvIxysT+GlbMvec10MVhaIoJwWqLMpy+KCt\nKHuMyiIpM5d/fL2JYXExXDW8fS0LpyiKUjeosijLgXX2tUXfanc1xnD3Z+spchmevKyvpsgqinLS\n4JOyEJHPROR8ETn5lUvSGvvaok+1u366KpGFW5O5c3w32jeJqGXBFEVR6g5fB/8XgGnAdhF5XES6\n+VGmuiVpLcR0grDIqtt6cCS/iMe/3cyg9o25+rQO/pFNURSljvBJWRhjvjPGXAkMBHZjtzn9RUSu\nE5GTa5XZMQa3X1q4k5QjBdx3QU91PymKctLhs1tJRJoA1wI3AKuB/2KVxwK/SFYXFOVDxl5o1qPq\nth7sz8jl1cXxXNSvFf3bRvtJOEVRlLrDp3UWIvI50A14F7jQGJPkXPpIRFb6S7jjTl6mffWxlIeb\nf8/figH+b/zJ651TFOXUxtdFef8zxvzo7YIxZnAtylO3uJVFmO/WwYbETD5blcjvz+xEm8bhfhJM\nURSlbvHVDdVTRI6OoCLSWERu8ZNMdUduhn0N872G0xNztxATEcItZ2nZcUVRTl58VRY3GmMy3AfG\nmHTgRv+IVIcctSx8UxZr9mWweHsKN43qSGTYyRXnVxRF8cRXZREoIkdTfEQkEAippP2JSV71LIvn\nfthBVINgrtSV2oqinOT4qizmYoPZY0RkDHa/7Ln+E6uOOBrgrjpmsTkpi+82H+T6M+JoGKr1GBVF\nObnxdZS7E7gJuNk5XgC85heJ6pJquKGe/3EHDUODuPb0Dv6VSVEUpR7gk7IwxriAF52/k5e8DAgM\ngaCwSpvFJx/hm/VJ3DSqk+58pyjKKYGv6yy6AI8BPYGjI6kxpqOf5Kob8jKtVSGVr8B+adFOQgID\n+N2IuOMkmKIoSt3ia8ziTaxVUQScBbwDvOcvoeoMt7KohINZeXy+OpErhrSlaaPQ4ySYoihK3eKr\nsmhgjPkeEGPMHmPMg8D5VXUSkfEislVEdojIXV6u/0dE1jh/20Qkw+Nasce12b5+oBqRl1nlgry3\nf9lNkcuoVaEoyimFrwHufKc8+XYRuRVIBBpW1sFJr30eGAskACtEZLYxZpO7jTHmLx7tbwMGeNwi\n1xjT30f5aocqlEVOQRHv/7qXcT1baAlyRVFOKXy1LP4EhAN/BAYB04FrqugzFNhhjIk3xhQAHwIX\nV9J+KjYlt+6owg318coEMnMLuXHUyRWqURRFqYoqlYVjIVxhjDlijEkwxlxnjLnUGLOsiq6tgX0e\nxwnOOW/PaA/EAT94nA4TkZUiskxEJlbQb4bTZmVycnJVH6VqcjMqVBbFLsPrS3YxsF00g9pXr9Cg\noijKiU6VysIYUwyM8LMcU4BPnGe5ae8UKZwGPCMi5YovGWNeMcYMNsYMbtq0ac0kMKZSy2L+xgPs\nTcvhxpFqVSiKcurha8xitRNk/hjIdp80xnxWSZ9EoK3HcRvnnDemAH/wPGGMSXRe40VkITaesdNH\neatPYS64CitUFm/8vIt2MeGc26uF30RQFEWpr/gaswgDUoGzgQudvwuq6LMC6CIicSISglUI5bKa\nRKQ70BhY6nGusYiEOu9jgTOATWX71iqVlPo4mJXHit3pTB7chkDdBU9RlFMQX1dwX1fdGxtjipzM\nqXlAIPCGMWajiDwMrDTGuBXHFOBDY4zx6N4DeFlEXFiF9rhnFpVfqKTUx3ebDwIwtqdaFYqinJr4\nuoL7TcCUPW+Mub6yfsaYOcCcMufuL3P8oJd+vwB9fJGt1qhMWWw6SLuYcLo2rzRbWFEU5aTF15jF\n1x7vw4BJwP7aF6cOOVqevLQbKju/iJ93pnLV8PZIFWVAFEVRTlZ8dUN96nksIjOBJX6RqK6owLL4\naVsyBUUuxvZsXgdCKYqi1A98DXCXpQvQrDYFqXMqUBYLNh0kOjyYwbq2QlGUUxhfYxaHKR2zOIDd\n4+LkwcsueUXFLn7YeoizuzcjKPBY9aqiKMqJj69uqEb+FqTOycuEoAYQVFJJdsXudDJyCjlXXVCK\nopzi+DRdFpFJIhLlcRxdUQmOExYvpT4WbDpISFAAI7vUcHW4oijKCY6vvpUHjDGZ7gNjTAbwgH9E\nqiO8lPr4YctBzujUhAjdY1tRlFMcX5WFt3Yn1whaRlnkFRazOzWHge00sK0oiuKrslgpIk+LSCfn\n72ngN38Kdtwpoyz2puUA0K5JeF1JpCiKUm/wVVncBhQAH2H3pcijTOG/E568zFJ1ofamWmWhmxwp\niqL4ng2VDZTbFvWkooxlscdtWcSoZaEoiuJrNtQCEYn2OG4sIvP8J9ZxxsteFvvScmgUGkTj8OA6\nFExRFKV+4KsbKtbJgEA9nhMAAA8eSURBVALAGJPOybSCu+AImOLSlkVqNm1jwrUelKIoCr4rC5eI\ntHMfiEgHvFShPWHxUupjb1oO7TW4rSiKAvie/noPsEREFgECjARm+E2q400ZZeFyGfal53JOD125\nrSiKAr4HuOeKyGCsglgNfAHk+lOw48pRZWHDMgey8igocmnarKIoioOvhQRvAP6E3Ud7DTAcuw3q\n2f4T7ThSxrJwr7FoH6Nps4qiKOB7zOJPwBBgjzHmLGAAkFF5lxOI3NIVZ91rLDRtVlEUxeKrssgz\nxuQBiEioMWYL0M1/Yh1nyrih9qblEBggtIoOq0OhFEVR6g++BrgTnHUWXwALRCQd2OM/sY4zR5VF\nJGAX5LWObqB7WCiKojj4GuCe5Lx9UER+BKKAuX6T6niTlwkhDSHQLsDTtFlFUZTSVLtyrDFmkT8E\nqVPKFhFMzea8Pi3rUCBFUZT6hV/9LCIyXkS2isgOESlXW0pE/iMia5y/bSKS4XHtGhHZ7vxd4085\nySvZ+Cgrr5D0nEINbiuKonjgtz0pRCQQeB4YCyQAK0RktjFmk7uNMeYvHu1vw2ZZISIx2M2VBmNX\niv/m9E33i7AelkVJtVlVFoqiKG78aVkMBXYYY+KNMQXY0uYXV9J+KjDTeT8OWGCMSXMUxAJgvN8k\n9bAs3Gss2qploSiKchR/KovWwD6P4wTnXDlEpD0QB/xQnb4iMkNEVorIyuTk5GOX1NOySNN9LBRF\nUcpSX3JDpwCfGGOKq9PJGPOKMWawMWZw06ZNj/3peZlH11jsSc2hSUQIDXXfbUVRlKP4U1kkAm09\njts457wxhRIXVHX71gyXC/KyjloW+9Jy1AWlKIpSBn8qixVAFxGJE5EQrEKYXbaRiHQHGmNrTbmZ\nB5zrbLLUGDjXOVf75GcB5qiy2JOWrcFtRVGUMvhNWRhjioBbsYP8ZmCWMWajiDwsIhd5NJ0CfGiM\nMR5904B/YBXOCuBh55wfBHVB70uhWQ9cLsP+jDzaNG7gl0cpiqKcqPjVMW+MmQPMKXPu/jLHD1bQ\n9w3gDb8J5yY8Bi6zj8nJL6LYZYhuEOL3xyqKopxI1JcAd70gJ78IgPDQwDqWRFEUpX6hysKD7AKb\njBURoplQiqIonqiy8CDbbVmEqGWhKIriiSoLD3ILHctC11goiqKUQpWFB2pZKIqieEeVhQc5Tswi\nXGMWiqIopVBl4YFaFoqiKN5RZeGB27LQmIWiKEppVFl4kF2gloWiKIo3VFl4kJNfTGCAEBqkX4ui\nKIonOip6kF1QRHhIICJS16IoiqLUK1RZeJCTX6yrtxVFUbygysKDnMJijVcoiqJ4QZWFBzn5RVpE\nUFEUxQuqLDywMQt1QymKopRFlYUHOQXFRKgbSlEUpRyqLDzIzi8iXBfkKYqilEOVhQdqWSiKonhH\nlYUH2fkas1AURfGGKgsPcgo0dVZRFMUbqiwcCopcFLmMFhFUFEXxgioLhxwtIqgoilIhqiwcst3l\nyTVmoSiKUg6/KgsRGS8iW0Vkh4jcVUGbySKySUQ2isgHHueLRWSN8zfbn3KCXb0N6ApuRVEUL/ht\nGi0igcDzwFggAVghIrONMZs82nQB7gbOMMaki0gzj1vkGmP6+0u+sqhloSiKUjH+tCyGAjuMMfHG\nmALgQ+DiMm1uBJ43xqQDGGMO+VGeSsnRLVUVRVEqxJ/KojWwz+M4wTnnSVegq4j8LCLLRGS8x7Uw\nEVnpnJ/o7QEiMsNpszI5OblGwrotC11noSiKUp66HhmDgC7AaKAN8JOI9DHGZADtjTGJItIR+P/2\n7j+2zqqO4/j7Q7uNdTNsyCC4TRhSVFR+2RBkahZQHEqEPxD5pQtR+QciiKJgFOKMiSZG1LAgE6Yj\nIqAI2JhFxIEoGmDlh8KKwBwqXYBNGCgtrmv79Y9zyq6Xe/t0XZ/dcu/nlTTrc+5z23N2tvu55zzP\nPedOSY9ExN8qnxwRK4GVAF1dXbErFXn1bihfszAze40yRxabgIUVxwtyWaU+oDsitkfEU8ATpPAg\nIjblPzcCvwOOLLGuDPiahZlZXWWGxTqgU9IiSdOB04Hqu5puI40qkLQPaVpqo6S5kmZUlC8GeilR\nv++GMjOrq7S30RExJOl84HagDVgVEeslLQd6IqI7P3aCpF5gGLg4Ip6XdCxwtaQRUqB9s/IuqjKM\njiw6pjkszMyqlTrnEhFrgDVVZZdVfB/ARfmr8pw/Ae8qs27V+geHmNG+B+1t/pyimVk1vzJmA9uG\nvS6UmVkdDossbanqKSgzs1ocFtnANi9PbmZWj8MiSyMLT0OZmdXisMgGBoeZ5dtmzcxqclhkaZc8\njyzMzGpxWGQDg0PM8jULM7OaHBZZ/7ZhOnzrrJlZTQ6LzCMLM7P6HBbAyEgwMDjMTF+zMDOryWEB\nvLJ9dMVZjyzMzGpxWJA+YwH4moWZWR0OC9Knt8EjCzOzehwWVCxP7msWZmY1OSzYsaWqP8FtZlab\nwwLo98jCzGxMDgtgYHRLVV+zMDOryWHBjpHFLI8szMxqcliw45pFh69ZmJnV5LAgrQsFHlmYmdXj\nsCCNLCTYc5r/OszMavGrI3njo+ntSGp0VczMpqRSw0LSUkmPS9og6ZI655wmqVfSekk/rShfJunJ\n/LWszHoODA75TigzszGUNkkvqQ1YAXwQ6APWSeqOiN6KczqBS4HFEbFV0r65fG/gcqALCOCB/Nyt\nZdS1f9uww8LMbAxljiyOBjZExMaIGARuBE6uOuczwIrREIiIzbn8Q8AdEfFCfuwOYGlZFU0jC1/c\nNjOrp8ywmA88XXHcl8sqHQIcIumPku6VtHQnnoukcyX1SOrZsmXLhCvav23YS32YmY2h0Re424FO\nYAlwBvBDSXPG++SIWBkRXRHRNW/evAlXwiMLM7OxlRkWm4CFFccLclmlPqA7IrZHxFPAE6TwGM9z\nJ03/oEcWZmZjKTMs1gGdkhZJmg6cDnRXnXMbaVSBpH1I01IbgduBEyTNlTQXOCGXlWJgm0cWZmZj\nKe0VMiKGJJ1PepFvA1ZFxHpJy4GeiOhmRyj0AsPAxRHxPICkr5MCB2B5RLxQVl37B4e98ZGZ2RhK\nfTsdEWuANVVll1V8H8BF+av6uauAVWXWb9Qrg8PM9MjCzKyuRl/gbrjBoREGh0c8sjAzG0PLh8Ur\noxsfzfDIwsysnpYPC4CTDtufg/ed3ehqmJlNWS3/dnqvjmlceeZRja6GmdmU5pGFmZkVcliYmVkh\nh4WZmRVyWJiZWSGHhZmZFXJYmJlZIYeFmZkVcliYmVkhpbX8Xv8kbQH+sQs/Yh/gX5NUndeLVmwz\ntGa7W7HN0Jrt3tk2HxARhbvHNU1Y7CpJPRHR1eh67E6t2GZozXa3YpuhNdtdVps9DWVmZoUcFmZm\nVshhscPKRlegAVqxzdCa7W7FNkNrtruUNvuahZmZFfLIwszMCjkszMysUMuHhaSlkh6XtEHSJY2u\nT1kkLZR0l6ReSeslXZDL95Z0h6Qn859zG13XySapTdJDkn6VjxdJui/3+U2Spje6jpNN0hxJN0v6\nq6THJL2n2fta0ufyv+1HJd0gac9m7GtJqyRtlvRoRVnNvlXy/dz+v0ia8E5vLR0WktqAFcCJwKHA\nGZIObWytSjMEfD4iDgWOAc7Lbb0EWBsRncDafNxsLgAeqzj+FnBFRBwMbAU+1ZBalet7wK8j4m3A\n4aT2N21fS5oPfBboioh3Am3A6TRnX/8YWFpVVq9vTwQ689e5wFUT/aUtHRbA0cCGiNgYEYPAjcDJ\nDa5TKSLimYh4MH//H9KLx3xSe1fn01YDpzSmhuWQtAD4CHBNPhZwHHBzPqUZ27wX8H7gWoCIGIyI\nF2nyviZtEz1TUjvQATxDE/Z1RPweeKGquF7fngxcF8m9wBxJ+0/k97Z6WMwHnq447stlTU3SgcCR\nwH3AfhHxTH7oWWC/BlWrLN8FvgiM5OM3Ai9GxFA+bsY+XwRsAX6Up9+ukTSLJu7riNgEfBv4Jykk\nXgIeoPn7elS9vp2017hWD4uWI2k28Avgwoj4d+Vjke6jbpp7qSWdBGyOiAcaXZfdrB04CrgqIo4E\n+qmacmrCvp5Lehe9CHgTMIvXTtW0hLL6ttXDYhOwsOJ4QS5rSpKmkYLi+oi4JRc/NzoszX9ublT9\nSrAY+Kikv5OmGI8jzeXPyVMV0Jx93gf0RcR9+fhmUng0c19/AHgqIrZExHbgFlL/N3tfj6rXt5P2\nGtfqYbEO6Mx3TEwnXRDrbnCdSpHn6q8FHouI71Q81A0sy98vA365u+tWloi4NCIWRMSBpL69MyLO\nAu4CTs2nNVWbASLiWeBpSW/NRccDvTRxX5Omn46R1JH/rY+2uan7ukK9vu0GPpnvijoGeKliumqn\ntPwnuCV9mDSv3QasiohvNLhKpZD0XuAPwCPsmL//Mum6xc+AN5OWeD8tIqovnr3uSVoCfCEiTpJ0\nEGmksTfwEHB2RGxrZP0mm6QjSBf1pwMbgXNIbw6btq8lfQ34OOnOv4eAT5Pm55uqryXdACwhLUX+\nHHA5cBs1+jYH55WkKbkB4JyI6JnQ7231sDAzs2KtPg1lZmbj4LAwM7NCDgszMyvksDAzs0IOCzMz\nK+SwMJsCJC0ZXRXXbCpyWJiZWSGHhdlOkHS2pPslPSzp6rxXxsuSrsh7KayVNC+fe4Ske/M+ArdW\n7DFwsKTfSvqzpAclvSX/+NkVe1Bcnz9QZTYlOCzMxknS20mfEF4cEUcAw8BZpEXreiLiHcDdpE/U\nAlwHfCkiDiN9cn60/HpgRUQcDhxLWiUV0krAF5L2VjmItLaR2ZTQXnyKmWXHA+8G1uU3/TNJC7aN\nADflc34C3JL3lJgTEXfn8tXAzyW9AZgfEbcCRMR/AfLPuz8i+vLxw8CBwD3lN8usmMPCbPwErI6I\nS/+vUPpq1XkTXUOncs2iYfz/06YQT0OZjd9a4FRJ+8Kr+x4fQPp/NLqy6ZnAPRHxErBV0vty+SeA\nu/MuhX2STsk/Y4akjt3aCrMJ8DsXs3GKiF5JXwF+I2kPYDtwHmlzoaPzY5tJ1zUgLRX9gxwGoyu/\nQgqOqyUtzz/jY7uxGWYT4lVnzXaRpJcjYnaj62FWJk9DmZlZIY8szMyskEcWZmZWyGFhZmaFHBZm\nZlbIYWFmZoUcFmZmVuh/2N+CQIhJJhYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: hard_model_random_erase4\n",
      "\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 1s 148us/sample - loss: 0.6980 - acc: 0.7147\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.71475, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 1.1073 - acc: 0.5801 - val_loss: 0.6980 - val_acc: 0.7147\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.5523 - acc: 0.7803\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.71475 to 0.78025, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.7695 - acc: 0.6848 - val_loss: 0.5523 - val_acc: 0.7803\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.6267 - acc: 0.7515\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.78025\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.6899 - acc: 0.7230 - val_loss: 0.6267 - val_acc: 0.7515\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 1s 130us/sample - loss: 0.5332 - acc: 0.7868\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.78025 to 0.78675, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6457 - acc: 0.7400 - val_loss: 0.5332 - val_acc: 0.7868\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.5383 - acc: 0.7830\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.78675\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.6187 - acc: 0.7522 - val_loss: 0.5383 - val_acc: 0.7830\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.4100 - acc: 0.8370\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.78675 to 0.83700, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5965 - acc: 0.7636 - val_loss: 0.4100 - val_acc: 0.8370\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.4484 - acc: 0.8223\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.83700\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5726 - acc: 0.7731 - val_loss: 0.4484 - val_acc: 0.8223\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.4370 - acc: 0.8215\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.83700\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5661 - acc: 0.7786 - val_loss: 0.4370 - val_acc: 0.8215\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.4560 - acc: 0.8112\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.83700\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5480 - acc: 0.7857 - val_loss: 0.4560 - val_acc: 0.8112\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.4607 - acc: 0.8083\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.83700\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5403 - acc: 0.7847 - val_loss: 0.4607 - val_acc: 0.8083\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.4036 - acc: 0.8410\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.83700 to 0.84100, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5239 - acc: 0.7965 - val_loss: 0.4036 - val_acc: 0.8410\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.4342 - acc: 0.8282\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.84100\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.5156 - acc: 0.7992 - val_loss: 0.4342 - val_acc: 0.8282\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3965 - acc: 0.8453\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.84100 to 0.84525, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5162 - acc: 0.7991 - val_loss: 0.3965 - val_acc: 0.8453\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 1s 135us/sample - loss: 0.4006 - acc: 0.8440\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.84525\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.5005 - acc: 0.8062 - val_loss: 0.4006 - val_acc: 0.8440\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3914 - acc: 0.8470\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.84525 to 0.84700, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.4932 - acc: 0.8094 - val_loss: 0.3914 - val_acc: 0.8470\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.4798 - acc: 0.8023\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.84700\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4868 - acc: 0.8109 - val_loss: 0.4798 - val_acc: 0.8023\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3866 - acc: 0.8455\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84700\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4826 - acc: 0.8100 - val_loss: 0.3866 - val_acc: 0.8455\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.3572 - acc: 0.8577\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.84700 to 0.85775, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4765 - acc: 0.8174 - val_loss: 0.3572 - val_acc: 0.8577\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3742 - acc: 0.8515\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85775\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4643 - acc: 0.8196 - val_loss: 0.3742 - val_acc: 0.8515\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3591 - acc: 0.8568\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85775\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4581 - acc: 0.8235 - val_loss: 0.3591 - val_acc: 0.8568\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 0s 114us/sample - loss: 0.3439 - acc: 0.8658\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.85775 to 0.86575, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4638 - acc: 0.8217 - val_loss: 0.3439 - val_acc: 0.8658\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3576 - acc: 0.8575\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86575\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4617 - acc: 0.8225 - val_loss: 0.3576 - val_acc: 0.8575\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 0s 123us/sample - loss: 0.3507 - acc: 0.8625\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86575\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.4488 - acc: 0.8252 - val_loss: 0.3507 - val_acc: 0.8625\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3321 - acc: 0.8687\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.86575 to 0.86875, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4514 - acc: 0.8257 - val_loss: 0.3321 - val_acc: 0.8687\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3225 - acc: 0.8712\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.86875 to 0.87125, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4447 - acc: 0.8281 - val_loss: 0.3225 - val_acc: 0.8712\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3723 - acc: 0.8593\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87125\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4444 - acc: 0.8289 - val_loss: 0.3723 - val_acc: 0.8593\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3591 - acc: 0.8587\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87125\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4319 - acc: 0.8341 - val_loss: 0.3591 - val_acc: 0.8587\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3118 - acc: 0.8783\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.87125 to 0.87825, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4324 - acc: 0.8352 - val_loss: 0.3118 - val_acc: 0.8783\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3337 - acc: 0.8735\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87825\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4344 - acc: 0.8324 - val_loss: 0.3337 - val_acc: 0.8735\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3229 - acc: 0.8755\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87825\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4299 - acc: 0.8348 - val_loss: 0.3229 - val_acc: 0.8755\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 0s 114us/sample - loss: 0.3318 - acc: 0.8705\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87825\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4270 - acc: 0.8349 - val_loss: 0.3318 - val_acc: 0.8705\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 1s 131us/sample - loss: 0.3149 - acc: 0.8752\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.87825\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.4291 - acc: 0.8364 - val_loss: 0.3149 - val_acc: 0.8752\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3029 - acc: 0.8805\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.87825 to 0.88050, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4241 - acc: 0.8375 - val_loss: 0.3029 - val_acc: 0.8805\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.4367 - acc: 0.8342\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88050\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4177 - acc: 0.8390 - val_loss: 0.4367 - val_acc: 0.8342\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3001 - acc: 0.8798\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88050\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4207 - acc: 0.8377 - val_loss: 0.3001 - val_acc: 0.8798\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3379 - acc: 0.8712\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88050\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4226 - acc: 0.8367 - val_loss: 0.3379 - val_acc: 0.8712\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3234 - acc: 0.8765\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88050\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4151 - acc: 0.8392 - val_loss: 0.3234 - val_acc: 0.8765\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3035 - acc: 0.8867\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.88050 to 0.88675, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4113 - acc: 0.8454 - val_loss: 0.3035 - val_acc: 0.8867\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3072 - acc: 0.8780\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88675\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4141 - acc: 0.8402 - val_loss: 0.3072 - val_acc: 0.8780\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3065 - acc: 0.8850\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88675\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.4071 - acc: 0.8441 - val_loss: 0.3065 - val_acc: 0.8850\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 1s 133us/sample - loss: 0.3235 - acc: 0.8777\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88675\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4050 - acc: 0.8467 - val_loss: 0.3235 - val_acc: 0.8777\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3163 - acc: 0.8795\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.88675\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4024 - acc: 0.8474 - val_loss: 0.3163 - val_acc: 0.8795\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2883 - acc: 0.8913\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.88675 to 0.89125, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4127 - acc: 0.8411 - val_loss: 0.2883 - val_acc: 0.8913\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3155 - acc: 0.8865\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4024 - acc: 0.8463 - val_loss: 0.3155 - val_acc: 0.8865\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3219 - acc: 0.8795\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3999 - acc: 0.8477 - val_loss: 0.3219 - val_acc: 0.8795\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3149 - acc: 0.8840\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3991 - acc: 0.8471 - val_loss: 0.3149 - val_acc: 0.8840\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2974 - acc: 0.8905\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4005 - acc: 0.8471 - val_loss: 0.2974 - val_acc: 0.8905\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3620 - acc: 0.8670\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3935 - acc: 0.8475 - val_loss: 0.3620 - val_acc: 0.8670\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3313 - acc: 0.8810\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4001 - acc: 0.8478 - val_loss: 0.3313 - val_acc: 0.8810\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 1s 141us/sample - loss: 0.3086 - acc: 0.8848\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.3955 - acc: 0.8482 - val_loss: 0.3086 - val_acc: 0.8848\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2995 - acc: 0.8802\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3930 - acc: 0.8504 - val_loss: 0.2995 - val_acc: 0.8802\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3078 - acc: 0.8815\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3937 - acc: 0.8500 - val_loss: 0.3078 - val_acc: 0.8815\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3116 - acc: 0.8780\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3861 - acc: 0.8531 - val_loss: 0.3116 - val_acc: 0.8780\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2867 - acc: 0.8913\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3906 - acc: 0.8512 - val_loss: 0.2867 - val_acc: 0.8913\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2969 - acc: 0.8850\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3973 - acc: 0.8518 - val_loss: 0.2969 - val_acc: 0.8850\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2973 - acc: 0.8920\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.89125 to 0.89200, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3841 - acc: 0.8549 - val_loss: 0.2973 - val_acc: 0.8920\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2967 - acc: 0.8953\n",
      "\n",
      "Epoch 00057: val_acc improved from 0.89200 to 0.89525, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3853 - acc: 0.8550 - val_loss: 0.2967 - val_acc: 0.8953\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3303 - acc: 0.8805\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3796 - acc: 0.8558 - val_loss: 0.3303 - val_acc: 0.8805\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 1s 137us/sample - loss: 0.3459 - acc: 0.8780\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3880 - acc: 0.8518 - val_loss: 0.3459 - val_acc: 0.8780\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3024 - acc: 0.8875\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3764 - acc: 0.8549 - val_loss: 0.3024 - val_acc: 0.8875\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2967 - acc: 0.8923\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3725 - acc: 0.8565 - val_loss: 0.2967 - val_acc: 0.8923\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3074 - acc: 0.8890\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3782 - acc: 0.8562 - val_loss: 0.3074 - val_acc: 0.8890\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2880 - acc: 0.8928\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3771 - acc: 0.8545 - val_loss: 0.2880 - val_acc: 0.8928\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3204 - acc: 0.8752\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3769 - acc: 0.8558 - val_loss: 0.3204 - val_acc: 0.8752\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2941 - acc: 0.8905\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3779 - acc: 0.8557 - val_loss: 0.2941 - val_acc: 0.8905\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 1s 136us/sample - loss: 0.2840 - acc: 0.8910\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3653 - acc: 0.8619 - val_loss: 0.2840 - val_acc: 0.8910\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2938 - acc: 0.8898\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3700 - acc: 0.8577 - val_loss: 0.2938 - val_acc: 0.8898\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 1s 128us/sample - loss: 0.2947 - acc: 0.8890\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3708 - acc: 0.8602 - val_loss: 0.2947 - val_acc: 0.8890\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3267 - acc: 0.8830\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3685 - acc: 0.8618 - val_loss: 0.3267 - val_acc: 0.8830\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2841 - acc: 0.8907\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89525\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3738 - acc: 0.8566 - val_loss: 0.2841 - val_acc: 0.8907\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2704 - acc: 0.9047\n",
      "\n",
      "Epoch 00071: val_acc improved from 0.89525 to 0.90475, saving model to hard_model_random_erase4.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3723 - acc: 0.8577 - val_loss: 0.2704 - val_acc: 0.9047\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2792 - acc: 0.8972\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3681 - acc: 0.8615 - val_loss: 0.2792 - val_acc: 0.8972\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.2678 - acc: 0.8985\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3650 - acc: 0.8578 - val_loss: 0.2678 - val_acc: 0.8985\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2768 - acc: 0.8985\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3608 - acc: 0.8634 - val_loss: 0.2768 - val_acc: 0.8985\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2842 - acc: 0.8995\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3586 - acc: 0.8627 - val_loss: 0.2842 - val_acc: 0.8995\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2703 - acc: 0.9018\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3634 - acc: 0.8619 - val_loss: 0.2703 - val_acc: 0.9018\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 1s 136us/sample - loss: 0.2775 - acc: 0.8980\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3587 - acc: 0.8616 - val_loss: 0.2775 - val_acc: 0.8980\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2747 - acc: 0.8972\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3582 - acc: 0.8615 - val_loss: 0.2747 - val_acc: 0.8972\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3032 - acc: 0.8855\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3571 - acc: 0.8666 - val_loss: 0.3032 - val_acc: 0.8855\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2760 - acc: 0.8957\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3600 - acc: 0.8637 - val_loss: 0.2760 - val_acc: 0.8957\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2810 - acc: 0.8928\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3570 - acc: 0.8655 - val_loss: 0.2810 - val_acc: 0.8928\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2841 - acc: 0.8950\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3560 - acc: 0.8643 - val_loss: 0.2841 - val_acc: 0.8950\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3192 - acc: 0.8777\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3587 - acc: 0.8626 - val_loss: 0.3192 - val_acc: 0.8777\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2717 - acc: 0.9028\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3505 - acc: 0.8680 - val_loss: 0.2717 - val_acc: 0.9028\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 1s 135us/sample - loss: 0.2683 - acc: 0.8980\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3568 - acc: 0.8647 - val_loss: 0.2683 - val_acc: 0.8980\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 1s 135us/sample - loss: 0.2880 - acc: 0.8955\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3538 - acc: 0.8643 - val_loss: 0.2880 - val_acc: 0.8955\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2749 - acc: 0.8967\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3530 - acc: 0.8651 - val_loss: 0.2749 - val_acc: 0.8967\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3027 - acc: 0.8850\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3589 - acc: 0.8640 - val_loss: 0.3027 - val_acc: 0.8850\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2724 - acc: 0.8967\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3568 - acc: 0.8648 - val_loss: 0.2724 - val_acc: 0.8967\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.2658 - acc: 0.9007\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3525 - acc: 0.8650 - val_loss: 0.2658 - val_acc: 0.9007\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3008 - acc: 0.8898\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3466 - acc: 0.8702 - val_loss: 0.3008 - val_acc: 0.8898\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2689 - acc: 0.9030\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3501 - acc: 0.8671 - val_loss: 0.2689 - val_acc: 0.9030\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.2800 - acc: 0.8970\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3472 - acc: 0.8697 - val_loss: 0.2800 - val_acc: 0.8970\n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2750 - acc: 0.9000\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3495 - acc: 0.8679 - val_loss: 0.2750 - val_acc: 0.9000\n",
      "Epoch 95/100\n",
      "4000/4000 [==============================] - 0s 118us/sample - loss: 0.2843 - acc: 0.9007\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3518 - acc: 0.8655 - val_loss: 0.2843 - val_acc: 0.9007\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.2875 - acc: 0.8885\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.3458 - acc: 0.8691 - val_loss: 0.2875 - val_acc: 0.8885\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 0s 115us/sample - loss: 0.2771 - acc: 0.9000\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3478 - acc: 0.8673 - val_loss: 0.2771 - val_acc: 0.9000\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.2775 - acc: 0.8963\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3458 - acc: 0.8665 - val_loss: 0.2775 - val_acc: 0.8963\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.2646 - acc: 0.9013\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3473 - acc: 0.8672 - val_loss: 0.2646 - val_acc: 0.9013\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2656 - acc: 0.9035\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3438 - acc: 0.8698 - val_loss: 0.2656 - val_acc: 0.9035\n",
      "\n",
      "Time to train classifier: 887.29 seconds\n",
      "\n",
      "hard_model_random_erase4 Test accuracy = 92.80%\n",
      "\n",
      "\n",
      "hard_model_random_erase4 Gestalt Test accuracy = 92.92%\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMr5LAwM7NCDgszMyv0P885oD11KP5eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: hard_model_random_erase5\n",
      "\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 1s 168us/sample - loss: 0.6756 - acc: 0.7268\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.72675, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 12s 19ms/step - loss: 1.0891 - acc: 0.5838 - val_loss: 0.6756 - val_acc: 0.7268\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.7246 - acc: 0.7128\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.72675\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.7614 - acc: 0.6900 - val_loss: 0.7246 - val_acc: 0.7128\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.6020 - acc: 0.7663\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.72675 to 0.76625, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6778 - acc: 0.7276 - val_loss: 0.6020 - val_acc: 0.7663\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 1s 138us/sample - loss: 0.4557 - acc: 0.8213\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.76625 to 0.82125, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.6374 - acc: 0.7458 - val_loss: 0.4557 - val_acc: 0.8213\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.4997 - acc: 0.8010\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82125\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.6083 - acc: 0.7575 - val_loss: 0.4997 - val_acc: 0.8010\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.4042 - acc: 0.8372\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.82125 to 0.83725, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5905 - acc: 0.7671 - val_loss: 0.4042 - val_acc: 0.8372\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.6164 - acc: 0.7573\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.83725\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.5701 - acc: 0.7744 - val_loss: 0.6164 - val_acc: 0.7573\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3972 - acc: 0.8420\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.83725 to 0.84200, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5569 - acc: 0.7809 - val_loss: 0.3972 - val_acc: 0.8420\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.4243 - acc: 0.8288\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.84200\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5484 - acc: 0.7863 - val_loss: 0.4243 - val_acc: 0.8288\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.5142 - acc: 0.7897\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.84200\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5357 - acc: 0.7876 - val_loss: 0.5142 - val_acc: 0.7897\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3782 - acc: 0.8487\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.84200 to 0.84875, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5146 - acc: 0.7966 - val_loss: 0.3782 - val_acc: 0.8487\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.4125 - acc: 0.8447\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.84875\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5086 - acc: 0.8007 - val_loss: 0.4125 - val_acc: 0.8447\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 1s 140us/sample - loss: 0.3984 - acc: 0.8475\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.84875\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.5103 - acc: 0.7976 - val_loss: 0.3984 - val_acc: 0.8475\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3683 - acc: 0.8580\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.84875 to 0.85800, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.5049 - acc: 0.8040 - val_loss: 0.3683 - val_acc: 0.8580\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.4304 - acc: 0.8342\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85800\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4876 - acc: 0.8097 - val_loss: 0.4304 - val_acc: 0.8342\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3681 - acc: 0.8593\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.85800 to 0.85925, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4831 - acc: 0.8147 - val_loss: 0.3681 - val_acc: 0.8593\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3465 - acc: 0.8648\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.85925 to 0.86475, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4791 - acc: 0.8155 - val_loss: 0.3465 - val_acc: 0.8648\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3476 - acc: 0.8645\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86475\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4766 - acc: 0.8162 - val_loss: 0.3476 - val_acc: 0.8645\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 1s 137us/sample - loss: 0.3612 - acc: 0.8553\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86475\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.4716 - acc: 0.8166 - val_loss: 0.3612 - val_acc: 0.8553\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3411 - acc: 0.8687\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.86475 to 0.86875, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4660 - acc: 0.8160 - val_loss: 0.3411 - val_acc: 0.8687\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.4116 - acc: 0.8342\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.86875\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4621 - acc: 0.8207 - val_loss: 0.4116 - val_acc: 0.8342\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 1s 138us/sample - loss: 0.3475 - acc: 0.8570\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86875\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4553 - acc: 0.8216 - val_loss: 0.3475 - val_acc: 0.8570\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3360 - acc: 0.8685\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86875\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4561 - acc: 0.8230 - val_loss: 0.3360 - val_acc: 0.8685\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.4225 - acc: 0.8300\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86875\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4456 - acc: 0.8241 - val_loss: 0.4225 - val_acc: 0.8300\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3271 - acc: 0.8755\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.86875 to 0.87550, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4400 - acc: 0.8297 - val_loss: 0.3271 - val_acc: 0.8755\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.3505 - acc: 0.8692\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.87550\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4357 - acc: 0.8325 - val_loss: 0.3505 - val_acc: 0.8692\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3675 - acc: 0.8570\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.87550\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4372 - acc: 0.8339 - val_loss: 0.3675 - val_acc: 0.8570\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 0s 102us/sample - loss: 0.3289 - acc: 0.8702\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87550\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4355 - acc: 0.8300 - val_loss: 0.3289 - val_acc: 0.8702\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3096 - acc: 0.8842\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.87550 to 0.88425, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4341 - acc: 0.8313 - val_loss: 0.3096 - val_acc: 0.8842\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3162 - acc: 0.8765\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.88425\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4289 - acc: 0.8346 - val_loss: 0.3162 - val_acc: 0.8765\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 1s 140us/sample - loss: 0.3203 - acc: 0.8813\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.88425\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4249 - acc: 0.8359 - val_loss: 0.3203 - val_acc: 0.8813\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3708 - acc: 0.8593\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.88425\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4261 - acc: 0.8342 - val_loss: 0.3708 - val_acc: 0.8593\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3677 - acc: 0.8615\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.88425\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4254 - acc: 0.8388 - val_loss: 0.3677 - val_acc: 0.8615\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 1s 145us/sample - loss: 0.3109 - acc: 0.8835\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.88425\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.4165 - acc: 0.8381 - val_loss: 0.3109 - val_acc: 0.8835\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3315 - acc: 0.8765\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.88425\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4090 - acc: 0.8418 - val_loss: 0.3315 - val_acc: 0.8765\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.3112 - acc: 0.8823\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.88425\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4161 - acc: 0.8411 - val_loss: 0.3112 - val_acc: 0.8823\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3408 - acc: 0.8695\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.88425\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4092 - acc: 0.8444 - val_loss: 0.3408 - val_acc: 0.8695\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3008 - acc: 0.8880\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.88425 to 0.88800, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4070 - acc: 0.8438 - val_loss: 0.3008 - val_acc: 0.8880\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3200 - acc: 0.8830\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.88800\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.4056 - acc: 0.8421 - val_loss: 0.3200 - val_acc: 0.8830\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 1s 138us/sample - loss: 0.3117 - acc: 0.8805\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.88800\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.4088 - acc: 0.8421 - val_loss: 0.3117 - val_acc: 0.8805\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3262 - acc: 0.8750\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.88800\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.4070 - acc: 0.8431 - val_loss: 0.3262 - val_acc: 0.8750\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2978 - acc: 0.8907\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.88800 to 0.89075, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.4027 - acc: 0.8460 - val_loss: 0.2978 - val_acc: 0.8907\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3037 - acc: 0.8855\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3990 - acc: 0.8493 - val_loss: 0.3037 - val_acc: 0.8855\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3241 - acc: 0.8712\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3985 - acc: 0.8481 - val_loss: 0.3241 - val_acc: 0.8712\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3148 - acc: 0.8795\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3987 - acc: 0.8483 - val_loss: 0.3148 - val_acc: 0.8795\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3015 - acc: 0.8808\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3875 - acc: 0.8509 - val_loss: 0.3015 - val_acc: 0.8808\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 0s 103us/sample - loss: 0.3232 - acc: 0.8777\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3847 - acc: 0.8549 - val_loss: 0.3232 - val_acc: 0.8777\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3037 - acc: 0.8817\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3939 - acc: 0.8503 - val_loss: 0.3037 - val_acc: 0.8817\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 1s 139us/sample - loss: 0.2876 - acc: 0.8900\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3918 - acc: 0.8516 - val_loss: 0.2876 - val_acc: 0.8900\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2994 - acc: 0.8840\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3876 - acc: 0.8498 - val_loss: 0.2994 - val_acc: 0.8840\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2995 - acc: 0.8898\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3831 - acc: 0.8553 - val_loss: 0.2995 - val_acc: 0.8898\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.3067 - acc: 0.8857\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3860 - acc: 0.8516 - val_loss: 0.3067 - val_acc: 0.8857\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3150 - acc: 0.8870\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3833 - acc: 0.8526 - val_loss: 0.3150 - val_acc: 0.8870\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 1s 140us/sample - loss: 0.3079 - acc: 0.8842\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3866 - acc: 0.8537 - val_loss: 0.3079 - val_acc: 0.8842\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3049 - acc: 0.8900\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3862 - acc: 0.8535 - val_loss: 0.3049 - val_acc: 0.8900\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.3005 - acc: 0.8882\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3843 - acc: 0.8517 - val_loss: 0.3005 - val_acc: 0.8882\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 0s 114us/sample - loss: 0.3162 - acc: 0.8827\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3846 - acc: 0.8564 - val_loss: 0.3162 - val_acc: 0.8827\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 1s 133us/sample - loss: 0.3141 - acc: 0.8832\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.89075\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3754 - acc: 0.8561 - val_loss: 0.3141 - val_acc: 0.8832\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.2879 - acc: 0.8923\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.89075 to 0.89225, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3792 - acc: 0.8533 - val_loss: 0.2879 - val_acc: 0.8923\n",
      "Epoch 60/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2761 - acc: 0.8978\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.89225 to 0.89775, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3843 - acc: 0.8519 - val_loss: 0.2761 - val_acc: 0.8978\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.3013 - acc: 0.8825\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3657 - acc: 0.8598 - val_loss: 0.3013 - val_acc: 0.8825\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2924 - acc: 0.8882\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3745 - acc: 0.8571 - val_loss: 0.2924 - val_acc: 0.8882\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2984 - acc: 0.8865\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3761 - acc: 0.8553 - val_loss: 0.2984 - val_acc: 0.8865\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.2837 - acc: 0.8907\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3664 - acc: 0.8626 - val_loss: 0.2837 - val_acc: 0.8907\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.3302 - acc: 0.8802\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3756 - acc: 0.8573 - val_loss: 0.3302 - val_acc: 0.8802\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3135 - acc: 0.8838\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3749 - acc: 0.8575 - val_loss: 0.3135 - val_acc: 0.8838\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 1s 130us/sample - loss: 0.3091 - acc: 0.8840\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3696 - acc: 0.8579 - val_loss: 0.3091 - val_acc: 0.8840\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2886 - acc: 0.8947\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.89775\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3666 - acc: 0.8607 - val_loss: 0.2886 - val_acc: 0.8947\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.2903 - acc: 0.8980\n",
      "\n",
      "Epoch 00069: val_acc improved from 0.89775 to 0.89800, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 10s 15ms/step - loss: 0.3665 - acc: 0.8602 - val_loss: 0.2903 - val_acc: 0.8980\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2911 - acc: 0.8900\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.89800\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3710 - acc: 0.8586 - val_loss: 0.2911 - val_acc: 0.8900\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2885 - acc: 0.8997\n",
      "\n",
      "Epoch 00071: val_acc improved from 0.89800 to 0.89975, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3654 - acc: 0.8616 - val_loss: 0.2885 - val_acc: 0.8997\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.3092 - acc: 0.8880\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3642 - acc: 0.8622 - val_loss: 0.3092 - val_acc: 0.8880\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.2802 - acc: 0.8965\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3620 - acc: 0.8632 - val_loss: 0.2802 - val_acc: 0.8965\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.2880 - acc: 0.8950\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3568 - acc: 0.8650 - val_loss: 0.2880 - val_acc: 0.8950\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.3028 - acc: 0.8867\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3574 - acc: 0.8626 - val_loss: 0.3028 - val_acc: 0.8867\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 1s 138us/sample - loss: 0.3575 - acc: 0.8792\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3612 - acc: 0.8642 - val_loss: 0.3575 - val_acc: 0.8792\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 0s 108us/sample - loss: 0.3238 - acc: 0.8845\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3581 - acc: 0.8634 - val_loss: 0.3238 - val_acc: 0.8845\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2772 - acc: 0.8970\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3605 - acc: 0.8633 - val_loss: 0.2772 - val_acc: 0.8970\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3247 - acc: 0.8748\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3532 - acc: 0.8624 - val_loss: 0.3247 - val_acc: 0.8748\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 0s 114us/sample - loss: 0.2928 - acc: 0.8870\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3549 - acc: 0.8672 - val_loss: 0.2928 - val_acc: 0.8870\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.2850 - acc: 0.8957\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3534 - acc: 0.8681 - val_loss: 0.2850 - val_acc: 0.8957\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2958 - acc: 0.8892\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3553 - acc: 0.8661 - val_loss: 0.2958 - val_acc: 0.8892\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2973 - acc: 0.8955\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3553 - acc: 0.8652 - val_loss: 0.2973 - val_acc: 0.8955\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 0s 106us/sample - loss: 0.2888 - acc: 0.8878\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3557 - acc: 0.8638 - val_loss: 0.2888 - val_acc: 0.8878\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 0s 120us/sample - loss: 0.2938 - acc: 0.8900\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3583 - acc: 0.8630 - val_loss: 0.2938 - val_acc: 0.8900\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3146 - acc: 0.8820\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.3564 - acc: 0.8655 - val_loss: 0.3146 - val_acc: 0.8820\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2762 - acc: 0.8975\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3508 - acc: 0.8677 - val_loss: 0.2762 - val_acc: 0.8975\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 0s 113us/sample - loss: 0.2850 - acc: 0.8930\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3516 - acc: 0.8647 - val_loss: 0.2850 - val_acc: 0.8930\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 1s 140us/sample - loss: 0.2813 - acc: 0.8923\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 15ms/step - loss: 0.3499 - acc: 0.8659 - val_loss: 0.2813 - val_acc: 0.8923\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2699 - acc: 0.8990\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.3436 - acc: 0.8708 - val_loss: 0.2699 - val_acc: 0.8990\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2833 - acc: 0.8940\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3473 - acc: 0.8708 - val_loss: 0.2833 - val_acc: 0.8940\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 0s 112us/sample - loss: 0.3404 - acc: 0.8815\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3495 - acc: 0.8676 - val_loss: 0.3404 - val_acc: 0.8815\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 0s 111us/sample - loss: 0.2683 - acc: 0.8972\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3517 - acc: 0.8689 - val_loss: 0.2683 - val_acc: 0.8972\n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 1s 136us/sample - loss: 0.2835 - acc: 0.8995\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.89975\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3455 - acc: 0.8674 - val_loss: 0.2835 - val_acc: 0.8995\n",
      "Epoch 95/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2883 - acc: 0.9005\n",
      "\n",
      "Epoch 00095: val_acc improved from 0.89975 to 0.90050, saving model to hard_model_random_erase5.weights.hdf5\n",
      "625/625 [==============================] - 10s 17ms/step - loss: 0.3495 - acc: 0.8655 - val_loss: 0.2883 - val_acc: 0.9005\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 0s 109us/sample - loss: 0.2838 - acc: 0.8903\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.90050\n",
      "625/625 [==============================] - 8s 13ms/step - loss: 0.3466 - acc: 0.8688 - val_loss: 0.2838 - val_acc: 0.8903\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 0s 110us/sample - loss: 0.2967 - acc: 0.8935\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.90050\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3451 - acc: 0.8687 - val_loss: 0.2967 - val_acc: 0.8935\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 0s 105us/sample - loss: 0.2977 - acc: 0.8842\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.90050\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3479 - acc: 0.8680 - val_loss: 0.2977 - val_acc: 0.8842\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 0s 104us/sample - loss: 0.2840 - acc: 0.8913\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.90050\n",
      "625/625 [==============================] - 8s 14ms/step - loss: 0.3413 - acc: 0.8712 - val_loss: 0.2840 - val_acc: 0.8913\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 0s 107us/sample - loss: 0.2812 - acc: 0.9003\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.90050\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.3415 - acc: 0.8706 - val_loss: 0.2812 - val_acc: 0.9003\n",
      "\n",
      "Time to train classifier: 892.04 seconds\n",
      "\n",
      "hard_model_random_erase5 Test accuracy = 93.00%\n",
      "\n",
      "\n",
      "hard_model_random_erase5 Gestalt Test accuracy = 93.53%\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFhER0df/A8r1/5jAZcTJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nModels = 5 # number of models to train\n",
    "probs = np.zeros((len(y_test), nClasses)) # mean probabilities for classes\n",
    "result = [] # for results\n",
    "\n",
    "yt = np.argmax(y_test, axis = 1) # true class for test data\n",
    "\n",
    "for i in range(nModels): \n",
    "  model = 'hard_model_random_erase'+str(i+1) # model name\n",
    "  print('\\nTraining Model: '+ model + '\\n')\n",
    "  bestWts = model+\".weights.hdf5\" # best weights file\n",
    "  \n",
    "  # checkpoint to save best model\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=bestWts,\n",
    "                                                  monitor='val_acc',\n",
    "                                                  verbose=1,\n",
    "                                                  save_best_only=True,\n",
    "                                                  mode='max')\n",
    "  callbacks = [checkpoint]\n",
    "  \n",
    "  # create model\n",
    "  mod = convNN(model, ch1, ch2, kernel, pool, nDense, drop, dropDense)\n",
    "  \n",
    "  st = time.time() # start time for training\n",
    "  \n",
    "  # train with augmented data and maintain history\n",
    "  hist = mod.fit_generator(datagen.flow(x_train, y_train, batch_size=batchSize),\n",
    "                           steps_per_epoch=x_train.shape[0] // batchSize,\n",
    "                           validation_data=(x_valid, y_valid),\n",
    "                           epochs=epochs, verbose=1,\n",
    "                           callbacks=callbacks)\n",
    "  \n",
    "  t = time.time() - st # time to train model\n",
    "  \n",
    "  print(\"\\nTime to train classifier: %4.2f seconds\\n\" %(t))\n",
    "  \n",
    "  mod.save_weights(bestWts) # save best weigths\n",
    "  \n",
    "  prob = mod.predict(x_test) # predict probabilities for test examples\n",
    "  predicted = prob.argmax(axis=1) # mst likely class\n",
    "  \n",
    "  acc = metrics.accuracy_score(yt, predicted) # test accuracy \n",
    "  print('%s Test accuracy = %4.2f%%\\n\\n' %(model, acc*100.0))\n",
    "  \n",
    "  probs += prob # compute aggregate probability for classes\n",
    "  predicted = probs.argmax(axis=1) # most likely class\n",
    "  accCum = metrics.accuracy_score(yt, predicted) # test accuracy \n",
    "  print('%s Gestalt Test accuracy = %4.2f%%\\n\\n' %(model,accCum*100.0))\n",
    "  \n",
    "  result.append([model,acc,accCum,t])\n",
    "  plotHistorty(model, hist) # display training and test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8XYnh-8qOFg"
   },
   "source": [
    "#### Show results on model accuracy, cumulative accuracy, and training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "62dhkVRyqRZx",
    "outputId": "f24ca00b-b0f6-46a6-a93e-30abec12693b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>cum. accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hard_model_random_erase1</td>\n",
       "      <td>0.91525</td>\n",
       "      <td>0.91525</td>\n",
       "      <td>903.716963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hard_model_random_erase2</td>\n",
       "      <td>0.91250</td>\n",
       "      <td>0.92000</td>\n",
       "      <td>887.891403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard_model_random_erase3</td>\n",
       "      <td>0.91900</td>\n",
       "      <td>0.92550</td>\n",
       "      <td>888.479018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hard_model_random_erase4</td>\n",
       "      <td>0.92800</td>\n",
       "      <td>0.92925</td>\n",
       "      <td>887.289185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard_model_random_erase5</td>\n",
       "      <td>0.93000</td>\n",
       "      <td>0.93525</td>\n",
       "      <td>892.044436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  accuracy  cum. accuracy        time\n",
       "0  hard_model_random_erase1   0.91525        0.91525  903.716963\n",
       "1  hard_model_random_erase2   0.91250        0.92000  887.891403\n",
       "2  hard_model_random_erase3   0.91900        0.92550  888.479018\n",
       "3  hard_model_random_erase4   0.92800        0.92925  887.289185\n",
       "4  hard_model_random_erase5   0.93000        0.93525  892.044436"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(result, \n",
    "                      columns=['model', 'accuracy', 'cum. accuracy', 'time'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YfcExCLlqhGj"
   },
   "source": [
    "#### specify label for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GrxNJXGfqiT0"
   },
   "outputs": [],
   "source": [
    "HardItems = ['T-shirt/top', 'Pullover', 'Coat', 'Shirt'] # labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4RHdK18-qpR7"
   },
   "source": [
    "#### Show confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "rkadyabqqtq7",
    "outputId": "b4915d43-5d6e-461c-8277-fabbb853e2f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>True</th>\n",
       "      <th>T-shirt/top</th>\n",
       "      <th>Pullover</th>\n",
       "      <th>Coat</th>\n",
       "      <th>Shirt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T-shirt/top</td>\n",
       "      <td>894</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pullover</td>\n",
       "      <td>2</td>\n",
       "      <td>909</td>\n",
       "      <td>34</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coat</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>977</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shirt</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          True  T-shirt/top  Pullover  Coat  Shirt\n",
       "0  T-shirt/top          894         3     1     81\n",
       "1     Pullover            2       909    34     25\n",
       "2         Coat            0        10   977     15\n",
       "3        Shirt           36        15    37    961"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = metrics.confusion_matrix(yt, predicted)\n",
    "cm = pd.DataFrame(cm, columns=HardItems)\n",
    "cm.insert(0,\"True\",HardItems)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UBDlyj07rE2R"
   },
   "source": [
    "#### Show classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "rUeUtOgKrG5y",
    "outputId": "6db78330-e107-4301-c630-3c3fd4ee84ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Coat       0.93      0.98      0.95      1002\n",
      "    Pullover       0.97      0.94      0.95       970\n",
      "       Shirt       0.89      0.92      0.90      1049\n",
      " T-shirt/top       0.96      0.91      0.94       979\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      4000\n",
      "   macro avg       0.94      0.94      0.94      4000\n",
      "weighted avg       0.94      0.94      0.94      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report([HardItems[i] for i in yt], \n",
    "                                    [HardItems[i] for i in predicted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QK4UxAm7rTNy"
   },
   "source": [
    "#### Use mode class for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "x5McwA5nrcP0",
    "outputId": "93aafa21-f63c-404d-8168-3a6292773360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing hard_model_random_erase1\n",
      "processing hard_model_random_erase2\n",
      "processing hard_model_random_erase3\n",
      "processing hard_model_random_erase4\n",
      "processing hard_model_random_erase5\n",
      "Accuracy based on mode class = 93.47%\n"
     ]
    }
   ],
   "source": [
    "predClass = pd.DataFrame()\n",
    "for i in range(nModels):\n",
    "  model = 'hard_model_random_erase'+str(i+1)\n",
    "  print(\"processing \" + model)\n",
    "  bestWts = model+\".weights.hdf5\" # best weights file\n",
    "  mod.load_weights(bestWts) # load best weights\n",
    "  prob = mod.predict(x_test) # predict probabilities for test examples\n",
    "  predClass[model] = prob.argmax(axis=1) # mst likely class\n",
    "  \n",
    "modeClass = predClass.mode(axis=1)\n",
    "modeAcc = metrics.accuracy_score(yt, modeClass[0])\n",
    "print(\"Accuracy based on mode class = %4.2f%%\" %(100.0*modeAcc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HardRandomEraseFM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
